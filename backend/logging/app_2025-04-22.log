2025-04-22 00:18:16,977 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:18:16,977 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:18:16,977 - app - INFO - file_storage:70 - File storage service initialized with upload directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:18:17,624 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:18:17,624 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:18:17,636 - app - INFO - main:24 - Starting Document Processor application
2025-04-22 00:50:27,650 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 00:50:27,651 - app - DEBUG - file_storage:40 - Generated file path for source L6_NLG tasks.pdf: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf
2025-04-22 00:50:27,651 - app - INFO - qa:66 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf']
2025-04-22 00:50:27,651 - app - ERROR - tools:51 - Failed to load PDF /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf: File path /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf is not a valid file or url
2025-04-22 00:50:27,651 - app - WARNING - tools:56 - No documents were successfully loaded.
2025-04-22 00:50:34,533 - app - ERROR - qa:117 - Error in QA processing: list index out of range
2025-04-22 00:54:45,796 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 00:54:45,796 - app - DEBUG - file_storage:40 - Generated file path for source L6_NLG tasks.pdf: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf
2025-04-22 00:54:45,796 - app - INFO - qa:66 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf']
2025-04-22 00:54:45,796 - app - ERROR - tools:51 - Failed to load PDF /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf: File path /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf is not a valid file or url
2025-04-22 00:54:45,796 - app - WARNING - tools:56 - No documents were successfully loaded.
2025-04-22 00:54:48,626 - app - ERROR - qa:117 - Error in QA processing: list index out of range
2025-04-22 00:58:48,620 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:58:48,620 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:58:48,620 - app - INFO - file_storage:70 - File storage service initialized with upload directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:58:49,249 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:58:49,249 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 00:58:49,262 - app - INFO - main:24 - Starting Document Processor application
2025-04-22 00:59:14,672 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 00:59:14,672 - app - DEBUG - file_storage:40 - Generated file path for source L6_NLG tasks.pdf: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf
2025-04-22 00:59:14,672 - app - INFO - qa:66 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf']
2025-04-22 00:59:14,672 - app - ERROR - tools:51 - Failed to load PDF /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf: File path /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf.pdf is not a valid file or url
2025-04-22 00:59:14,672 - app - WARNING - tools:56 - No documents were successfully loaded.
2025-04-22 00:59:20,493 - app - ERROR - qa:117 - Error in QA processing: list index out of range
2025-04-22 01:04:10,992 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:04:10,993 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:04:10,993 - app - INFO - file_storage:75 - File storage service initialized with upload directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:04:11,604 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:04:11,604 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:04:11,616 - app - INFO - main:24 - Starting Document Processor application
2025-04-22 01:04:30,752 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:04:30,752 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:04:30,752 - app - WARNING - qa:53 - File does not exist at path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:04:30,752 - app - DEBUG - file_storage:45 - Generated file path for source L6_NLG tasks: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:04:30,752 - app - ERROR - qa:62 - File not found at either path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf or /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:04:30,752 - app - ERROR - qa:74 - Error retrieving file path for source ID L6_NLG tasks.pdf: File with ID L6_NLG tasks.pdf not found
2025-04-22 01:07:36,047 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:07:36,047 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:07:36,047 - app - INFO - file_storage:104 - File storage service initialized with upload directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:07:36,746 - app - DEBUG - file_storage:13 - Initializing file storage with directory: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:07:36,746 - app - DEBUG - file_storage:22 - Upload directory already exists: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources
2025-04-22 01:07:36,759 - app - INFO - main:24 - Starting Document Processor application
2025-04-22 01:07:41,780 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:07:41,780 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:07:41,780 - app - DEBUG - file_storage:57 - File not found at /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf, checking for alternatives
2025-04-22 01:07:41,780 - app - ERROR - qa:53 - File not found: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:07:41,780 - app - ERROR - qa:60 - Error retrieving file path for source ID L6_NLG tasks.pdf: File not found at: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:08:41,635 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:08:41,635 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:08:41,636 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:08:41,636 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:08:41,636 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:08:41,821 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:08:41,822 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:08:41,822 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:08:48,366 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:08:48,366 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:08:48,393 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main categories of NLG tasks mentioned in Lecture 6?
2025-04-22 01:08:51,668 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:08:51,668 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation (机器翻译)
*   Paraphrasing (改写)
*   Repor...
2025-04-22 01:08:51,668 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:08:51,668 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:08:51,668 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:10:48,087 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:10:48,088 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:10:48,088 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:48,088 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:10:48,088 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:48,248 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:48,248 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:10:48,248 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:10:51,441 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:10:51,441 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:10:51,443 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main categories of NLG tasks mentioned in Lecture 6?
2025-04-22 01:10:54,693 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:10:54,693 - app - INFO - qa:84 - Generated answer: 根据提供的文档“Lecture 6: NLP tasks (2)”，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation
*   Paraphra...
2025-04-22 01:10:54,695 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:10:54,695 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:10:54,695 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:10:54,702 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:10:54,703 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:10:54,703 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:54,703 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:10:54,703 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:54,863 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:10:54,863 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:10:54,864 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:10:57,526 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:10:57,526 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:10:57,527 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:10:57,527 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:10:57,529 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main categories of NLG tasks mentioned in Lecture 6?
2025-04-22 01:10:59,979 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:10:59,980 - app - INFO - qa:84 - Generated answer: 根据提供的文档“Lecture 6: NLP tasks (2)”，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation
*   Paraphra...
2025-04-22 01:10:59,980 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:10:59,980 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:10:59,980 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:11:00,061 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:11:00,061 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:11:00,062 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:00,062 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:11:00,062 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:00,221 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:00,221 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:11:00,222 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:11:03,029 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:11:03,029 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:11:03,031 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main categories of NLG tasks mentioned in Lecture 6?
2025-04-22 01:11:05,406 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:11:05,407 - app - INFO - qa:84 - Generated answer: 根据提供的文档“Lecture 6: NLP tasks (2)”，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation
*   Paraphra...
2025-04-22 01:11:05,407 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:11:05,407 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:11:05,407 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:11:11,156 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:11:11,157 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:11:11,157 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:11,157 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:11:11,157 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:11,314 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:11,314 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:11:11,314 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:11:14,592 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:11:14,592 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:11:14,594 - app - INFO - qa:78 - Invoking RAG chain with question: According to the slides for Lecture 6, what were the three main eras of machine translation development discussed (rule-based, example-based, statistical)?
2025-04-22 01:11:17,335 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:11:17,336 - app - INFO - qa:84 - Generated answer: According to the slides, the three main eras of machine translation development discussed were: rule...
2025-04-22 01:11:17,336 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:11:17,336 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:11:17,336 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:11:24,222 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:11:24,223 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:11:24,223 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:24,223 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:11:24,223 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:24,388 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:24,388 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:11:24,388 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:11:27,009 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:11:27,009 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:11:27,011 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the key components or steps involved in a task-oriented dialogue system as presented in Lecture 6.
2025-04-22 01:11:30,400 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:11:30,400 - app - INFO - qa:84 - Generated answer: According to the document, a task-oriented dialogue system involves the following key components or ...
2025-04-22 01:11:30,400 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:11:30,400 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:11:30,400 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:11:38,231 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:11:38,231 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:11:38,232 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:38,232 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:11:38,232 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:38,439 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:38,439 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:11:38,440 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:11:41,150 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:11:41,151 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:11:41,153 - app - INFO - qa:78 - Invoking RAG chain with question: Compare ranking models and generative models for dialogue systems based on the evaluation discussion in Lecture 6.
2025-04-22 01:11:50,367 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:11:50,368 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Lecture 6 讨论了两种对话系统模型：排序模型和生成模型，但没有提供关于它们之间比较的详细信息。

文档提到：

*   **生成模型**用于构建从离线对话语料库中生成对话的闲聊...
2025-04-22 01:11:50,368 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:11:50,368 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:11:50,368 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:11:58,455 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:11:58,455 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:11:58,455 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:58,455 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:11:58,456 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:58,745 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:11:58,745 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:11:58,745 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:12:01,483 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:12:01,484 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:12:01,486 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 6 provide detailed Python code examples for implementing the PERSONACHAT model?
2025-04-22 01:12:05,436 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:12:05,437 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了可以使用预训练模型（如BERT、GPT、DialoGPT、DialogBERT）来实现对话系统，但没有说明Lecture 6是否提供了PERSONAC...
2025-04-22 01:12:05,437 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:12:05,437 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:12:05,437 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:12:12,757 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:12:12,757 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:12:12,757 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:12,758 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:12:12,758 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:12,842 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:12,842 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:12:12,842 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:12:15,641 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:12:15,641 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:12:15,643 - app - INFO - qa:78 - Invoking RAG chain with question: What does RLHF stand for, as explained in Lecture 7 on Large Language Models?
2025-04-22 01:12:18,182 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:12:18,182 - app - INFO - qa:84 - Generated answer: According to the provided document, RLHF is listed as a topic covered in Lecture 7 on Large Language...
2025-04-22 01:12:18,182 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:12:18,182 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:12:18,183 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:12:24,727 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:12:24,727 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:12:24,728 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:24,728 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:12:24,728 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:24,808 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:24,808 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:12:24,808 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:12:27,451 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:12:27,451 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:12:27,453 - app - INFO - qa:78 - Invoking RAG chain with question: What are the two main components of LLM alignment discussed in Lecture 7 (Instruction Tuning and RLHF)?
2025-04-22 01:12:30,656 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:12:30,656 - app - INFO - qa:84 - Generated answer: According to the provided document, the two main components of LLM alignment discussed in Lecture 7 ...
2025-04-22 01:12:30,656 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:12:30,656 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:12:30,657 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:12:36,538 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:12:36,538 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:12:36,538 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:36,538 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:12:36,538 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:36,607 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:36,607 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:12:36,607 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:12:39,696 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:12:39,696 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:12:39,698 - app - INFO - qa:78 - Invoking RAG chain with question: Briefly explain the concept of 'emergent abilities' in Large Language Models according to Lecture 7.
2025-04-22 01:12:42,775 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:12:42,776 - app - INFO - qa:84 - Generated answer: According to Lecture 7, emergent abilities relate to the scaling law in large language models. Speci...
2025-04-22 01:12:42,777 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:12:42,777 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:12:42,777 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:12:50,545 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:12:50,545 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:12:50,546 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:50,546 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:12:50,546 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:50,627 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:12:50,627 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:12:50,627 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:12:53,371 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:12:53,371 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:12:53,373 - app - INFO - qa:78 - Invoking RAG chain with question: How does instruction finetuning differ from Reinforcement Learning from Human Feedback (RLHF) based on the descriptions in Lecture 7?
2025-04-22 01:12:56,513 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:12:56,513 - app - INFO - qa:84 - Generated answer: 根据提供的文档，instruction finetuning 涉及收集（指令，输出）对的例子，并在多个任务上微调一个语言模型 [FLAN-T5; Chung et al., 2022]。然后，在未见过...
2025-04-22 01:12:56,514 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:12:56,514 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:12:56,514 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:13:04,084 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:13:04,085 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:13:04,085 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:13:04,085 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:13:04,085 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:13:04,163 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:13:04,163 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:13:04,164 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:13:06,800 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:13:06,800 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:13:06,802 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 7 specify the exact size of the dataset used for training the initial ChatGPT model released in November 2022?
2025-04-22 01:13:09,343 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:13:09,344 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。虽然文档提到了ChatGPT于2022年11月发布，并提到了其他大型语言模型的参数大小和训练数据量，但它没有明确说明用于训练初始ChatGPT模型的具体数据集大...
2025-04-22 01:13:09,344 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:13:09,344 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:13:09,344 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:13:17,023 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:13:17,024 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:13:17,024 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:17,024 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:13:17,024 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:17,200 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:17,200 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:13:17,201 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:13:20,310 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:13:20,311 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:13:20,313 - app - INFO - qa:78 - Invoking RAG chain with question: What is the main motivation cited in Lecture 8 for using prompt learning instead of full fine-tuning for large Pre-trained Language Models (PLMs)?
2025-04-22 01:13:22,991 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:13:22,991 - app - INFO - qa:84 - Generated answer: According to the document, the main motivation for using prompt learning instead of full fine-tuning...
2025-04-22 01:13:22,992 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:13:22,992 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:13:22,992 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:13:29,682 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:13:29,682 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:13:29,683 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:29,683 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:13:29,683 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:29,855 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:29,855 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:13:29,856 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:13:32,612 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:13:32,613 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:13:32,615 - app - INFO - qa:78 - Invoking RAG chain with question: What does 'Chain of Thought Prompting' aim to achieve according to Lecture 8?
2025-04-22 01:13:34,879 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:13:34,879 - app - INFO - qa:84 - Generated answer: According to Lecture 8, 'Chain of Thought Prompting' aims to elicit reasoning in large language mode...
2025-04-22 01:13:34,880 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:13:34,880 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:13:34,880 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:13:40,964 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:13:40,964 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:13:40,964 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:40,965 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:13:40,965 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:41,143 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:41,143 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:13:41,143 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:13:43,852 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:13:43,852 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:13:43,854 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the concept of P-tuning v2 as mentioned in Lecture 8.
2025-04-22 01:13:46,792 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:13:46,792 - app - INFO - qa:84 - Generated answer: According to the provided documents, P-tuning v2 is a prompt tuning method that "can be comparable t...
2025-04-22 01:13:46,792 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:13:46,792 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:13:46,792 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:13:52,914 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:13:52,915 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:13:52,915 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:52,915 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:13:52,915 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:53,190 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:13:53,190 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:13:53,191 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:13:55,914 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:13:55,915 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:13:55,917 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the context provided in Lecture 8, what is the main difference between zero-shot and few-shot prompting?
2025-04-22 01:13:57,903 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:13:57,904 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档提到了“few-shot”学习，但没有解释“zero-shot”学习，也没有比较这两种 prompting 方法的主要区别。...
2025-04-22 01:13:57,904 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:13:57,904 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:13:57,904 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:14:04,428 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:14:04,429 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:14:04,430 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:14:04,430 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:14:04,430 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:14:04,599 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:14:04,599 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:14:04,600 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:14:07,325 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:14:07,326 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:14:07,328 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 8 provide a specific cost comparison, for instance in US dollars, between fine-tuning LLaMa and using prompt tuning techniques?
2025-04-22 01:14:09,482 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:14:09,482 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了微调大型语言模型（如LLaMa）的成本，以及提示调整作为一种降低成本的方法，但没有提供具体的成本比较，例如美元金额。...
2025-04-22 01:14:09,482 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:14:09,482 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:14:09,482 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:14:17,759 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:14:17,759 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:14:17,760 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:17,760 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:14:17,760 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:17,898 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:17,898 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:14:17,898 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:14:20,712 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:14:20,712 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:14:20,714 - app - INFO - qa:78 - Invoking RAG chain with question: According to Lecture 9, what are some limitations of standard Large Language Models that LLM agents aim to address?
2025-04-22 01:14:23,157 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:14:23,157 - app - INFO - qa:84 - Generated answer: According to Lecture 9, standard Large Language Models (LLMs) have the following limitations:

*   H...
2025-04-22 01:14:23,157 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:14:23,157 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:14:23,158 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:14:29,787 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:14:29,787 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:14:29,787 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:29,788 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:14:29,788 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:29,917 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:29,918 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:14:29,918 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:14:32,633 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:14:32,633 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:14:32,635 - app - INFO - qa:78 - Invoking RAG chain with question: What are the three core capabilities or components of an LLM agent discussed in Lecture 9 (Reasoning, Tool Learning, Knowledge Incorporation/RAG)?
2025-04-22 01:14:35,223 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:14:35,223 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Lecture 9 讨论的 LLM agent 的三个核心能力或组件是：

1. **Reasoning**
2. **Tool learning**
3. **Knowledge i...
2025-04-22 01:14:35,223 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:14:35,223 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:14:35,223 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:14:41,682 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:14:41,683 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:14:41,683 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:41,683 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:14:41,683 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:41,816 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:41,816 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:14:41,816 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:14:44,523 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:14:44,523 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:14:44,525 - app - INFO - qa:78 - Invoking RAG chain with question: Provide a high-level summary of how LLM agents utilize external tools, based on the concepts presented in Lecture 9.
2025-04-22 01:14:47,228 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:14:47,228 - app - INFO - qa:84 - Generated answer: Based on Lecture 9, LLM agents utilize external tools by **grounding language models into executable...
2025-04-22 01:14:47,228 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:14:47,228 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:14:47,228 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:14:55,285 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:14:55,286 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:14:55,286 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:55,286 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:14:55,286 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:55,415 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:14:55,415 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:14:55,416 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:14:58,171 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:14:58,171 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:14:58,173 - app - INFO - qa:78 - Invoking RAG chain with question: Contrast the role of internal reasoning/planning and external tool use within LLM agents as presented in Lecture 9.
2025-04-22 01:15:01,838 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:15:01,839 - app - INFO - qa:84 - Generated answer: According to the provided document, Lecture 9 presents LLM agents as utilizing both internal reasoni...
2025-04-22 01:15:01,839 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:15:01,839 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:15:01,839 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:15:10,608 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:15:10,609 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:15:10,609 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:15:10,609 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:15:10,609 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:15:10,733 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:15:10,734 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:15:10,734 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:15:13,747 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:15:13,747 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:15:13,749 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 9 provide specific details about the algorithms or neural network architecture used within the AlphaGo agent?
2025-04-22 01:15:16,040 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:15:16,040 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档只列出了关于“agents”、“agent examples”和“LLM agents”的主题，并没有关于AlphaGo的具体细节，例如其算法或神经网...
2025-04-22 01:15:16,040 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:15:16,041 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:15:16,041 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:15:21,877 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:15:21,879 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:15:21,880 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:21,880 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:15:21,880 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:22,077 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:22,077 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:15:22,078 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:15:25,601 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:15:25,601 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:15:25,603 - app - INFO - qa:78 - Invoking RAG chain with question: What are the three main parallelization strategies discussed in Lecture 10 for efficiently training Large Language Models?
2025-04-22 01:15:27,941 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:15:27,941 - app - INFO - qa:84 - Generated answer: According to the provided outline, the three main parallelization strategies discussed in Lecture 10...
2025-04-22 01:15:27,941 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:15:27,942 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:15:27,942 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:15:33,414 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:15:33,414 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:15:33,414 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:33,415 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:15:33,415 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:33,713 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:33,713 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:15:33,714 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:15:36,447 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:15:36,447 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:15:36,450 - app - INFO - qa:78 - Invoking RAG chain with question: What does LoRA stand for in the context of Parameter-Efficient Fine-Tuning (PEFT) as mentioned in Lecture 10?
2025-04-22 01:15:38,765 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:15:38,766 - app - INFO - qa:84 - Generated answer: I cannot find the answer to what LoRA stands for in the provided documents. The documents describe *...
2025-04-22 01:15:38,766 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:15:38,767 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:15:38,767 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:15:44,873 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:15:44,873 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:15:44,873 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:44,873 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:15:44,874 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:45,069 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:15:45,069 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:15:45,070 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:15:47,742 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:15:47,743 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:15:47,745 - app - INFO - qa:78 - Invoking RAG chain with question: Briefly explain the concept of Data Parallelism for training neural networks as described in Lecture 10.
2025-04-22 01:15:52,281 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:15:52,281 - app - INFO - qa:84 - Generated answer: 根据提供的文档，数据并行（DP）是指将数据集分割成多个分片，每个分片分配给一个设备。每个设备都持有模型的一个完整副本，并在分配给它的数据集分片上进行训练。 [文档10]

参考文献：
* Huang,...
2025-04-22 01:15:52,281 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:15:52,281 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:15:52,282 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:16:00,264 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:16:00,264 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:16:00,264 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:00,264 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:16:00,265 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:00,456 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:00,456 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:16:00,457 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:16:03,250 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:16:03,251 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:16:03,253 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the descriptions in Lecture 10, what is the fundamental difference between Model Parallelism (or Tensor Parallelism) and Pipeline Parallelism?
2025-04-22 01:16:06,186 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:16:06,186 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Model Parallelism (MP) 通过垂直划分的方式，将模型层组分布在多个GPU上，而Tensor Parallelism (TP) 则将矩阵乘法分解到多个GPU上。此外，...
2025-04-22 01:16:06,187 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:16:06,187 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:16:06,187 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:16:13,581 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:16:13,581 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:16:13,581 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:13,582 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:16:13,582 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:13,775 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:16:13,776 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:16:13,777 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:16:16,656 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:16:16,656 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:16:16,658 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 10 provide specific benchmark results comparing the training speed improvement of ZeRO versus Megatron-LM on a defined hardware configuration like A100 GPUs?
2025-04-22 01:16:20,044 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:16:20,044 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了 Megatron-LM 的性能（scaling efficiency 和在测试集上的结果），以及 ZeRO 和 Megatron-LM 的一些优缺点...
2025-04-22 01:16:20,044 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:16:20,044 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:16:20,044 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:16:29,539 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:16:29,539 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:16:29,540 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:29,540 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:16:29,540 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:29,610 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:29,611 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:16:29,611 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:16:33,210 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:16:33,210 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:16:33,212 - app - INFO - qa:78 - Invoking RAG chain with question: What does RAG stand for in the context of Large Language Models, according to Lecture 11?
2025-04-22 01:16:35,406 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:16:35,406 - app - INFO - qa:84 - Generated answer: According to Lecture 11, RAG stands for Retrieval-Augmented Generation. However, the provided docume...
2025-04-22 01:16:35,407 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:16:35,407 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:16:35,407 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:16:42,385 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:16:42,385 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:16:42,386 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:42,386 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:16:42,386 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:42,456 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:42,456 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:16:42,456 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:16:45,281 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:16:45,281 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:16:45,283 - app - INFO - qa:78 - Invoking RAG chain with question: List three distinct reasons why Retrieval-Augmented Generation (RAG) is considered beneficial for LLMs, as mentioned in the Lecture 11 slides.
2025-04-22 01:16:48,118 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:16:48,119 - app - INFO - qa:84 - Generated answer: According to the provided Lecture 11 slides, RAG is beneficial for LLMs because it:

1.  Enhances LL...
2025-04-22 01:16:48,119 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:16:48,119 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:16:48,119 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:16:54,237 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:16:54,238 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:16:54,238 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:54,238 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:16:54,238 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:54,311 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:16:54,311 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:16:54,311 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:16:57,306 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:16:57,306 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:16:57,308 - app - INFO - qa:78 - Invoking RAG chain with question: Describe the basic workflow of a Naive RAG system (Query -> Retrieve -> Augment -> Generate) as depicted in Lecture 11.
2025-04-22 01:17:01,370 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:17:01,371 - app - INFO - qa:84 - Generated answer: 根据提供的文档，一个 Naive RAG 系统（Query -> Retrieve -> Augment -> Generate）的基本流程可以描述如下：

1. **Querying (Retrie...
2025-04-22 01:17:01,371 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:17:01,371 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:17:01,371 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:17:09,443 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:17:09,443 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:17:09,443 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:09,443 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:17:09,444 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:09,515 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:09,515 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:17:09,515 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:17:13,276 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:17:13,277 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:17:13,281 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the overview provided in Lecture 11, how does Advanced RAG generally differ from Naive RAG (e.g., involving optimization in indexing, pre/post-retrieval)?
2025-04-22 01:17:15,729 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:17:15,729 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档仅包含“Advanced RAG”、“Main issues in RAG – what/when/how”和“Naïve RAG”这些标题，没有关于...
2025-04-22 01:17:15,729 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:17:15,729 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:17:15,729 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:17:21,886 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:17:21,886 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:17:21,887 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:21,887 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:17:21,887 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:21,956 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:17:21,956 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:17:21,957 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:17:25,490 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:17:25,491 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:17:25,493 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 11 provide the specific mathematical formula or algorithm used by the RePLUG model for its retrieval mechanism?
2025-04-22 01:17:33,575 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:17:33,576 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档只列出了与检索增强语言模型相关的参考文献，并没有提供关于 RePLUG 模型检索机制的具体数学公式或算法的细节。...
2025-04-22 01:17:33,576 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:17:33,576 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:17:33,576 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:17:39,951 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:17:39,952 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:17:39,952 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:39,952 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:17:39,952 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:40,264 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:40,264 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:17:40,266 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:17:43,912 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:17:43,912 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:17:43,914 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main assessment components and their percentage weightings for the CS6493 course, as stated in the Lecture 12 review slides?
2025-04-22 01:17:46,851 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:17:46,851 - app - INFO - qa:84 - Generated answer: According to the provided document, the main assessment components and their percentage weightings f...
2025-04-22 01:17:46,852 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:17:46,852 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:17:46,852 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:17:52,259 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:17:52,259 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:17:52,260 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:52,260 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:17:52,260 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:52,722 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:17:52,722 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:17:52,723 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:17:55,854 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:17:55,854 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:17:55,856 - app - INFO - qa:78 - Invoking RAG chain with question: According to the Lecture 12 course review slides, what are the key NLP preprocessing steps mentioned (e.g., tokenization, normalization)?
2025-04-22 01:17:58,545 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:17:58,546 - app - INFO - qa:84 - Generated answer: According to the Lecture 12 course review slides, the key NLP preprocessing steps mentioned are:

* ...
2025-04-22 01:17:58,546 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:17:58,546 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:17:58,546 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:18:04,899 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:18:04,900 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:18:04,900 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:04,900 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:18:04,900 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:05,213 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:05,213 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:18:05,214 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:18:08,127 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:18:08,127 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:18:08,129 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main high-level topics covered in the CS6493 course, as outlined in the 'Bird's-eye view of this course' slide in Lecture 12.
2025-04-22 01:18:11,721 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:18:11,722 - app - INFO - qa:84 - Generated answer: 根据Lecture 12的“Bird’s-eye view of this course”幻灯片，CS6493课程涵盖的主要主题包括：

*   **基础知识 (Basics):** 语言学、语言模型...
2025-04-22 01:18:11,722 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:18:11,722 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:18:11,722 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:18:20,321 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:18:20,323 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:18:20,323 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:20,323 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:18:20,323 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:20,626 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:20,626 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:18:20,627 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:18:23,591 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:18:23,591 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:18:23,593 - app - INFO - qa:78 - Invoking RAG chain with question: What is the format of the final exam for CS6493 described in Lecture 12 (e.g., closed-book, question types)?
2025-04-22 01:18:26,934 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:18:26,935 - app - INFO - qa:84 - Generated answer: 根据提供的文档，CS6493的期末考试将是闭卷考试，不允许携带任何材料或电子设备，包括计算器[文档1]。考试范围涵盖所有讲座、辅导课和作业[文档1]。

考试将包含总共5道题[文档2]。其中约50%为...
2025-04-22 01:18:26,935 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:18:26,935 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:18:26,935 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:18:35,184 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:18:35,185 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:18:35,185 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:35,185 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:18:35,185 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:35,484 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:18:35,484 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:18:35,485 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:18:38,243 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:18:38,243 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:18:38,245 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 12 specify the exact date and time for the final exam review session mentioned?
2025-04-22 01:18:45,943 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:18:45,944 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。虽然Lecture 12提到了会安排一次Zoom office hour作为期末考试的答疑，但具体日期和时间将“稍后公布”([文档121])。...
2025-04-22 01:18:45,944 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:18:45,944 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:18:45,944 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:18:52,913 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:18:52,914 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:18:52,914 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:18:52,914 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:18:52,914 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:18:52,979 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:18:52,979 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:18:52,980 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:18:56,308 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:18:56,308 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:18:56,310 - app - INFO - qa:78 - Invoking RAG chain with question: What is the deadline for submitting the project report and source code for the CS6493 course, according to the instructions document?
2025-04-22 01:18:58,449 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:18:58,450 - app - INFO - qa:84 - Generated answer: According to the instructions document, the deadline for submitting the project report and source co...
2025-04-22 01:18:58,450 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:18:58,450 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:18:58,450 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:19:03,667 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:19:03,669 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:19:03,669 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:03,669 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:19:03,669 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:03,736 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:03,736 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:19:03,737 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:19:07,845 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:19:07,845 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:19:07,848 - app - INFO - qa:78 - Invoking RAG chain with question: What are the mandatory sections that the CS6493 project report must consist of?
2025-04-22 01:19:09,588 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:19:09,589 - app - INFO - qa:84 - Generated answer: The CS6493 project report must consist of the following sections: introduction, related work, method...
2025-04-22 01:19:09,589 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:19:09,589 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:19:09,589 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:19:15,001 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:19:15,002 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:19:15,002 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:15,002 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:19:15,002 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:15,068 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:15,068 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:19:15,069 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:19:17,979 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:19:17,979 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:19:17,981 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the core idea and requirements for project 'Topic 6: Open Your Mind' in the CS6493 project instructions.
2025-04-22 01:19:24,419 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:19:24,419 - app - INFO - qa:84 - Generated answer: The core idea of project 'Topic 6: Open Your Mind' (and all projects in CS6493) is to select a topic...
2025-04-22 01:19:24,419 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:19:24,419 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:19:24,419 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:19:33,252 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:19:33,252 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:19:33,253 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:33,253 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:19:33,253 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:33,314 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:33,314 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:19:33,315 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:19:36,072 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:19:36,072 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:19:36,074 - app - INFO - qa:78 - Invoking RAG chain with question: What is the maximum number of members allowed in a project group for CS6493?
2025-04-22 01:19:37,911 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:19:37,913 - app - INFO - qa:84 - Generated answer: 根据提供的文档，CS6493项目的团队成员数量为1-6人。[文档：CS6493: Natural Language Processing - Projects, Instruction 2]...
2025-04-22 01:19:37,913 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:19:37,913 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:19:37,913 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:19:43,081 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:19:43,081 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:19:43,082 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:43,082 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:19:43,082 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:43,150 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:19:43,150 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:19:43,151 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:19:45,881 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:19:45,881 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:19:45,883 - app - INFO - qa:78 - Invoking RAG chain with question: Does the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf specify which particular pre-trained LLMs (e.g., GPT-4, Llama 3) must be used for the experiments?
2025-04-22 01:19:49,965 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:19:49,966 - app - INFO - qa:84 - Generated answer: No, the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf ...
2025-04-22 01:19:49,966 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:19:49,966 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:19:49,966 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:19:58,549 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:19:58,550 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:19:58,550 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:19:58,550 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:19:58,550 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:19:58,677 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:19:58,677 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:19:58,678 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:20:01,916 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:20:01,916 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:20:01,918 - app - INFO - qa:78 - Invoking RAG chain with question: What are the two novel model architectures proposed in the word2vec paper (word2vec.pdf) for computing vector representations?
2025-04-22 01:20:04,324 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:20:04,324 - app - INFO - qa:84 - Generated answer: According to the provided document, many different types of models were proposed for estimating cont...
2025-04-22 01:20:04,324 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:20:04,324 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:20:04,325 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:20:11,015 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:20:11,016 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:20:11,016 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:11,016 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:20:11,016 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:11,250 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:11,250 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:20:11,251 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:20:14,051 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:20:14,052 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:20:14,054 - app - INFO - qa:78 - Invoking RAG chain with question: What task is primarily used in the word2vec paper (word2vec.pdf) to measure the quality of the learned word vector representations?
2025-04-22 01:20:15,954 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:20:15,954 - app - INFO - qa:84 - Generated answer: According to the provided document, the quality of the learned word vector representations was measu...
2025-04-22 01:20:15,954 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:20:15,954 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:20:15,955 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:20:23,034 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:20:23,035 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:20:23,035 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:23,035 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:20:23,035 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:23,158 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:23,159 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:20:23,159 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:20:26,325 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:20:26,325 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:20:26,327 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main contribution claimed by the authors in the abstract of the word2vec paper (word2vec.pdf).
2025-04-22 01:20:29,156 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:20:29,157 - app - INFO - qa:84 - Generated answer: According to the abstract of the word2vec paper, the main contribution is focusing on learning word ...
2025-04-22 01:20:29,157 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:20:29,157 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:20:29,157 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:20:37,610 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:20:37,611 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:20:37,611 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:37,611 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:20:37,611 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:37,726 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:37,726 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:20:37,727 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:20:40,608 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:20:40,608 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:20:40,611 - app - INFO - qa:78 - Invoking RAG chain with question: According to the introduction of the word2vec paper (word2vec.pdf), what is the N-gram model commonly used for?
2025-04-22 01:20:43,184 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:20:43,185 - app - INFO - qa:84 - Generated answer: According to the provided document, the weight matrix between the input and the projection layer is ...
2025-04-22 01:20:43,185 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:20:43,185 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:20:43,185 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:20:50,223 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:20:50,224 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:20:50,224 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:50,225 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:20:50,225 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:50,348 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:20:50,348 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:20:50,349 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:20:53,078 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:20:53,078 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:20:53,079 - app - INFO - qa:78 - Invoking RAG chain with question: What is the key difference highlighted in the abstract of word2vec.pdf between the proposed models and previous neural network approaches regarding computational cost and accuracy?
2025-04-22 01:20:57,652 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:20:57,652 - app - INFO - qa:84 - Generated answer: According to the document, the key difference highlighted is that the proposed models (CBOW and Skip...
2025-04-22 01:20:57,652 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:20:57,652 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:20:57,652 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:21:08,468 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:21:08,468 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:21:08,468 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:08,468 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:21:08,469 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:08,581 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:08,582 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:21:08,583 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:21:11,906 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:21:11,907 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:21:11,909 - app - INFO - qa:78 - Invoking RAG chain with question: What size of dataset is mentioned in the abstract of word2vec.pdf from which high-quality word vectors could be learned in less than a day?
2025-04-22 01:21:13,668 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:21:13,669 - app - INFO - qa:84 - Generated answer: According to the document, high-quality word vectors were learned from a Google News corpus containi...
2025-04-22 01:21:13,669 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:21:13,669 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:21:13,669 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:21:19,142 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:21:19,143 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:21:19,143 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:19,144 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:21:19,144 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:19,264 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:19,264 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:21:19,265 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:21:22,042 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:21:22,042 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:21:22,043 - app - INFO - qa:78 - Invoking RAG chain with question: Explain the primary motivation for developing continuous vector representations of words, as outlined in the introduction of word2vec.pdf.
2025-04-22 01:21:24,308 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:21:24,308 - app - INFO - qa:84 - Generated answer: According to the provided document, the primary motivation for developing continuous vector represen...
2025-04-22 01:21:24,308 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:21:24,308 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:21:24,308 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:21:31,736 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:21:31,736 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:21:31,736 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:31,736 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:21:31,736 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:31,965 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:31,965 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:21:31,966 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:21:34,823 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:21:34,823 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:21:34,825 - app - INFO - qa:78 - Invoking RAG chain with question: How does the word2vec paper (word2vec.pdf) state that many traditional NLP systems often treat words?
2025-04-22 01:21:37,071 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:21:37,072 - app - INFO - qa:84 - Generated answer: According to the word2vec paper (word2vec.pdf), many current NLP systems and techniques treat words ...
2025-04-22 01:21:37,073 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:21:37,073 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:21:37,073 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:21:43,793 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:21:43,793 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:21:43,794 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:43,794 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:21:43,794 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:43,922 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:43,922 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:21:43,923 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:21:46,692 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:21:46,693 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:21:46,695 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the conceptual difference between the Skip-gram and CBOW architectures as described in the word2vec paper (word2vec.pdf).
2025-04-22 01:21:49,566 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:21:49,567 - app - INFO - qa:84 - Generated answer: According to the provided document, the key conceptual difference between the CBOW and Skip-gram arc...
2025-04-22 01:21:49,567 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:21:49,567 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:21:49,567 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:21:56,477 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:21:56,477 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:21:56,477 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:56,478 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:21:56,480 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:56,613 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:21:56,613 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:21:56,614 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:21:59,360 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:21:59,360 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:21:59,362 - app - INFO - qa:78 - Invoking RAG chain with question: Does the word2vec paper (word2vec.pdf) provide the specific hyperparameter values (e.g., learning rate, window size) used for the hierarchical softmax optimization experiments?
2025-04-22 01:22:01,898 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:22:01,898 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了 hierarchical softmax 的使用以及它如何减少计算复杂度，但没有提供 hierarchical softmax 优化实验中使用的具体...
2025-04-22 01:22:01,898 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:22:01,898 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:22:01,898 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:22:08,946 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:22:08,946 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:22:08,946 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:08,946 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:22:08,946 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:17,063 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:17,063 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:22:17,113 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:22:30,296 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:22:30,296 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:22:30,303 - app - INFO - qa:78 - Invoking RAG chain with question: What are the full names of the two authors of the 'Speech and Language Processing' book (3rd Edition Draft, ed3book.pdf)?
2025-04-22 01:22:32,631 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:22:32,631 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了 James Baker 和 Frederick Jelinek，但没有提供他们的全名，也没有提及他们是“Speech and Language P...
2025-04-22 01:22:32,631 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:22:32,631 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:22:32,631 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:22:41,374 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:22:41,376 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:22:41,377 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:41,377 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:22:41,377 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:49,241 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:22:49,250 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:22:49,299 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:22:58,306 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:22:58,307 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:22:58,309 - app - INFO - qa:78 - Invoking RAG chain with question: According to the table of contents in ed3book.pdf, which chapter number covers N-gram Language Models?
2025-04-22 01:23:00,191 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:23:00,192 - app - INFO - qa:84 - Generated answer: According to the table of contents in ed3book.pdf, Chapter 3 covers N-gram Language Models [文档: 48 C...
2025-04-22 01:23:00,192 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:23:00,192 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:23:00,192 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:23:06,111 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:23:06,111 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:23:06,111 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:06,112 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:23:06,112 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:14,078 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:14,078 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:23:14,129 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:23:22,088 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:23:22,089 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:23:22,092 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main topics covered in Part I, 'Fundamental Algorithms for NLP', based on the chapter titles listed in the table of contents of ed3book.pdf.
2025-04-22 01:23:26,447 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:23:26,448 - app - INFO - qa:84 - Generated answer: Based on the table of contents from ed3book.pdf, Part I, 'Fundamental Algorithms for NLP', covers th...
2025-04-22 01:23:26,448 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:23:26,448 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:23:26,448 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:23:34,159 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:23:34,160 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:23:34,160 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:34,160 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:23:34,160 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:42,120 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:42,120 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:23:42,170 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:23:50,226 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:23:50,227 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:23:50,230 - app - INFO - qa:78 - Invoking RAG chain with question: Which chapter number in ed3book.pdf discusses The Transformer architecture?
2025-04-22 01:23:52,916 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:23:52,917 - app - INFO - qa:84 - Generated answer: According to the provided document, Chapter 9 discusses The Transformer architecture [ed3book.pdf]. ...
2025-04-22 01:23:52,917 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:23:52,917 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:23:52,917 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:23:59,016 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:23:59,017 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:23:59,018 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:23:59,018 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:23:59,018 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:07,074 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:07,074 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:24:07,124 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:24:14,769 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:24:14,770 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:24:14,773 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the chapter titles in the table of contents of ed3book.pdf, what distinguishes Chapter 10 (Large Language Models) from Chapter 11 (Masked Language Models)?
2025-04-22 01:24:19,056 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:24:19,056 - app - INFO - qa:84 - Generated answer: Based on the provided document, Chapter 10 (Large Language Models) is distinguished from Chapter 11 ...
2025-04-22 01:24:19,056 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:24:19,056 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:24:19,056 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:24:27,925 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:24:27,925 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:24:27,925 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:27,925 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:24:27,926 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:36,026 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:36,026 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:24:36,075 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:24:43,863 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:24:43,864 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:24:43,867 - app - INFO - qa:78 - Invoking RAG chain with question: What NLP application is the main focus of Chapter 14 in ed3book.pdf, according to the table of contents?
2025-04-22 01:24:46,162 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:24:46,163 - app - INFO - qa:84 - Generated answer: According to the provided table of contents, the main focus of Part II, which includes Chapter 14, i...
2025-04-22 01:24:46,163 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:24:46,163 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:24:46,163 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:24:53,172 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:24:53,172 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:24:53,172 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:24:53,173 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:24:53,173 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:01,181 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:01,182 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:25:01,231 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:25:08,804 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:25:08,805 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:25:08,808 - app - INFO - qa:78 - Invoking RAG chain with question: Provide a brief overview of the topics likely covered in Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings'.
2025-04-22 01:25:14,431 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:25:14,431 - app - INFO - qa:84 - Generated answer: Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings', likely covers the following topi...
2025-04-22 01:25:14,431 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:25:14,431 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:25:14,431 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:25:22,577 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:25:22,579 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:25:22,579 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:22,579 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:25:22,580 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:30,813 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:30,813 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:25:30,865 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:25:38,557 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:25:38,557 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:25:38,561 - app - INFO - qa:78 - Invoking RAG chain with question: What is the specific draft date mentioned on the title page of ed3book.pdf?
2025-04-22 01:25:40,612 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:25:40,612 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了文档日期为 July 2, 2007 (根据文档)，但没有提及 ed3book.pdf 标题页上的特定草稿日期。...
2025-04-22 01:25:40,612 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:25:40,612 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:25:40,612 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:25:48,855 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:25:48,857 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:25:48,857 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:48,857 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:25:48,858 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:57,200 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:25:57,200 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:25:57,250 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:26:06,289 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:26:06,290 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:26:06,294 - app - INFO - qa:78 - Invoking RAG chain with question: According to the index snippets provided for ed3book.pdf, compare the primary page number listed for the 'Viterbi algorithm' versus the start of the 'Transformer' chapter.
2025-04-22 01:26:08,732 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:26:08,732 - app - INFO - qa:84 - Generated answer: According to the provided index snippets from ed3book.pdf, the 'Viterbi algorithm' is discussed star...
2025-04-22 01:26:08,733 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:26:08,733 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:26:08,733 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:26:15,988 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:26:15,989 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:26:15,990 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:26:15,990 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:26:15,991 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:26:24,038 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:26:24,038 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:26:24,088 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:26:32,270 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:26:32,271 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:26:32,274 - app - INFO - qa:78 - Invoking RAG chain with question: Does the table of contents in ed3book.pdf list a specific, distinct chapter dedicated solely to the topic of 'Ethical Considerations in NLP'?
2025-04-22 01:26:34,555 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:26:34,555 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档内容是书籍的章节列表和索引的一部分，但没有明确列出关于“自然语言处理中的伦理考量”的独立章节。...
2025-04-22 01:26:34,556 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:26:34,556 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:26:34,556 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:26:41,362 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:26:41,363 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:26:41,364 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:41,365 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:26:41,365 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:41,550 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:41,550 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:26:41,551 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:26:44,572 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:26:44,573 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:26:44,574 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main steps involved in text preprocessing according to L1_Introduction.pdf?
2025-04-22 01:26:48,993 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:26:48,993 - app - INFO - qa:84 - Generated answer: According to L1_Introduction.pdf, the main steps involved in text preprocessing are:

1. **Transform...
2025-04-22 01:26:48,993 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:26:48,993 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:26:48,994 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:26:55,625 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:26:55,626 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:26:55,626 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:55,626 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:26:55,627 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:55,806 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:26:55,806 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:26:55,807 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:26:58,553 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:26:58,553 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:26:58,555 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the key challenges in NLP mentioned in L1_Introduction.pdf.
2025-04-22 01:27:01,318 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:27:01,318 - app - INFO - qa:84 - Generated answer: According to L1_Introduction.pdf, the key challenges in NLP are:

*   **Ambiguity:** Similar strings...
2025-04-22 01:27:01,318 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:27:01,318 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:27:01,318 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:27:07,347 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:27:07,347 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:27:07,347 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:07,348 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:27:07,348 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:07,671 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:07,671 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:27:07,672 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:27:10,774 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:27:10,774 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:27:10,776 - app - INFO - qa:78 - Invoking RAG chain with question: Compare Stemming and Lemmatization based on the descriptions in L1_Introduction.pdf.
2025-04-22 01:27:16,696 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:27:16,696 - app - INFO - qa:84 - Generated answer: Based on the provided documents, here's a comparison of stemming and lemmatization:

**Stemming:**

...
2025-04-22 01:27:16,696 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:27:16,696 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:27:16,697 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:27:25,732 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:27:25,733 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:27:25,733 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:25,733 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:27:25,734 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:25,918 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:25,918 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:27:25,919 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:27:28,701 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:27:28,701 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:27:28,703 - app - INFO - qa:78 - Invoking RAG chain with question: What is Byte Pair Encoding (BPE) as explained in the appendix of L1_Introduction.pdf?
2025-04-22 01:27:33,094 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:27:33,095 - app - INFO - qa:84 - Generated answer: According to the provided document, Byte Pair Encoding (BPE) is a word segmentation algorithm that i...
2025-04-22 01:27:33,095 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:27:33,095 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:27:33,095 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:27:41,125 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:27:41,125 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:27:41,126 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:41,126 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:27:41,126 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:41,312 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:27:41,312 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:27:41,313 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:27:44,545 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:27:44,546 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:27:44,547 - app - INFO - qa:78 - Invoking RAG chain with question: Does L1_Introduction.pdf provide specific Python code examples for implementing TF-IDF?
2025-04-22 01:27:47,109 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:27:47,110 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。该文档解释了 TF-IDF 的概念和计算方法，并提供了一些示例，但没有提供具体的 Python 代码示例。...
2025-04-22 01:27:47,110 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:27:47,110 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:27:47,110 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:27:55,378 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:27:55,379 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:27:55,379 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:27:55,380 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:27:55,380 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:27:55,533 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:27:55,533 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:27:55,534 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:27:58,341 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:27:58,342 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:27:58,344 - app - INFO - qa:78 - Invoking RAG chain with question: What is the definition of a language model according to L2_LanguageModel.pdf?
2025-04-22 01:28:02,968 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:28:02,968 - app - INFO - qa:84 - Generated answer: According to the provided document, a language model is defined in two ways:

1.  As assigning a pro...
2025-04-22 01:28:02,968 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:28:02,968 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:28:02,969 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:28:10,107 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:28:10,108 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:28:10,108 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:10,109 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:28:10,109 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:10,255 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:10,255 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:28:10,256 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:28:13,438 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:28:13,439 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:28:13,443 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the sparsity and storage problems associated with n-gram models as discussed in L2_LanguageModel.pdf.
2025-04-22 01:28:16,232 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:28:16,233 - app - INFO - qa:84 - Generated answer: According to the provided document, n-gram models face two main problems:

*   **Storage problems:**...
2025-04-22 01:28:16,233 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:28:16,233 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:28:16,233 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:28:22,800 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:28:22,800 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:28:22,800 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:22,800 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:28:22,801 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:22,948 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:22,948 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:28:22,949 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:28:25,675 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:28:25,675 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:28:25,677 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the fixed-window neural language model with the RNN-based language model based on L2_LanguageModel.pdf.
2025-04-22 01:28:31,355 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:28:31,355 - app - INFO - qa:84 - Generated answer: Based on the provided document, here's a comparison between the fixed-window neural language model a...
2025-04-22 01:28:31,356 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:28:31,356 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:28:31,356 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:28:39,123 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:28:39,124 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:28:39,131 - app - DEBUG - sources:28 - Retrieved 1 sources
2025-04-22 01:28:39,142 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:28:39,142 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:28:39,142 - app - DEBUG - sources:28 - Retrieved 1 sources
2025-04-22 01:28:39,294 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:28:39,294 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:28:39,294 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:39,294 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:28:39,294 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:39,436 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:39,436 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:28:39,437 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:28:42,267 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:28:42,267 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:28:42,269 - app - INFO - qa:78 - Invoking RAG chain with question: How is perplexity defined and used to evaluate language models in L2_LanguageModel.pdf?
2025-04-22 01:28:44,380 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:28:44,380 - app - INFO - qa:84 - Generated answer: 根据提供的文档，perplexity 是语言模型（LM）的标准评估指标[文档2]。perplexity 可以从交叉熵损失 𝐽𝐽(𝜃𝜃) 直接推导出来[文档2]。较低的 perplexity 更好[文档...
2025-04-22 01:28:44,381 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:28:44,381 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:28:44,381 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:28:52,076 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:28:52,077 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:28:52,077 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:52,077 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:28:52,077 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:52,360 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:28:52,361 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:28:52,361 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:28:55,100 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:28:55,100 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:28:55,102 - app - INFO - qa:78 - Invoking RAG chain with question: Does L2_LanguageModel.pdf provide a detailed mathematical derivation for the GRU update gates?
2025-04-22 01:28:56,842 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:28:56,842 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了 GRU，但没有提供 GRU 更新门的详细数学推导。...
2025-04-22 01:28:56,842 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:28:56,842 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:28:56,843 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:28:56,849 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:28:56,849 - app - DEBUG - file_storage:45 - Generated file path for source 6a1b0369-37a4-4e0b-8dda-78a57e1b18d8: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6a1b0369-37a4-4e0b-8dda-78a57e1b18d8.pdf
2025-04-22 01:28:56,849 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6a1b0369-37a4-4e0b-8dda-78a57e1b18d8.pdf
2025-04-22 01:28:56,849 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6a1b0369-37a4-4e0b-8dda-78a57e1b18d8.pdf']
2025-04-22 01:28:56,849 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6a1b0369-37a4-4e0b-8dda-78a57e1b18d8.pdf
2025-04-22 01:29:04,516 - app - DEBUG - tools:61 - Loaded 649 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6a1b0369-37a4-4e0b-8dda-78a57e1b18d8.pdf
2025-04-22 01:29:04,516 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:29:04,554 - app - INFO - tools:110 - Split 649 pages into 3469 chunks.
2025-04-22 01:29:14,023 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:29:14,024 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:29:14,030 - app - INFO - qa:78 - Invoking RAG chain with question: What is this source about?
2025-04-22 01:29:18,257 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:29:18,257 - app - INFO - qa:84 - Generated answer: This source is about the cryptographic core of the TLS protocol, specifically relating to TLS 1.3. I...
2025-04-22 01:29:18,257 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:29:18,257 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:29:18,257 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:29:18,261 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:29:18,261 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:29:18,261 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:18,261 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:29:18,262 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:18,414 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:18,415 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:29:18,415 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:29:21,509 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:29:21,509 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:29:21,511 - app - INFO - qa:78 - Invoking RAG chain with question: What is the Distributional Hypothesis as explained in L3_WordEmbedding.pdf?
2025-04-22 01:29:25,203 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:29:25,204 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Distributional Hypothesis 指的是：在相似语境中出现的词语往往具有相似的含义。这个假设由 J. R. Firth 在 1957 年提出，可以用“通过它所处的环境...
2025-04-22 01:29:25,206 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:29:25,206 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:29:25,206 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:29:35,252 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:29:35,253 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:29:35,254 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:35,254 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:29:35,254 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:35,428 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:35,428 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:29:35,429 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:29:38,925 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:29:38,925 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:29:38,928 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the Skip-gram and CBOW models within the Word2vec framework based on L3_WordEmbedding.pdf.
2025-04-22 01:29:40,794 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:29:40,794 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档仅提到了 Word2vec 包含 CBOW 和 skip-gram 模型，但没有对这两种模型进行比较。...
2025-04-22 01:29:40,794 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:29:40,795 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:29:40,795 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:29:47,192 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:29:47,193 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:29:47,193 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:47,193 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:29:47,194 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:47,354 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:29:47,354 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:29:47,355 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:29:50,392 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:29:50,392 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:29:50,395 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the Negative Sampling technique used to improve Word2vec training efficiency, as described in L3_WordEmbedding.pdf.
2025-04-22 01:29:52,847 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:29:52,847 - app - INFO - qa:84 - Generated answer: 根据提供的文档“L3_WordEmbedding.pdf”，Negative sampling 是一种用于提高 Word2vec 训练效率的技术。该文档将 Negative sampling 和 Hi...
2025-04-22 01:29:52,848 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:29:52,848 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:29:52,848 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:30:00,388 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:30:00,389 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:30:00,389 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:00,389 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:30:00,390 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:00,551 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:00,551 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:30:00,552 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:30:03,284 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:30:03,285 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:30:03,287 - app - INFO - qa:78 - Invoking RAG chain with question: What is the core idea behind GloVe embeddings according to L3_WordEmbedding.pdf?
2025-04-22 01:30:06,214 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:30:06,214 - app - INFO - qa:84 - Generated answer: According to L3_WordEmbedding.pdf, the core idea behind GloVe embeddings is to combine **global stat...
2025-04-22 01:30:06,215 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:30:06,215 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:30:06,215 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:30:10,686 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:30:10,687 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:30:10,691 - app - DEBUG - sources:28 - Retrieved 1 sources
2025-04-22 01:30:10,710 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:30:10,710 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:30:10,710 - app - DEBUG - sources:28 - Retrieved 1 sources
2025-04-22 01:30:14,425 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:30:14,425 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:30:14,425 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:14,425 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:30:14,425 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:14,759 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:30:14,760 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:30:14,760 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:30:17,711 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:30:17,711 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:30:17,713 - app - INFO - qa:78 - Invoking RAG chain with question: Does L3_WordEmbedding.pdf provide performance comparisons (e.g., accuracy scores on specific tasks) between Word2vec and GloVe?
2025-04-22 01:30:19,563 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:30:19,563 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了Word2vec和GloVe，但没有提供它们在特定任务上的性能比较（例如，准确率得分）。...
2025-04-22 01:30:19,563 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:30:19,563 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:30:19,563 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:30:19,565 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:30:19,565 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:30:19,566 - app - DEBUG - sources:28 - Retrieved 1 sources
2025-04-22 01:30:19,575 - app - INFO - sources:41 - API request: Upload source file: CS6493_projects_DG.pdf
2025-04-22 01:30:19,577 - app - DEBUG - sources:76 - Generated source ID: 5959f510-d524-4ca0-b3a2-426549322520
2025-04-22 01:30:19,577 - app - INFO - source:25 - Creating new source: CS6493_projects_DG.pdf
2025-04-22 01:30:19,582 - app - DEBUG - source:31 - Created source with ID: be525ecd-7971-457c-be39-b730bd79afca
2025-04-22 01:30:19,582 - app - DEBUG - sources:84 - Created source record with ID: be525ecd-7971-457c-be39-b730bd79afca
2025-04-22 01:30:19,582 - app - DEBUG - file_storage:45 - Generated file path for source be525ecd-7971-457c-be39-b730bd79afca: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:30:19,582 - app - DEBUG - file_storage:57 - File not found at /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf, checking for alternatives
2025-04-22 01:30:19,583 - app - DEBUG - sources:88 - File will be saved to: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:30:19,584 - app - DEBUG - sources:96 - Successfully saved file to disk: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:30:19,584 - app - DEBUG - sources:100 - Verified file exists at: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:30:19,584 - app - INFO - sources:117 - Successfully uploaded source: CS6493_projects_DG.pdf (ID: be525ecd-7971-457c-be39-b730bd79afca)
2025-04-22 01:30:19,588 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:30:19,588 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:30:19,588 - app - DEBUG - sources:28 - Retrieved 2 sources
2025-04-22 01:30:19,597 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:30:19,597 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:30:19,598 - app - DEBUG - sources:28 - Retrieved 2 sources
2025-04-22 01:30:26,022 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:30:26,023 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:26,023 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:26,023 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:30:26,024 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:26,252 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:26,252 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:30:26,253 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:30:29,061 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:30:29,061 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:30:29,063 - app - INFO - qa:78 - Invoking RAG chain with question: What are the Query, Key, and Value vectors used for in the attention mechanism described in L4_Transformer and pretraining-finetuning.pdf?
2025-04-22 01:30:33,949 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:30:33,950 - app - INFO - qa:84 - Generated answer: According to the provided documents, the Query, Key, and Value vectors are used as follows in the at...
2025-04-22 01:30:33,950 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:30:33,950 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:30:33,950 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:30:41,625 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:30:41,626 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:41,626 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:41,626 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:30:41,626 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:41,845 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:30:41,846 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:30:41,846 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:30:44,519 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:30:44,520 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:30:44,522 - app - INFO - qa:78 - Invoking RAG chain with question: Explain the concept of Multi-Head Self-Attention as presented in L4_Transformer and pretraining-finetuning.pdf.
2025-04-22 01:30:49,067 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:30:49,067 - app - INFO - qa:84 - Generated answer: According to the provided documents, Multi-Head Self-Attention is the main technique used in the Tra...
2025-04-22 01:30:49,067 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:30:49,068 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:30:49,068 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:00,382 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:00,383 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:00,383 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:00,383 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:31:00,384 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:00,608 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:00,608 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:00,609 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:31:03,566 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:03,566 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:03,568 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the pre-training objectives (MLM and NSP) used in BERT according to L4_Transformer and pretraining-finetuning.pdf.
2025-04-22 01:31:06,536 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:31:06,536 - app - INFO - qa:84 - Generated answer: According to the provided documents, BERT is pre-trained using two self-supervised tasks: Masked Lan...
2025-04-22 01:31:06,536 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:31:06,536 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:31:06,536 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:12,247 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:12,247 - app - DEBUG - file_storage:45 - Generated file path for source be525ecd-7971-457c-be39-b730bd79afca: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:12,247 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:12,247 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf']
2025-04-22 01:31:12,247 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:12,309 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:12,309 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:12,309 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:31:16,027 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:16,028 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:16,031 - app - INFO - qa:78 - Invoking RAG chain with question: What are the requirements of the project.
2025-04-22 01:31:20,380 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:31:20,380 - app - INFO - qa:84 - Generated answer: The project requirements are as follows:

*   **Report Structure:** The project report must include ...
2025-04-22 01:31:20,380 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:31:20,380 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:31:20,380 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:20,383 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:20,383 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:20,383 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:20,384 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:31:20,384 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:20,614 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:20,614 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:20,615 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:31:23,396 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:23,396 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:23,399 - app - INFO - qa:78 - Invoking RAG chain with question: How does the Transformer architecture incorporate positional information according to L4_Transformer and pretraining-finetuning.pdf?
2025-04-22 01:31:27,140 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:31:27,140 - app - INFO - qa:84 - Generated answer: According to the provided documents, the Transformer architecture incorporates positional informatio...
2025-04-22 01:31:27,140 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:31:27,141 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:31:27,141 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:36,475 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:36,475 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:36,476 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:36,477 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:31:36,477 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:36,808 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:31:36,808 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:36,809 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:31:39,522 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:39,522 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:39,524 - app - INFO - qa:78 - Invoking RAG chain with question: Does L4_Transformer and pretraining-finetuning.pdf provide the exact number of parameters for the different GPT versions (GPT-1, GPT-2, GPT-3)?
2025-04-22 01:31:41,454 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:31:41,455 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了GPT框架和预训练模型，但没有提供GPT-1、GPT-2和GPT-3的具体参数数量。...
2025-04-22 01:31:41,455 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:31:41,455 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:31:41,455 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:48,908 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:48,910 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:31:48,911 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:31:48,912 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:31:48,912 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:31:49,093 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:31:49,093 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:49,094 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:31:52,128 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:52,128 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:52,129 - app - INFO - qa:78 - Invoking RAG chain with question: What is the difference between NLU and NLG as defined in L5_NLU tasks.pdf?
2025-04-22 01:31:55,614 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:31:55,615 - app - INFO - qa:84 - Generated answer: According to the document, the difference between NLU and NLG is as follows:

* **NLU (Natural Langu...
2025-04-22 01:31:55,615 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:31:55,615 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:31:55,615 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:31:55,619 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:31:55,619 - app - DEBUG - file_storage:45 - Generated file path for source be525ecd-7971-457c-be39-b730bd79afca: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:55,619 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:55,619 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf']
2025-04-22 01:31:55,619 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:55,686 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 01:31:55,686 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:31:55,687 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:31:58,666 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:31:58,666 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:31:58,668 - app - INFO - qa:78 - Invoking RAG chain with question: What are the project's topics?
2025-04-22 01:32:01,832 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:01,833 - app - INFO - qa:84 - Generated answer: The project's topics are among the following 6 topics [文档: CS6493: Natural Language Processing - Pro...
2025-04-22 01:32:01,833 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:01,833 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:01,833 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:32:02,681 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:32:02,681 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:32:02,681 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:02,681 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:32:02,682 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:02,867 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:02,867 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:32:02,868 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:32:05,601 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:32:05,601 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:32:05,602 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the different types of text classification mentioned in L5_NLU tasks.pdf (e.g., binary, multi-class, multi-label, ordinal).
2025-04-22 01:32:09,116 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:09,117 - app - INFO - qa:84 - Generated answer: 根据提供的文档，L5_NLU tasks.pdf 提到了文本分类是一个将标签或标记分配给文本单元（如句子、查询、段落和文档）的经典自然语言处理 (NLP) 问题。文档中列举了以下文本分类任务示例：垃圾...
2025-04-22 01:32:09,119 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:09,119 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:09,119 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:32:16,388 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:32:16,389 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:32:16,389 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:16,390 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:32:16,390 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:16,568 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:16,568 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:32:16,569 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:32:19,754 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:32:19,754 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:32:19,757 - app - INFO - qa:78 - Invoking RAG chain with question: Compare IR-based QA and Knowledge-based QA approaches as described in L5_NLU tasks.pdf.
2025-04-22 01:32:23,629 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:23,630 - app - INFO - qa:84 - Generated answer: According to the document, here's a comparison of IR-based QA and Knowledge-based QA:

**IR-based QA...
2025-04-22 01:32:23,630 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:23,632 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:23,632 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:32:24,501 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:32:24,501 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:32:24,502 - app - DEBUG - sources:28 - Retrieved 2 sources
2025-04-22 01:32:24,510 - app - INFO - sources:41 - API request: Upload source file: frontend_modification.pdf
2025-04-22 01:32:24,511 - app - DEBUG - sources:76 - Generated source ID: e06e2ab5-c334-4a75-a9f1-63c448408aff
2025-04-22 01:32:24,511 - app - INFO - source:25 - Creating new source: frontend_modification.pdf
2025-04-22 01:32:24,514 - app - DEBUG - source:31 - Created source with ID: ebad72fe-bf25-4e65-8606-c28eb68f5041
2025-04-22 01:32:24,514 - app - DEBUG - sources:84 - Created source record with ID: ebad72fe-bf25-4e65-8606-c28eb68f5041
2025-04-22 01:32:24,514 - app - DEBUG - file_storage:45 - Generated file path for source ebad72fe-bf25-4e65-8606-c28eb68f5041: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:24,514 - app - DEBUG - file_storage:57 - File not found at /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf, checking for alternatives
2025-04-22 01:32:24,515 - app - DEBUG - sources:88 - File will be saved to: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:24,515 - app - DEBUG - sources:96 - Successfully saved file to disk: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:24,515 - app - DEBUG - sources:100 - Verified file exists at: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:24,515 - app - INFO - sources:117 - Successfully uploaded source: frontend_modification.pdf (ID: ebad72fe-bf25-4e65-8606-c28eb68f5041)
2025-04-22 01:32:24,517 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:32:24,518 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:32:24,518 - app - DEBUG - sources:28 - Retrieved 3 sources
2025-04-22 01:32:24,526 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 01:32:24,526 - app - DEBUG - source:18 - Getting all sources
2025-04-22 01:32:24,526 - app - DEBUG - sources:28 - Retrieved 3 sources
2025-04-22 01:32:31,185 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:32:31,186 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:32:31,186 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:31,186 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:32:31,186 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:31,362 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:31,362 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:32:31,363 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:32:38,384 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:32:38,385 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:32:38,387 - app - INFO - qa:78 - Invoking RAG chain with question: What are the key characteristics of the SQuAD 1.1 dataset according to L5_NLU tasks.pdf?
2025-04-22 01:32:41,616 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:41,617 - app - INFO - qa:84 - Generated answer: According to the provided document, SQuAD 1.1 consists of 100,000 questions that were combined with ...
2025-04-22 01:32:41,617 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:41,617 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:41,617 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:32:41,620 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:32:41,620 - app - DEBUG - file_storage:45 - Generated file path for source ebad72fe-bf25-4e65-8606-c28eb68f5041: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:41,620 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:41,620 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf']
2025-04-22 01:32:41,620 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:41,652 - app - DEBUG - tools:61 - Loaded 5 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ebad72fe-bf25-4e65-8606-c28eb68f5041.pdf
2025-04-22 01:32:41,652 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:32:41,652 - app - INFO - tools:110 - Split 5 pages into 5 chunks.
2025-04-22 01:32:44,341 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:32:44,341 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:32:44,343 - app - INFO - qa:78 - Invoking RAG chain with question: Please explain the modifications.
2025-04-22 01:32:55,210 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:55,212 - app - INFO - qa:84 - Generated answer: The modifications include changes to the main interface, chat interface, mindmap display, and saving...
2025-04-22 01:32:55,212 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:55,212 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:55,212 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:32:55,216 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 01:32:55,216 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:32:55,216 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:55,216 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:32:55,216 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:55,394 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:32:55,394 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:32:55,395 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:32:58,048 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 01:32:58,048 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:32:58,051 - app - INFO - qa:78 - Invoking RAG chain with question: Does L5_NLU tasks.pdf provide specific accuracy results for the BiDAF model on the CoLA dataset from GLUE?
2025-04-22 01:32:59,981 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:32:59,982 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了GLUE基准测试和BiDAF模型，但没有提供BiDAF模型在CoLA数据集上的具体准确性结果。...
2025-04-22 01:32:59,982 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:32:59,982 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:32:59,982 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:33:06,847 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:33:06,848 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:33:06,848 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:06,848 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:33:06,848 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:07,012 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:07,012 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:33:07,013 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:33:10,285 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:33:10,285 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:33:10,286 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:33:10,286 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:33:10,288 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main categories of NLG tasks mentioned in Lecture 6?
2025-04-22 01:33:13,112 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:33:13,113 - app - INFO - qa:84 - Generated answer: 根据提供的文档“Lecture 6: NLP tasks (2)”，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation
*   Paraphra...
2025-04-22 01:33:13,113 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:33:13,113 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:33:13,114 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:33:20,449 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:33:20,449 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:33:20,449 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:20,449 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:33:20,450 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:20,789 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:20,789 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:33:20,790 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:33:23,788 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:33:23,789 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:33:23,789 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:33:23,790 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:33:23,792 - app - INFO - qa:78 - Invoking RAG chain with question: According to the slides for Lecture 6, what were the three main eras of machine translation development discussed (rule-based, example-based, statistical)?
2025-04-22 01:33:26,057 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:33:26,057 - app - INFO - qa:84 - Generated answer: According to the slides, the three main eras of machine translation development discussed were: rule...
2025-04-22 01:33:26,057 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:33:26,058 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:33:26,058 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:33:33,634 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:33:33,634 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:33:33,635 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:33,635 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:33:33,636 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:33,799 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:33,799 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:33:33,799 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:33:37,425 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:33:37,425 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:33:37,425 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:33:37,425 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:33:37,427 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the key components or steps involved in a task-oriented dialogue system as presented in Lecture 6.
2025-04-22 01:33:40,720 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:33:40,721 - app - INFO - qa:84 - Generated answer: According to the document, a task-oriented dialogue system involves the following key components or ...
2025-04-22 01:33:40,721 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:33:40,721 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:33:40,721 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:33:47,983 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:33:47,984 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:33:47,984 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:47,984 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:33:47,984 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:48,201 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:33:48,201 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:33:48,202 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:33:50,936 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:33:50,936 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:33:50,936 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:33:50,936 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:33:50,939 - app - INFO - qa:78 - Invoking RAG chain with question: Compare ranking models and generative models for dialogue systems based on the evaluation discussion in Lecture 6.
2025-04-22 01:33:55,218 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:33:55,218 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Lecture 6 讨论了 ranking 模型和 generative 模型，但没有提供关于它们之间比较的详细信息。

文档提到：

*   **Generative 模型** (例...
2025-04-22 01:33:55,218 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:33:55,218 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:33:55,219 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:34:04,451 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:34:04,451 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L6_NLG tasks.pdf
2025-04-22 01:34:04,452 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:34:04,452 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf']
2025-04-22 01:34:04,452 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:34:04,619 - app - DEBUG - tools:61 - Loaded 64 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L6_NLG tasks.pdf
2025-04-22 01:34:04,619 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:34:04,620 - app - INFO - tools:110 - Split 64 pages into 70 chunks.
2025-04-22 01:34:07,742 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:34:07,742 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:34:07,743 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:34:07,743 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:34:07,745 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 6 provide detailed Python code examples for implementing the PERSONACHAT model?
2025-04-22 01:34:10,266 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:34:10,266 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了可以使用预训练模型（如BERT、GPT、DialoGPT、DialogBERT）来实现对话系统，但没有说明Lecture 6是否提供了PERSONAC...
2025-04-22 01:34:10,266 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:34:10,266 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:34:10,266 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:34:18,921 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:34:18,921 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:34:18,923 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:18,923 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:34:18,924 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:19,006 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:19,006 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:34:19,007 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:34:21,635 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:34:21,636 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:34:21,636 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:34:21,636 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:34:21,638 - app - INFO - qa:78 - Invoking RAG chain with question: What does RLHF stand for, as explained in Lecture 7 on Large Language Models?
2025-04-22 01:34:24,116 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:34:24,117 - app - INFO - qa:84 - Generated answer: According to the provided document, RLHF is listed as a topic covered in Lecture 7 on Large Language...
2025-04-22 01:34:24,117 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:34:24,117 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:34:24,117 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:34:31,217 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:34:31,218 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:34:31,218 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:31,218 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:34:31,219 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:31,292 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:31,292 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:34:31,293 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:34:34,361 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:34:34,361 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:34:34,361 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:34:34,361 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:34:34,363 - app - INFO - qa:78 - Invoking RAG chain with question: What are the two main components of LLM alignment discussed in Lecture 7 (Instruction Tuning and RLHF)?
2025-04-22 01:34:37,688 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:34:37,688 - app - INFO - qa:84 - Generated answer: According to the provided document, the two main components of LLM alignment discussed in Lecture 7 ...
2025-04-22 01:34:37,688 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:34:37,689 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:34:37,689 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:34:45,599 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:34:45,600 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:34:45,600 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:45,600 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:34:45,600 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:45,681 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:45,681 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:34:45,681 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:34:48,300 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:34:48,300 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:34:48,300 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:34:48,300 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:34:48,303 - app - INFO - qa:78 - Invoking RAG chain with question: Briefly explain the concept of 'emergent abilities' in Large Language Models according to Lecture 7.
2025-04-22 01:34:51,288 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:34:51,289 - app - INFO - qa:84 - Generated answer: According to Lecture 7, emergent abilities relate to the scaling law in large language models. Speci...
2025-04-22 01:34:51,289 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:34:51,289 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:34:51,289 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:34:58,772 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:34:58,773 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:34:58,773 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:58,774 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:34:58,774 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:58,855 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:34:58,855 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:34:58,855 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:35:01,623 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:35:01,623 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:35:01,623 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:35:01,623 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:35:01,625 - app - INFO - qa:78 - Invoking RAG chain with question: How does instruction finetuning differ from Reinforcement Learning from Human Feedback (RLHF) based on the descriptions in Lecture 7?
2025-04-22 01:35:04,580 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:35:04,581 - app - INFO - qa:84 - Generated answer: 根据提供的文档，instruction finetuning 涉及收集（指令，输出）对的例子，并在多个任务上微调一个语言模型 [FLAN-T5; Chung et al., 2022]。然后，在未见过...
2025-04-22 01:35:04,581 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:35:04,581 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:35:04,581 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:35:12,506 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:35:12,507 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L7_LLM.pdf
2025-04-22 01:35:12,508 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:35:12,508 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf']
2025-04-22 01:35:12,508 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:35:12,597 - app - DEBUG - tools:61 - Loaded 31 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L7_LLM.pdf
2025-04-22 01:35:12,597 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:35:12,598 - app - INFO - tools:110 - Split 31 pages into 31 chunks.
2025-04-22 01:35:15,785 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:35:15,785 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:35:15,786 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:35:15,786 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:35:15,789 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 7 specify the exact size of the dataset used for training the initial ChatGPT model released in November 2022?
2025-04-22 01:35:18,038 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:35:18,039 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。虽然文档提到了ChatGPT于2022年11月发布，并提到了其他大型语言模型的参数大小和训练数据量，但它没有明确说明用于训练初始ChatGPT模型的具体数据集大...
2025-04-22 01:35:18,039 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:35:18,039 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:35:18,039 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:35:26,298 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:35:26,299 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:35:26,300 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:26,300 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:35:26,301 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:26,480 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:26,480 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:35:26,481 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:35:29,388 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:35:29,388 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:35:29,389 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:35:29,389 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:35:29,390 - app - INFO - qa:78 - Invoking RAG chain with question: What is the main motivation cited in Lecture 8 for using prompt learning instead of full fine-tuning for large Pre-trained Language Models (PLMs)?
2025-04-22 01:35:32,199 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:35:32,201 - app - INFO - qa:84 - Generated answer: According to the document, the main motivation for using prompt learning instead of full fine-tuning...
2025-04-22 01:35:32,201 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:35:32,201 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:35:32,201 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:35:39,870 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:35:39,870 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:35:39,871 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:39,871 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:35:39,871 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:40,167 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:40,167 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:35:40,168 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:35:42,834 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:35:42,834 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:35:42,834 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:35:42,834 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:35:42,836 - app - INFO - qa:78 - Invoking RAG chain with question: What does 'Chain of Thought Prompting' aim to achieve according to Lecture 8?
2025-04-22 01:35:45,647 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:35:45,648 - app - INFO - qa:84 - Generated answer: According to Lecture 8, 'Chain of Thought Prompting' aims to elicit reasoning in large language mode...
2025-04-22 01:35:45,648 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:35:45,648 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:35:45,648 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:35:52,744 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:35:52,745 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:35:52,745 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:52,746 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:35:52,746 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:52,930 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:35:52,930 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:35:52,931 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:35:55,760 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:35:55,760 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:35:55,761 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:35:55,761 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:35:55,764 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the concept of P-tuning v2 as mentioned in Lecture 8.
2025-04-22 01:35:58,774 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:35:58,775 - app - INFO - qa:84 - Generated answer: According to the provided documents, P-tuning v2 is a prompt tuning method that "can be comparable t...
2025-04-22 01:35:58,776 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:35:58,777 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:35:58,777 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:36:07,030 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:36:07,031 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:36:07,032 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:07,032 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:36:07,032 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:07,214 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:07,214 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:36:07,215 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:36:10,406 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:36:10,406 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:36:10,406 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:36:10,406 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:36:10,409 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the context provided in Lecture 8, what is the main difference between zero-shot and few-shot prompting?
2025-04-22 01:36:12,322 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:36:12,322 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档提到了“few-shot”学习，但没有解释“zero-shot”学习，也没有比较这两种 prompting 方法的主要区别。...
2025-04-22 01:36:12,323 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:36:12,323 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:36:12,323 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:36:18,764 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:36:18,764 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L8_prompts_alignment.pdf
2025-04-22 01:36:18,765 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:18,765 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf']
2025-04-22 01:36:18,765 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:18,940 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L8_prompts_alignment.pdf
2025-04-22 01:36:18,940 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:36:18,941 - app - INFO - tools:110 - Split 61 pages into 81 chunks.
2025-04-22 01:36:22,429 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:36:22,430 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:36:22,430 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:36:22,430 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:36:22,432 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 8 provide a specific cost comparison, for instance in US dollars, between fine-tuning LLaMa and using prompt tuning techniques?
2025-04-22 01:36:24,625 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:36:24,626 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了微调大型语言模型（如LLaMa）的成本，以及提示调整作为一种降低成本的方法，但没有提供具体的成本比较，例如美元金额。...
2025-04-22 01:36:24,626 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:36:24,626 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:36:24,626 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:36:33,736 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:36:33,737 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:36:33,739 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:33,739 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:36:33,739 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:33,877 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:33,878 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:36:33,878 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:36:38,237 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:36:38,237 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:36:38,237 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:36:38,237 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:36:38,239 - app - INFO - qa:78 - Invoking RAG chain with question: According to Lecture 9, what are some limitations of standard Large Language Models that LLM agents aim to address?
2025-04-22 01:36:40,626 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:36:40,626 - app - INFO - qa:84 - Generated answer: According to Lecture 9, standard Large Language Models (LLMs) have the following limitations:

*   H...
2025-04-22 01:36:40,626 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:36:40,626 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:36:40,627 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:36:47,469 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:36:47,470 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:36:47,471 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:47,471 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:36:47,471 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:47,608 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:47,608 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:36:47,608 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:36:50,248 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:36:50,248 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:36:50,248 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:36:50,248 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:36:50,250 - app - INFO - qa:78 - Invoking RAG chain with question: What are the three core capabilities or components of an LLM agent discussed in Lecture 9 (Reasoning, Tool Learning, Knowledge Incorporation/RAG)?
2025-04-22 01:36:52,805 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:36:52,805 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Lecture 9 讨论的 LLM agent 的三个核心能力或组件是：

1. **Reasoning**
2. **Tool learning**
3. **Knowledge i...
2025-04-22 01:36:52,806 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:36:52,806 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:36:52,806 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:36:59,670 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:36:59,671 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:36:59,672 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:59,672 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:36:59,672 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:59,805 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:36:59,805 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:36:59,805 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:37:02,503 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:37:02,503 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:37:02,503 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:37:02,503 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:37:02,505 - app - INFO - qa:78 - Invoking RAG chain with question: Provide a high-level summary of how LLM agents utilize external tools, based on the concepts presented in Lecture 9.
2025-04-22 01:37:06,100 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:37:06,101 - app - INFO - qa:84 - Generated answer: Based on Lecture 9, LLM agents utilize external tools by **grounding language models into executable...
2025-04-22 01:37:06,101 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:37:06,101 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:37:06,101 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:37:15,337 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:37:15,337 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:37:15,337 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:15,338 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:37:15,338 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:15,588 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:15,588 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:37:15,588 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:37:18,450 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:37:18,451 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:37:18,451 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:37:18,451 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:37:18,453 - app - INFO - qa:78 - Invoking RAG chain with question: Contrast the role of internal reasoning/planning and external tool use within LLM agents as presented in Lecture 9.
2025-04-22 01:37:22,089 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:37:22,089 - app - INFO - qa:84 - Generated answer: According to the provided document, Lecture 9 presents LLM agents as utilizing both internal reasoni...
2025-04-22 01:37:22,090 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:37:22,090 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:37:22,090 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:37:29,664 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:37:29,664 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L9_LLMAgents.pdf
2025-04-22 01:37:29,664 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:29,665 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf']
2025-04-22 01:37:29,665 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:29,801 - app - DEBUG - tools:61 - Loaded 42 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L9_LLMAgents.pdf
2025-04-22 01:37:29,801 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:37:29,802 - app - INFO - tools:110 - Split 42 pages into 57 chunks.
2025-04-22 01:37:32,459 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:37:32,460 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:37:32,460 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:37:32,460 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:37:32,462 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 9 provide specific details about the algorithms or neural network architecture used within the AlphaGo agent?
2025-04-22 01:37:34,471 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:37:34,471 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档只列出了关于“agents”、“agent examples”和“LLM agents”的主题，并没有关于AlphaGo的具体细节，例如其算法或神经网...
2025-04-22 01:37:34,472 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:37:34,472 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:37:34,472 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:37:41,795 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:37:41,796 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:37:41,796 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:41,796 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:37:41,796 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:41,992 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:41,993 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:37:41,993 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:37:46,923 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:37:46,923 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:37:46,923 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:37:46,923 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:37:46,925 - app - INFO - qa:78 - Invoking RAG chain with question: What are the three main parallelization strategies discussed in Lecture 10 for efficiently training Large Language Models?
2025-04-22 01:37:49,321 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:37:49,321 - app - INFO - qa:84 - Generated answer: According to the provided outline, the three main parallelization strategies discussed in Lecture 10...
2025-04-22 01:37:49,322 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:37:49,322 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:37:49,322 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:37:55,343 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:37:55,343 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:37:55,344 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:55,344 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:37:55,344 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:55,545 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:37:55,545 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:37:55,546 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:37:58,392 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:37:58,392 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:37:58,392 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:37:58,392 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:37:58,395 - app - INFO - qa:78 - Invoking RAG chain with question: What does LoRA stand for in the context of Parameter-Efficient Fine-Tuning (PEFT) as mentioned in Lecture 10?
2025-04-22 01:38:00,547 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:38:00,548 - app - INFO - qa:84 - Generated answer: I cannot find the answer to what LoRA stands for in the provided documents. The documents describe *...
2025-04-22 01:38:00,548 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:38:00,548 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:38:00,548 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:38:06,859 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:38:06,860 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:38:06,860 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:06,861 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:38:06,861 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:07,058 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:07,058 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:38:07,058 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:38:09,829 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:38:09,829 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:38:09,829 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:38:09,829 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:38:09,832 - app - INFO - qa:78 - Invoking RAG chain with question: Briefly explain the concept of Data Parallelism for training neural networks as described in Lecture 10.
2025-04-22 01:38:14,397 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:38:14,398 - app - INFO - qa:84 - Generated answer: 根据提供的文档，数据并行（DP）是指将数据集分割成多个分片，每个分片分配给一个设备。每个设备都持有模型的一个完整副本，并在分配给它的数据集分片上进行训练。 [文档10]

参考文献：
* Huang,...
2025-04-22 01:38:14,398 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:38:14,398 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:38:14,399 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:38:23,853 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:38:23,853 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:38:23,854 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:23,854 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:38:23,854 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:24,050 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:24,050 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:38:24,050 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:38:28,038 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:38:28,038 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:38:28,039 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:38:28,039 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:38:28,041 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the descriptions in Lecture 10, what is the fundamental difference between Model Parallelism (or Tensor Parallelism) and Pipeline Parallelism?
2025-04-22 01:38:31,333 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:38:31,333 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Model Parallelism (MP) 通过垂直划分的方式，将模型层组分布在多个GPU上，而Tensor Parallelism (TP) 则将矩阵乘法分解到多个GPU上。此外，...
2025-04-22 01:38:31,333 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:38:31,333 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:38:31,334 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:38:40,514 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:38:40,515 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L10_Efficient Training of LLM.pdf
2025-04-22 01:38:40,516 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:40,517 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf']
2025-04-22 01:38:40,517 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:40,709 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L10_Efficient Training of LLM.pdf
2025-04-22 01:38:40,709 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:38:40,710 - app - INFO - tools:110 - Split 56 pages into 68 chunks.
2025-04-22 01:38:43,566 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:38:43,566 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:38:43,567 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:38:43,567 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:38:43,569 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 10 provide specific benchmark results comparing the training speed improvement of ZeRO versus Megatron-LM on a defined hardware configuration like A100 GPUs?
2025-04-22 01:38:46,217 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:38:46,219 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了 Megatron-LM 的性能（scaling efficiency 和在测试集上的结果），以及 ZeRO 和 Megatron-LM 的一些优缺点...
2025-04-22 01:38:46,219 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:38:46,219 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:38:46,219 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:38:55,290 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:38:55,290 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:38:55,291 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:38:55,291 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:38:55,291 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:38:55,364 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:38:55,364 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:38:55,364 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:38:58,381 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:38:58,381 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:38:58,382 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:38:58,382 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:38:58,383 - app - INFO - qa:78 - Invoking RAG chain with question: What does RAG stand for in the context of Large Language Models, according to Lecture 11?
2025-04-22 01:39:00,771 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:39:00,772 - app - INFO - qa:84 - Generated answer: According to Lecture 11, RAG stands for Retrieval-Augmented Generation. However, the provided docume...
2025-04-22 01:39:00,772 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:39:00,772 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:39:00,772 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:39:08,181 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:39:08,181 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:39:08,182 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:08,182 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:39:08,182 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:08,256 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:08,256 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:39:08,256 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:39:10,936 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:39:10,939 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:39:10,939 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:39:10,939 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:39:10,941 - app - INFO - qa:78 - Invoking RAG chain with question: List three distinct reasons why Retrieval-Augmented Generation (RAG) is considered beneficial for LLMs, as mentioned in the Lecture 11 slides.
2025-04-22 01:39:13,531 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:39:13,532 - app - INFO - qa:84 - Generated answer: According to the provided Lecture 11 slides, RAG is beneficial for LLMs because it:

1.  Enhances LL...
2025-04-22 01:39:13,532 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:39:13,532 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:39:13,532 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:39:21,686 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:39:21,687 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:39:21,689 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:21,689 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:39:21,689 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:21,910 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:21,910 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:39:21,910 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:39:25,303 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:39:25,303 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:39:25,304 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:39:25,304 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:39:25,306 - app - INFO - qa:78 - Invoking RAG chain with question: Describe the basic workflow of a Naive RAG system (Query -> Retrieve -> Augment -> Generate) as depicted in Lecture 11.
2025-04-22 01:39:29,249 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:39:29,249 - app - INFO - qa:84 - Generated answer: 根据提供的文档，一个 Naive RAG 系统（Query -> Retrieve -> Augment -> Generate）的基本流程可以描述如下：

1. **Querying (Retrie...
2025-04-22 01:39:29,249 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:39:29,250 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:39:29,250 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:39:37,576 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:39:37,577 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:39:37,577 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:37,577 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:39:37,577 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:37,649 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:37,649 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:39:37,649 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:39:41,060 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:39:41,060 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:39:41,060 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:39:41,060 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:39:41,062 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the overview provided in Lecture 11, how does Advanced RAG generally differ from Naive RAG (e.g., involving optimization in indexing, pre/post-retrieval)?
2025-04-22 01:39:43,525 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:39:43,525 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档仅包含“Advanced RAG”、“Main issues in RAG – what/when/how”和“Naïve RAG”这些标题，没有关于...
2025-04-22 01:39:43,525 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:39:43,525 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:39:43,525 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:39:50,470 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:39:50,470 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L11_RAG.pdf
2025-04-22 01:39:50,470 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:50,470 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf']
2025-04-22 01:39:50,470 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:50,545 - app - DEBUG - tools:61 - Loaded 24 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L11_RAG.pdf
2025-04-22 01:39:50,545 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:39:50,545 - app - INFO - tools:110 - Split 24 pages into 32 chunks.
2025-04-22 01:39:53,388 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:39:53,388 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:39:53,389 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:39:53,389 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:39:53,391 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 11 provide the specific mathematical formula or algorithm used by the RePLUG model for its retrieval mechanism?
2025-04-22 01:39:55,613 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:39:55,614 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档仅列出了与检索增强语言模型相关的参考文献，并没有提供 RePLUG 模型检索机制的具体数学公式或算法。...
2025-04-22 01:39:55,614 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:39:55,614 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:39:55,614 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:40:03,355 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:40:03,356 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:40:03,358 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:03,358 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:40:03,358 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:03,666 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:03,666 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:40:03,668 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:40:06,814 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:40:06,814 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:40:06,814 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:40:06,814 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:40:06,817 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main assessment components and their percentage weightings for the CS6493 course, as stated in the Lecture 12 review slides?
2025-04-22 01:40:09,599 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:40:09,599 - app - INFO - qa:84 - Generated answer: According to the provided document, the main assessment components and their percentage weightings f...
2025-04-22 01:40:09,600 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:40:09,600 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:40:09,600 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:40:16,017 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:40:16,017 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:40:16,018 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:16,018 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:40:16,018 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:16,321 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:16,321 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:40:16,323 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:40:19,273 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:40:19,273 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:40:19,273 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:40:19,273 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:40:19,276 - app - INFO - qa:78 - Invoking RAG chain with question: According to the Lecture 12 course review slides, what are the key NLP preprocessing steps mentioned (e.g., tokenization, normalization)?
2025-04-22 01:40:21,859 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:40:21,860 - app - INFO - qa:84 - Generated answer: According to the Lecture 12 course review slides, the key NLP preprocessing steps mentioned are:

* ...
2025-04-22 01:40:21,860 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:40:21,860 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:40:21,860 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:40:28,420 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:40:28,421 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:40:28,422 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:28,422 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:40:28,422 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:28,726 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:28,726 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:40:28,727 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:40:31,456 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:40:31,456 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:40:31,456 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:40:31,456 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:40:31,459 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main high-level topics covered in the CS6493 course, as outlined in the 'Bird's-eye view of this course' slide in Lecture 12.
2025-04-22 01:40:34,913 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:40:34,914 - app - INFO - qa:84 - Generated answer: 根据Lecture 12的“Bird’s-eye view of this course”幻灯片，CS6493课程涵盖的主要主题包括：

*   **基础知识 (Basics):** 语言学、语言模型...
2025-04-22 01:40:34,914 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:40:34,914 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:40:34,914 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:40:44,233 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:40:44,233 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:40:44,234 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:44,234 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:40:44,234 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:44,539 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:44,539 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:40:44,540 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:40:47,286 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:40:47,286 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:40:47,287 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:40:47,287 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:40:47,289 - app - INFO - qa:78 - Invoking RAG chain with question: What is the format of the final exam for CS6493 described in Lecture 12 (e.g., closed-book, question types)?
2025-04-22 01:40:50,446 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:40:50,446 - app - INFO - qa:84 - Generated answer: 根据提供的文档，CS6493的期末考试将是闭卷考试，不允许携带任何材料或电子设备，包括计算器[文档1]。考试范围涵盖所有讲座、辅导课和作业[文档1]。

考试将包含总共5道题[文档2]。其中约50%为...
2025-04-22 01:40:50,446 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:40:50,446 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:40:50,446 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:40:59,439 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:40:59,439 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L12_CourseReview.pdf
2025-04-22 01:40:59,441 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:59,441 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf']
2025-04-22 01:40:59,441 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:59,839 - app - DEBUG - tools:61 - Loaded 122 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L12_CourseReview.pdf
2025-04-22 01:40:59,839 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:40:59,840 - app - INFO - tools:110 - Split 122 pages into 136 chunks.
2025-04-22 01:41:02,584 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:41:02,585 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:41:02,585 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:41:02,585 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:41:02,590 - app - INFO - qa:78 - Invoking RAG chain with question: Does Lecture 12 specify the exact date and time for the final exam review session mentioned?
2025-04-22 01:41:05,673 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:41:05,674 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。虽然Lecture 12提到了将通过Zoom举行一次答疑时间（“We will hold an office hour via Zoom (to be anno...
2025-04-22 01:41:05,674 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:41:05,674 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:41:05,675 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:41:11,731 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:41:11,732 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:41:11,733 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:11,733 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:41:11,734 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:11,798 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:11,798 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:41:11,798 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:41:14,581 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:41:14,582 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:41:14,582 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:41:14,582 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:41:14,584 - app - INFO - qa:78 - Invoking RAG chain with question: What is the deadline for submitting the project report and source code for the CS6493 course, according to the instructions document?
2025-04-22 01:41:16,674 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:41:16,675 - app - INFO - qa:84 - Generated answer: According to the instructions document, the deadline for submitting the project report and source co...
2025-04-22 01:41:16,675 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:41:16,675 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:41:16,675 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:41:22,157 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:41:22,157 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:41:22,158 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:22,158 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:41:22,159 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:22,225 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:22,225 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:41:22,225 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:41:24,896 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:41:24,896 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:41:24,896 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:41:24,896 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:41:24,898 - app - INFO - qa:78 - Invoking RAG chain with question: What are the mandatory sections that the CS6493 project report must consist of?
2025-04-22 01:41:26,659 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:41:26,661 - app - INFO - qa:84 - Generated answer: The CS6493 project report must consist of the following sections: introduction, related work, method...
2025-04-22 01:41:26,661 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:41:26,661 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:41:26,661 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:41:32,118 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:41:32,119 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:41:32,119 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:32,119 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:41:32,119 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:32,187 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:32,187 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:41:32,188 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:41:35,543 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:41:35,543 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:41:35,543 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:41:35,543 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:41:35,545 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the core idea and requirements for project 'Topic 6: Open Your Mind' in the CS6493 project instructions.
2025-04-22 01:41:41,932 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:41:41,934 - app - INFO - qa:84 - Generated answer: The core idea of project 'Topic 6: Open Your Mind' (and all projects in CS6493) is to select a topic...
2025-04-22 01:41:41,934 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:41:41,934 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:41:41,934 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:41:51,773 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:41:51,773 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:41:51,774 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:51,774 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:41:51,774 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:51,842 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:41:51,842 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:41:51,842 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:41:54,853 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:41:54,854 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:41:54,854 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:41:54,854 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:41:54,856 - app - INFO - qa:78 - Invoking RAG chain with question: What is the maximum number of members allowed in a project group for CS6493?
2025-04-22 01:41:56,867 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:41:56,868 - app - INFO - qa:84 - Generated answer: 根据提供的文档，CS6493项目的团队成员数量为1-6人。[文档：CS6493: Natural Language Processing - Projects, Instruction 2]...
2025-04-22 01:41:56,868 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:41:56,868 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:41:56,868 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:42:03,710 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:42:03,711 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: CS6493_projects_DG.pdf
2025-04-22 01:42:03,711 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:42:03,712 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf']
2025-04-22 01:42:03,712 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:42:03,780 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/CS6493_projects_DG.pdf
2025-04-22 01:42:03,780 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:42:03,780 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 01:42:06,430 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:42:06,430 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:42:06,430 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:42:06,431 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:42:06,432 - app - INFO - qa:78 - Invoking RAG chain with question: Does the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf specify which particular pre-trained LLMs (e.g., GPT-4, Llama 3) must be used for the experiments?
2025-04-22 01:42:10,679 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:42:10,680 - app - INFO - qa:84 - Generated answer: No, the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf ...
2025-04-22 01:42:10,680 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:42:10,680 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:42:10,680 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:42:19,512 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:42:19,512 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:42:19,514 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:19,515 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:42:19,515 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:19,642 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:19,642 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:42:19,643 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:42:21,442 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:42:21,442 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:42:21,443 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:42:21,443 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:42:21,445 - app - INFO - qa:78 - Invoking RAG chain with question: What are the two novel model architectures proposed in the word2vec paper (word2vec.pdf) for computing vector representations?
2025-04-22 01:42:24,390 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:42:24,391 - app - INFO - qa:84 - Generated answer: According to the document, many different types of models were proposed for estimating continuous re...
2025-04-22 01:42:24,391 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:42:24,391 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:42:24,391 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:42:31,607 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:42:31,607 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:42:31,608 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:31,608 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:42:31,608 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:31,732 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:31,732 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:42:31,733 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:42:34,366 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:42:34,366 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:42:34,366 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:42:34,366 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:42:34,368 - app - INFO - qa:78 - Invoking RAG chain with question: What task is primarily used in the word2vec paper (word2vec.pdf) to measure the quality of the learned word vector representations?
2025-04-22 01:42:36,393 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:42:36,394 - app - INFO - qa:84 - Generated answer: According to the provided document, the quality of the learned word vector representations was measu...
2025-04-22 01:42:36,394 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:42:36,394 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:42:36,394 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:42:43,178 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:42:43,179 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:42:43,179 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:43,179 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:42:43,179 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:43,302 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:43,302 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:42:43,303 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:42:45,969 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:42:45,969 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:42:45,970 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:42:45,970 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:42:45,972 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main contribution claimed by the authors in the abstract of the word2vec paper (word2vec.pdf).
2025-04-22 01:42:48,919 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:42:48,920 - app - INFO - qa:84 - Generated answer: According to the abstract of the word2vec paper, the main contribution is focusing on learning word ...
2025-04-22 01:42:48,920 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:42:48,920 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:42:48,920 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:42:56,648 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:42:56,649 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:42:56,649 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:56,649 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:42:56,649 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:56,778 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:42:56,778 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:42:56,779 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:42:59,426 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:42:59,426 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:42:59,426 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:42:59,426 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:42:59,428 - app - INFO - qa:78 - Invoking RAG chain with question: According to the introduction of the word2vec paper (word2vec.pdf), what is the N-gram model commonly used for?
2025-04-22 01:43:02,290 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:43:02,291 - app - INFO - qa:84 - Generated answer: According to the provided document, the weight matrix between the input and the projection layer is ...
2025-04-22 01:43:02,291 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:43:02,291 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:43:02,291 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:43:09,074 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:43:09,075 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:43:09,075 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:09,075 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:43:09,075 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:09,231 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:09,231 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:43:09,232 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:43:12,028 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:43:12,029 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:43:12,029 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:43:12,029 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:43:12,032 - app - INFO - qa:78 - Invoking RAG chain with question: What is the key difference highlighted in the abstract of word2vec.pdf between the proposed models and previous neural network approaches regarding computational cost and accuracy?
2025-04-22 01:43:15,603 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:43:15,604 - app - INFO - qa:84 - Generated answer: According to the document, the key difference highlighted is that the proposed models (CBOW and Skip...
2025-04-22 01:43:15,604 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:43:15,604 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:43:15,604 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:43:24,373 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:43:24,373 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:43:24,373 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:24,374 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:43:24,374 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:24,608 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:24,608 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:43:24,609 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:43:27,982 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:43:27,983 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:43:27,983 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:43:27,983 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:43:27,986 - app - INFO - qa:78 - Invoking RAG chain with question: What size of dataset is mentioned in the abstract of word2vec.pdf from which high-quality word vectors could be learned in less than a day?
2025-04-22 01:43:29,809 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:43:29,810 - app - INFO - qa:84 - Generated answer: According to the document, high-quality word vectors were learned from a Google News corpus containi...
2025-04-22 01:43:29,810 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:43:29,810 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:43:29,810 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:43:36,547 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:43:36,547 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:43:36,547 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:36,547 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:43:36,548 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:36,676 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:36,677 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:43:36,678 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:43:39,407 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:43:39,407 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:43:39,407 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:43:39,407 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:43:39,409 - app - INFO - qa:78 - Invoking RAG chain with question: Explain the primary motivation for developing continuous vector representations of words, as outlined in the introduction of word2vec.pdf.
2025-04-22 01:43:41,711 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:43:41,712 - app - INFO - qa:84 - Generated answer: According to the provided document, the primary motivation for developing continuous vector represen...
2025-04-22 01:43:41,712 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:43:41,712 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:43:41,712 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:43:49,662 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:43:49,662 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:43:49,662 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:49,662 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:43:49,662 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:49,786 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:43:49,786 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:43:49,787 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:43:52,508 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:43:52,508 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:43:52,508 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:43:52,508 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:43:52,510 - app - INFO - qa:78 - Invoking RAG chain with question: How does the word2vec paper (word2vec.pdf) state that many traditional NLP systems often treat words?
2025-04-22 01:43:54,706 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:43:54,706 - app - INFO - qa:84 - Generated answer: According to the word2vec paper (word2vec.pdf), many current NLP systems and techniques treat words ...
2025-04-22 01:43:54,707 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:43:54,707 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:43:54,707 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:44:02,808 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:44:02,808 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:44:02,809 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:02,809 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:44:02,809 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:02,934 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:02,934 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:44:02,935 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:44:06,301 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:44:06,302 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:44:06,302 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:44:06,302 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:44:06,304 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the conceptual difference between the Skip-gram and CBOW architectures as described in the word2vec paper (word2vec.pdf).
2025-04-22 01:44:09,095 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:44:09,096 - app - INFO - qa:84 - Generated answer: According to the provided document, the key conceptual difference between the CBOW and Skip-gram arc...
2025-04-22 01:44:09,096 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:44:09,096 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:44:09,096 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:44:16,905 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:44:16,905 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: word2vec.pdf
2025-04-22 01:44:16,905 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:16,905 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf']
2025-04-22 01:44:16,905 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:17,029 - app - DEBUG - tools:61 - Loaded 12 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/word2vec.pdf
2025-04-22 01:44:17,029 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:44:17,030 - app - INFO - tools:110 - Split 12 pages into 92 chunks.
2025-04-22 01:44:19,793 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:44:19,793 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:44:19,793 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:44:19,793 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:44:19,795 - app - INFO - qa:78 - Invoking RAG chain with question: Does the word2vec paper (word2vec.pdf) provide the specific hyperparameter values (e.g., learning rate, window size) used for the hierarchical softmax optimization experiments?
2025-04-22 01:44:21,908 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:44:21,908 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了 hierarchical softmax 的使用以及它如何减少计算复杂度，但没有提供 hierarchical softmax 优化实验中使用的具体...
2025-04-22 01:44:21,909 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:44:21,909 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:44:21,909 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:44:30,835 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:44:30,836 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:44:30,838 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:44:30,838 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:44:30,838 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:44:38,689 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:44:38,690 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:44:38,739 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:44:46,840 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:44:46,841 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:44:46,841 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:44:46,841 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:44:46,844 - app - INFO - qa:78 - Invoking RAG chain with question: What are the full names of the two authors of the 'Speech and Language Processing' book (3rd Edition Draft, ed3book.pdf)?
2025-04-22 01:44:49,651 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:44:49,652 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了 James Baker 和 Frederick Jelinek，但没有提供他们的全名，也没有提及他们是“Speech and Language P...
2025-04-22 01:44:49,652 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:44:49,652 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:44:49,652 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:44:58,255 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:44:58,256 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:44:58,256 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:44:58,256 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:44:58,257 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:06,067 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:06,067 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:45:06,117 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:45:14,204 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:45:14,205 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:45:14,206 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:45:14,206 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:45:14,209 - app - INFO - qa:78 - Invoking RAG chain with question: According to the table of contents in ed3book.pdf, which chapter number covers N-gram Language Models?
2025-04-22 01:45:16,206 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:45:16,206 - app - INFO - qa:84 - Generated answer: According to the table of contents in ed3book.pdf, Chapter 3 covers N-gram Language Models [文档: 48 C...
2025-04-22 01:45:16,206 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:45:16,206 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:45:16,206 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:45:22,359 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:45:22,360 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:45:22,361 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:22,362 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:45:22,362 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:30,406 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:30,406 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:45:30,456 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:45:37,817 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:45:37,818 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:45:37,818 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:45:37,818 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:45:37,821 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the main topics covered in Part I, 'Fundamental Algorithms for NLP', based on the chapter titles listed in the table of contents of ed3book.pdf.
2025-04-22 01:45:41,788 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:45:41,789 - app - INFO - qa:84 - Generated answer: Based on the table of contents from ed3book.pdf, Part I, 'Fundamental Algorithms for NLP', covers th...
2025-04-22 01:45:41,789 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:45:41,789 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:45:41,789 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:45:48,829 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:45:48,829 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:45:48,830 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:48,830 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:45:48,830 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:56,673 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:45:56,673 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:45:56,723 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:46:04,322 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:46:04,323 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:46:04,324 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:46:04,324 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:46:04,326 - app - INFO - qa:78 - Invoking RAG chain with question: Which chapter number in ed3book.pdf discusses The Transformer architecture?
2025-04-22 01:46:07,198 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:46:07,198 - app - INFO - qa:84 - Generated answer: According to the provided document, Chapter 9 discusses The Transformer architecture [ed3book.pdf]. ...
2025-04-22 01:46:07,199 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:46:07,199 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:46:07,199 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:46:13,201 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:46:13,201 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:46:13,202 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:13,203 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:46:13,203 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:21,216 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:21,216 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:46:21,266 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:46:29,233 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:46:29,234 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:46:29,235 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:46:29,235 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:46:29,237 - app - INFO - qa:78 - Invoking RAG chain with question: Based on the chapter titles in the table of contents of ed3book.pdf, what distinguishes Chapter 10 (Large Language Models) from Chapter 11 (Masked Language Models)?
2025-04-22 01:46:32,406 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:46:32,406 - app - INFO - qa:84 - Generated answer: Based on the provided document, Chapter 10 (Large Language Models) is not explicitly described, but ...
2025-04-22 01:46:32,406 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:46:32,406 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:46:32,407 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:46:42,336 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:46:42,337 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:46:42,338 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:42,338 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:46:42,338 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:50,308 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:46:50,308 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:46:50,358 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:46:58,036 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:46:58,036 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:46:58,037 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:46:58,037 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:46:58,040 - app - INFO - qa:78 - Invoking RAG chain with question: What NLP application is the main focus of Chapter 14 in ed3book.pdf, according to the table of contents?
2025-04-22 01:47:00,336 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:47:00,336 - app - INFO - qa:84 - Generated answer: According to the provided table of contents, the main focus of Part II, which includes Chapter 14, i...
2025-04-22 01:47:00,336 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:47:00,336 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:47:00,336 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:47:07,221 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:47:07,222 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:47:07,223 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:07,223 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:47:07,224 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:15,301 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:15,302 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:47:15,354 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:47:23,162 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:47:23,162 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:47:23,163 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:47:23,163 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:47:23,169 - app - INFO - qa:78 - Invoking RAG chain with question: Provide a brief overview of the topics likely covered in Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings'.
2025-04-22 01:47:28,183 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:47:28,183 - app - INFO - qa:84 - Generated answer: Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings', likely covers the following topi...
2025-04-22 01:47:28,183 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:47:28,184 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:47:28,184 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:47:35,947 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:47:35,948 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:47:35,949 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:35,950 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:47:35,950 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:43,981 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:47:43,981 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:47:44,032 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:47:51,802 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:47:51,803 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:47:51,803 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:47:51,803 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:47:51,805 - app - INFO - qa:78 - Invoking RAG chain with question: What is the specific draft date mentioned on the title page of ed3book.pdf?
2025-04-22 01:47:53,949 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:47:53,949 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了文档日期为 July 2, 2007 (根据文档)，但没有提及 ed3book.pdf 标题页上的特定草稿日期。...
2025-04-22 01:47:53,949 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:47:53,950 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:47:53,950 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:48:01,475 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:48:01,476 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:48:01,476 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:01,477 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:48:01,477 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:09,464 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:09,465 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:48:09,515 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:48:17,679 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:48:17,680 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:48:17,680 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:48:17,680 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:48:17,682 - app - INFO - qa:78 - Invoking RAG chain with question: According to the index snippets provided for ed3book.pdf, compare the primary page number listed for the 'Viterbi algorithm' versus the start of the 'Transformer' chapter.
2025-04-22 01:48:19,990 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:48:19,990 - app - INFO - qa:84 - Generated answer: According to the provided index snippets from ed3book.pdf, the 'Viterbi algorithm' is discussed star...
2025-04-22 01:48:19,990 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:48:19,990 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:48:19,990 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:48:27,728 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:48:27,728 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: ed3book.pdf
2025-04-22 01:48:27,729 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:27,729 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf']
2025-04-22 01:48:27,729 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:35,808 - app - DEBUG - tools:61 - Loaded 599 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/ed3book.pdf
2025-04-22 01:48:35,808 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:48:35,860 - app - INFO - tools:110 - Split 599 pages into 4265 chunks.
2025-04-22 01:48:43,853 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:48:43,855 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:48:43,855 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:48:43,855 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:48:43,859 - app - INFO - qa:78 - Invoking RAG chain with question: Does the table of contents in ed3book.pdf list a specific, distinct chapter dedicated solely to the topic of 'Ethical Considerations in NLP'?
2025-04-22 01:48:46,306 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:48:46,307 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。提供的文档内容是关于自然语言处理（NLP）的章节列表和索引，但没有明确提到一个专门的章节是关于“NLP中的伦理考量”。...
2025-04-22 01:48:46,307 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:48:46,307 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:48:46,307 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:48:53,450 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:48:53,450 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:48:53,451 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:48:53,452 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:48:53,452 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:48:53,635 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:48:53,635 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:48:53,636 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:48:56,502 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:48:56,502 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:48:56,502 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:48:56,502 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:48:56,504 - app - INFO - qa:78 - Invoking RAG chain with question: What are the main steps involved in text preprocessing according to L1_Introduction.pdf?
2025-04-22 01:49:01,020 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:49:01,021 - app - INFO - qa:84 - Generated answer: According to L1_Introduction.pdf, the main steps involved in text preprocessing are:

1. **Transform...
2025-04-22 01:49:01,022 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:49:01,022 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:49:01,022 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:49:08,191 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:49:08,191 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:49:08,193 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:08,193 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:49:08,193 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:08,380 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:08,380 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:49:08,381 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:49:11,113 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:49:11,113 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:49:11,113 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:49:11,113 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:49:11,116 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the key challenges in NLP mentioned in L1_Introduction.pdf.
2025-04-22 01:49:13,777 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:49:13,777 - app - INFO - qa:84 - Generated answer: According to L1_Introduction.pdf, the key challenges in NLP are:

*   **Ambiguity:** Similar strings...
2025-04-22 01:49:13,777 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:49:13,777 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:49:13,777 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:49:20,365 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:49:20,366 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:49:20,366 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:20,366 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:49:20,366 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:20,549 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:20,549 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:49:20,550 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:49:25,776 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:49:25,777 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:49:25,777 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:49:25,777 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:49:25,780 - app - INFO - qa:78 - Invoking RAG chain with question: Compare Stemming and Lemmatization based on the descriptions in L1_Introduction.pdf.
2025-04-22 01:49:31,472 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:49:31,472 - app - INFO - qa:84 - Generated answer: Based on the provided documents, here's a comparison of stemming and lemmatization:

*   **Stemming:...
2025-04-22 01:49:31,472 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:49:31,472 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:49:31,473 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:49:40,165 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:49:40,165 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:49:40,166 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:40,166 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:49:40,166 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:40,353 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:40,353 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:49:40,354 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:49:43,274 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:49:43,275 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:49:43,275 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:49:43,276 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:49:43,277 - app - INFO - qa:78 - Invoking RAG chain with question: What is Byte Pair Encoding (BPE) as explained in the appendix of L1_Introduction.pdf?
2025-04-22 01:49:47,761 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:49:47,761 - app - INFO - qa:84 - Generated answer: According to the provided document, Byte Pair Encoding (BPE) is a word segmentation algorithm that i...
2025-04-22 01:49:47,761 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:49:47,762 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:49:47,762 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:49:56,169 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:49:56,169 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L1_Introduction.pdf
2025-04-22 01:49:56,171 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:56,171 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf']
2025-04-22 01:49:56,172 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:56,357 - app - DEBUG - tools:61 - Loaded 56 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L1_Introduction.pdf
2025-04-22 01:49:56,357 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:49:56,358 - app - INFO - tools:110 - Split 56 pages into 70 chunks.
2025-04-22 01:49:59,162 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:49:59,162 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:49:59,162 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:49:59,162 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:49:59,164 - app - INFO - qa:78 - Invoking RAG chain with question: Does L1_Introduction.pdf provide specific Python code examples for implementing TF-IDF?
2025-04-22 01:50:02,271 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:50:02,272 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。该文档解释了 TF-IDF 的概念和计算方法，并提供了一些示例，但没有提供具体的 Python 代码示例。...
2025-04-22 01:50:02,272 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:50:02,272 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:50:02,272 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:50:09,899 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:50:09,900 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:50:09,901 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:09,902 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:50:09,902 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:10,045 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:10,045 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:50:10,046 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:50:13,032 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:50:13,032 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:50:13,032 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:50:13,032 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:50:13,035 - app - INFO - qa:78 - Invoking RAG chain with question: What is the definition of a language model according to L2_LanguageModel.pdf?
2025-04-22 01:50:17,481 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:50:17,482 - app - INFO - qa:84 - Generated answer: According to the provided document, a language model is defined in two ways:

1.  As assigning a pro...
2025-04-22 01:50:17,482 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:50:17,482 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:50:17,482 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:50:24,853 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:50:24,853 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:50:24,854 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:24,854 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:50:24,854 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:25,175 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:25,175 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:50:25,176 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:50:28,003 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:50:28,003 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:50:28,003 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:50:28,003 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:50:28,006 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the sparsity and storage problems associated with n-gram models as discussed in L2_LanguageModel.pdf.
2025-04-22 01:50:30,779 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:50:30,780 - app - INFO - qa:84 - Generated answer: According to the provided document, n-gram models face two main problems:

*   **Storage problems:**...
2025-04-22 01:50:30,780 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:50:30,780 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:50:30,780 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:50:39,451 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:50:39,452 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:50:39,452 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:39,452 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:50:39,452 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:39,597 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:39,597 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:50:39,598 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:50:42,493 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:50:42,493 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:50:42,494 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:50:42,494 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:50:42,496 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the fixed-window neural language model with the RNN-based language model based on L2_LanguageModel.pdf.
2025-04-22 01:50:48,588 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:50:48,589 - app - INFO - qa:84 - Generated answer: Based on the provided document, here's a comparison between the fixed-window neural language model a...
2025-04-22 01:50:48,589 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:50:48,589 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:50:48,589 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:50:57,485 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:50:57,485 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:50:57,486 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:57,486 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:50:57,486 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:57,631 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:50:57,631 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:50:57,632 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:51:00,273 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:51:00,273 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:51:00,273 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:51:00,273 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:51:00,275 - app - INFO - qa:78 - Invoking RAG chain with question: How is perplexity defined and used to evaluate language models in L2_LanguageModel.pdf?
2025-04-22 01:51:03,103 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:51:03,104 - app - INFO - qa:84 - Generated answer: 根据提供的文档，perplexity 是语言模型（LM）的标准评估指标[文档2]。perplexity 可以从交叉熵损失 𝐽𝐽(𝜃𝜃) 直接推导出来[文档2]。较低的 perplexity 更好[文档...
2025-04-22 01:51:03,104 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:51:03,104 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:51:03,105 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:51:10,782 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:51:10,783 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L2_LanguageModel.pdf
2025-04-22 01:51:10,783 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:51:10,783 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf']
2025-04-22 01:51:10,783 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:51:10,948 - app - DEBUG - tools:61 - Loaded 45 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L2_LanguageModel.pdf
2025-04-22 01:51:10,948 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:51:10,949 - app - INFO - tools:110 - Split 45 pages into 50 chunks.
2025-04-22 01:51:13,717 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:51:13,718 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:51:13,718 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:51:13,718 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:51:13,720 - app - INFO - qa:78 - Invoking RAG chain with question: Does L2_LanguageModel.pdf provide a detailed mathematical derivation for the GRU update gates?
2025-04-22 01:51:15,549 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:51:15,550 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了 GRU，但没有提供 GRU 更新门的详细数学推导。...
2025-04-22 01:51:15,550 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:51:15,550 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:51:15,550 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:51:23,178 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:51:23,179 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:51:23,179 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:23,179 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:51:23,179 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:23,350 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:23,351 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:51:23,351 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:51:26,034 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:51:26,034 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:51:26,035 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:51:26,035 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:51:26,037 - app - INFO - qa:78 - Invoking RAG chain with question: What is the Distributional Hypothesis as explained in L3_WordEmbedding.pdf?
2025-04-22 01:51:29,628 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:51:29,629 - app - INFO - qa:84 - Generated answer: 根据提供的文档，Distributional Hypothesis 指的是：在相似语境中出现的词语往往具有相似的含义。这个假设由 J. R. Firth 在 1957 年提出，可以用“通过它所处的环境...
2025-04-22 01:51:29,629 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:51:29,629 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:51:29,629 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:51:39,272 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:51:39,272 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:51:39,273 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:39,273 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:51:39,273 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:39,437 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:39,437 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:51:39,438 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:51:42,124 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:51:42,124 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:51:42,125 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:51:42,125 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:51:42,128 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the Skip-gram and CBOW models within the Word2vec framework based on L3_WordEmbedding.pdf.
2025-04-22 01:51:44,651 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:51:44,652 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档仅提到了 Word2vec 包含 CBOW 和 skip-gram 模型，但没有对这两种模型进行比较。...
2025-04-22 01:51:44,652 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:51:44,652 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:51:44,652 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:51:51,454 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:51:51,455 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:51:51,457 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:51,457 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:51:51,457 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:51,727 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:51:51,728 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:51:51,728 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:51:54,881 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:51:54,881 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:51:54,881 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:51:54,881 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:51:54,884 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the Negative Sampling technique used to improve Word2vec training efficiency, as described in L3_WordEmbedding.pdf.
2025-04-22 01:51:57,239 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:51:57,239 - app - INFO - qa:84 - Generated answer: 根据提供的文档“L3_WordEmbedding.pdf”，Negative sampling 是一种用于提高 Word2vec 训练效率的技术。该文档将 Negative sampling 和 Hi...
2025-04-22 01:51:57,239 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:51:57,239 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:51:57,240 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:52:04,867 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:52:04,868 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:52:04,868 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:04,868 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:52:04,868 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:05,040 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:05,040 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:52:05,041 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:52:09,083 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:52:09,084 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:52:09,084 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:52:09,084 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:52:09,087 - app - INFO - qa:78 - Invoking RAG chain with question: What is the core idea behind GloVe embeddings according to L3_WordEmbedding.pdf?
2025-04-22 01:52:12,256 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:52:12,257 - app - INFO - qa:84 - Generated answer: According to L3_WordEmbedding.pdf, the core idea behind GloVe embeddings is to combine **global stat...
2025-04-22 01:52:12,257 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:52:12,257 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:52:12,257 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:52:20,788 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:52:20,789 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L3_WordEmbedding.pdf
2025-04-22 01:52:20,789 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:20,789 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf']
2025-04-22 01:52:20,789 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:20,963 - app - DEBUG - tools:61 - Loaded 61 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L3_WordEmbedding.pdf
2025-04-22 01:52:20,963 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:52:20,964 - app - INFO - tools:110 - Split 61 pages into 69 chunks.
2025-04-22 01:52:23,714 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:52:23,715 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:52:23,715 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:52:23,715 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:52:23,717 - app - INFO - qa:78 - Invoking RAG chain with question: Does L3_WordEmbedding.pdf provide performance comparisons (e.g., accuracy scores on specific tasks) between Word2vec and GloVe?
2025-04-22 01:52:25,493 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:52:25,494 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了Word2vec和GloVe，但没有提供它们在特定任务上的性能比较（例如，准确率）。...
2025-04-22 01:52:25,494 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:52:25,494 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:52:25,494 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:52:33,188 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:52:33,188 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:33,190 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:33,190 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:52:33,190 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:33,410 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:33,410 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:52:33,411 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:52:36,072 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:52:36,072 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:52:36,073 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:52:36,073 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:52:36,074 - app - INFO - qa:78 - Invoking RAG chain with question: What are the Query, Key, and Value vectors used for in the attention mechanism described in L4_Transformer and pretraining-finetuning.pdf?
2025-04-22 01:52:40,943 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:52:40,943 - app - INFO - qa:84 - Generated answer: According to the provided documents, the Query, Key, and Value vectors are used as follows in the at...
2025-04-22 01:52:40,944 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:52:40,944 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:52:40,944 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:52:50,679 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:52:50,680 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:50,680 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:50,680 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:52:50,680 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:50,898 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:52:50,898 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:52:50,899 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:52:53,523 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:52:53,523 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:52:53,523 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:52:53,524 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:52:53,526 - app - INFO - qa:78 - Invoking RAG chain with question: Explain the concept of Multi-Head Self-Attention as presented in L4_Transformer and pretraining-finetuning.pdf.
2025-04-22 01:52:57,488 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:52:57,488 - app - INFO - qa:84 - Generated answer: According to the provided documents, Multi-Head Self-Attention is the main technique used in the Tra...
2025-04-22 01:52:57,488 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:52:57,488 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:52:57,488 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:53:06,414 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:53:06,415 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:06,416 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:06,416 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:53:06,417 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:06,746 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:06,746 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:53:06,747 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:53:09,782 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:53:09,783 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:53:09,783 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:53:09,783 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:53:09,785 - app - INFO - qa:78 - Invoking RAG chain with question: Compare the pre-training objectives (MLM and NSP) used in BERT according to L4_Transformer and pretraining-finetuning.pdf.
2025-04-22 01:53:13,743 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:53:13,744 - app - INFO - qa:84 - Generated answer: According to the provided documents, BERT is pre-trained using two self-supervised tasks: Masked Lan...
2025-04-22 01:53:13,744 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:53:13,744 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:53:13,744 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:53:23,216 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:53:23,216 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:23,216 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:23,217 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:53:23,217 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:23,436 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:23,436 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:53:23,437 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:53:26,209 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:53:26,209 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:53:26,210 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:53:26,210 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:53:26,211 - app - INFO - qa:78 - Invoking RAG chain with question: How does the Transformer architecture incorporate positional information according to L4_Transformer and pretraining-finetuning.pdf?
2025-04-22 01:53:29,991 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:53:29,992 - app - INFO - qa:84 - Generated answer: According to the provided documents, the Transformer architecture incorporates positional informatio...
2025-04-22 01:53:29,992 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:53:29,993 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:53:29,993 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:53:39,266 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:53:39,266 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:39,266 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:39,266 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf']
2025-04-22 01:53:39,267 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:39,482 - app - DEBUG - tools:61 - Loaded 57 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L4_Transformer and pretraining-finetuning.pdf
2025-04-22 01:53:39,482 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:53:39,483 - app - INFO - tools:110 - Split 57 pages into 70 chunks.
2025-04-22 01:53:42,528 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:53:42,528 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:53:42,528 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:53:42,528 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:53:42,530 - app - INFO - qa:78 - Invoking RAG chain with question: Does L4_Transformer and pretraining-finetuning.pdf provide the exact number of parameters for the different GPT versions (GPT-1, GPT-2, GPT-3)?
2025-04-22 01:53:44,578 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:53:44,579 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档中提到了GPT框架和预训练模型，但没有提供GPT-1、GPT-2和GPT-3的具体参数数量。...
2025-04-22 01:53:44,579 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:53:44,579 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:53:44,579 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:53:52,616 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:53:52,616 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:53:52,616 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:53:52,616 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:53:52,616 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:53:52,796 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:53:52,796 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:53:52,797 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:53:55,478 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:53:55,479 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:53:55,479 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:53:55,479 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:53:55,481 - app - INFO - qa:78 - Invoking RAG chain with question: What is the difference between NLU and NLG as defined in L5_NLU tasks.pdf?
2025-04-22 01:53:58,772 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:53:58,772 - app - INFO - qa:84 - Generated answer: According to the document, the difference between NLU and NLG is as follows:

* **NLU (Natural Langu...
2025-04-22 01:53:58,772 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:53:58,773 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:53:58,773 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:54:07,035 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:54:07,035 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:54:07,035 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:07,036 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:54:07,036 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:07,210 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:07,210 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:54:07,211 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:54:10,615 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:54:10,616 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:54:10,616 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:54:10,616 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:54:10,618 - app - INFO - qa:78 - Invoking RAG chain with question: Summarize the different types of text classification mentioned in L5_NLU tasks.pdf (e.g., binary, multi-class, multi-label, ordinal).
2025-04-22 01:54:14,054 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:54:14,054 - app - INFO - qa:84 - Generated answer: 根据提供的文档，L5_NLU tasks.pdf 提到了文本分类是一个将标签或标记分配给文本单元（如句子、查询、段落和文档）的经典自然语言处理 (NLP) 问题。文档中列举了以下文本分类任务示例：垃圾...
2025-04-22 01:54:14,054 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:54:14,054 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:54:14,054 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:54:21,094 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:54:21,095 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:54:21,096 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:21,096 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:54:21,096 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:21,272 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:21,272 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:54:21,273 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:54:24,210 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:54:24,210 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:54:24,211 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:54:24,211 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:54:24,213 - app - INFO - qa:78 - Invoking RAG chain with question: Compare IR-based QA and Knowledge-based QA approaches as described in L5_NLU tasks.pdf.
2025-04-22 01:54:28,281 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:54:28,282 - app - INFO - qa:84 - Generated answer: According to the document, here's a comparison of IR-based QA and Knowledge-based QA:

**IR-based QA...
2025-04-22 01:54:28,282 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:54:28,282 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:54:28,282 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:54:35,953 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:54:35,953 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:54:35,953 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:35,953 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:54:35,953 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:36,129 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:36,130 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:54:36,130 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:54:39,150 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:54:39,151 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:54:39,151 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:54:39,151 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:54:39,153 - app - INFO - qa:78 - Invoking RAG chain with question: What are the key characteristics of the SQuAD 1.1 dataset according to L5_NLU tasks.pdf?
2025-04-22 01:54:42,438 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:54:42,438 - app - INFO - qa:84 - Generated answer: According to the provided document, SQuAD 1.1 consists of 100,000 questions that were combined with ...
2025-04-22 01:54:42,439 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:54:42,439 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:54:42,439 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 01:54:52,293 - app - INFO - qa:36 - Received QA request with 1 sources and model llama4
2025-04-22 01:54:52,293 - app - DEBUG - file_storage:41 - Using source ID with existing PDF extension: L5_NLU tasks.pdf
2025-04-22 01:54:52,295 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:52,295 - app - INFO - qa:74 - Creating RAG chain with model llama4 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf']
2025-04-22 01:54:52,295 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:52,467 - app - DEBUG - tools:61 - Loaded 63 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/L5_NLU tasks.pdf
2025-04-22 01:54:52,467 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 01:54:52,468 - app - INFO - tools:110 - Split 63 pages into 85 chunks.
2025-04-22 01:54:55,121 - app - INFO - llm_config:23 - Initializing LLM with model: llama4
2025-04-22 01:54:55,121 - app - INFO - llm_config:29 - Using Llama 4 model
2025-04-22 01:54:55,121 - app - ERROR - llm_config:38 - Failed to initialize Llama 4 model: Could not import text_generation python package. Please install it with `pip install text_generation`.. Falling back to Gemma 3.
2025-04-22 01:54:55,121 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 01:54:55,123 - app - INFO - qa:78 - Invoking RAG chain with question: Does L5_NLU tasks.pdf provide specific accuracy results for the BiDAF model on the CoLA dataset from GLUE?
2025-04-22 01:54:57,206 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 01:54:57,207 - app - INFO - qa:84 - Generated answer: 我无法从提供的文档中找到这个问题的答案。文档提到了GLUE基准测试和BiDAF模型，但没有提供BiDAF模型在CoLA数据集上的具体准确性结果。...
2025-04-22 01:54:57,207 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 01:54:57,207 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 01:54:57,207 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 03:40:37,683 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 03:40:37,684 - app - DEBUG - source:18 - Getting all sources
2025-04-22 03:40:37,684 - app - DEBUG - sources:28 - Retrieved 3 sources
2025-04-22 03:40:37,704 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 03:40:37,704 - app - DEBUG - source:18 - Getting all sources
2025-04-22 03:40:37,704 - app - DEBUG - sources:28 - Retrieved 3 sources
2025-04-22 03:40:47,741 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 03:40:47,742 - app - DEBUG - file_storage:45 - Generated file path for source be525ecd-7971-457c-be39-b730bd79afca: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 03:40:47,742 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 03:40:47,742 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf']
2025-04-22 03:40:47,742 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 03:40:47,816 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 03:40:47,816 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 03:40:47,816 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 03:40:51,749 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 03:40:51,749 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 03:40:51,757 - app - INFO - qa:78 - Invoking RAG chain with question: What is this source about
2025-04-22 03:40:55,585 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 03:40:55,585 - app - INFO - qa:84 - Generated answer: This source is about the instructions for a group project in the CS6493: Natural Language Processing...
2025-04-22 03:40:55,585 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 03:40:55,585 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 03:40:55,585 - app - INFO - qa:114 - Generated answer with 1 unique source references
2025-04-22 04:06:49,206 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 04:06:49,211 - app - DEBUG - source:18 - Getting all sources
2025-04-22 04:06:49,218 - app - DEBUG - sources:28 - Retrieved 3 sources
2025-04-22 04:06:49,227 - app - INFO - sources:41 - API request: Upload source file: NLP_project_slides_v1.pdf
2025-04-22 04:06:49,228 - app - DEBUG - sources:76 - Generated source ID: cfbda31d-fb56-4c96-bf36-0acb32a3ea3d
2025-04-22 04:06:49,228 - app - INFO - source:25 - Creating new source: NLP_project_slides_v1.pdf
2025-04-22 04:06:49,231 - app - DEBUG - source:31 - Created source with ID: 6803811a-60d0-4f7e-be58-33705189e6b3
2025-04-22 04:06:49,231 - app - DEBUG - sources:84 - Created source record with ID: 6803811a-60d0-4f7e-be58-33705189e6b3
2025-04-22 04:06:49,231 - app - DEBUG - file_storage:45 - Generated file path for source 6803811a-60d0-4f7e-be58-33705189e6b3: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6803811a-60d0-4f7e-be58-33705189e6b3.pdf
2025-04-22 04:06:49,231 - app - DEBUG - file_storage:57 - File not found at /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6803811a-60d0-4f7e-be58-33705189e6b3.pdf, checking for alternatives
2025-04-22 04:06:49,232 - app - DEBUG - sources:88 - File will be saved to: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6803811a-60d0-4f7e-be58-33705189e6b3.pdf
2025-04-22 04:06:49,234 - app - DEBUG - sources:96 - Successfully saved file to disk: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6803811a-60d0-4f7e-be58-33705189e6b3.pdf
2025-04-22 04:06:49,234 - app - DEBUG - sources:100 - Verified file exists at: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/6803811a-60d0-4f7e-be58-33705189e6b3.pdf
2025-04-22 04:06:49,234 - app - INFO - sources:117 - Successfully uploaded source: NLP_project_slides_v1.pdf (ID: 6803811a-60d0-4f7e-be58-33705189e6b3)
2025-04-22 04:06:49,236 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 04:06:49,236 - app - DEBUG - source:18 - Getting all sources
2025-04-22 04:06:49,236 - app - DEBUG - sources:28 - Retrieved 4 sources
2025-04-22 04:06:49,245 - app - INFO - sources:26 - API request: Get all sources
2025-04-22 04:06:49,245 - app - DEBUG - source:18 - Getting all sources
2025-04-22 04:06:49,245 - app - DEBUG - sources:28 - Retrieved 4 sources
2025-04-22 04:07:09,647 - app - INFO - qa:36 - Received QA request with 1 sources and model gemma3
2025-04-22 04:07:09,648 - app - DEBUG - file_storage:45 - Generated file path for source be525ecd-7971-457c-be39-b730bd79afca: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 04:07:09,649 - app - INFO - qa:58 - Added file path: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 04:07:09,649 - app - INFO - qa:74 - Creating RAG chain with model gemma3 and paths: ['/Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf']
2025-04-22 04:07:09,649 - app - INFO - tools:52 - Attempting to load PDF from: /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 04:07:09,719 - app - DEBUG - tools:61 - Loaded 7 pages from /Users/wanghaonan/WebstormProjects/np_project/backend/uploaded_sources/be525ecd-7971-457c-be39-b730bd79afca.pdf
2025-04-22 04:07:09,719 - app - INFO - tools:99 - Using chunk_size=500, chunk_overlap=50
2025-04-22 04:07:09,719 - app - INFO - tools:110 - Split 7 pages into 34 chunks.
2025-04-22 04:07:13,012 - app - INFO - llm_config:23 - Initializing LLM with model: gemma3
2025-04-22 04:07:13,012 - app - INFO - llm_config:60 - Using Gemma 3-27B model
2025-04-22 04:07:13,017 - app - INFO - qa:78 - Invoking RAG chain with question: What is this source about?
2025-04-22 04:07:16,088 - app - INFO - qa:80 - RAG chain result keys: dict_keys(['input', 'context', 'answer'])
2025-04-22 04:07:16,088 - app - INFO - qa:84 - Generated answer: This source is about a class project and strategies to mitigate hallucinations in models. Specifical...
2025-04-22 04:07:16,088 - app - INFO - qa:90 - Retrieved 3 context chunks.
2025-04-22 04:07:16,089 - app - INFO - qa:97 - Context has 3 documents
2025-04-22 04:07:16,089 - app - INFO - qa:114 - Generated answer with 1 unique source references
