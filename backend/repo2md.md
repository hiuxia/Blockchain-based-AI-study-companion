# Backend Documentation

 of NLP ProjectGenerated on 4/19/2025

This doc provides a comprehensive overview of the backend of the NLP Project.

## Table of Contents

- ğŸ“ app/
  - ğŸ“ api/
    - ğŸ“„ [history.py](#app-api-history-py)
    - ğŸ“„ [process.py](#app-api-process-py)
    - ğŸ“„ [sources.py](#app-api-sources-py)
  - ğŸ“ core/
    - ğŸ“„ [config.py](#app-core-config-py)
    - ğŸ“„ [cors.py](#app-core-cors-py)
    - ğŸ“„ [database.py](#app-core-database-py)
  - ğŸ“ crud/
    - ğŸ“„ [history.py](#app-crud-history-py)
    - ğŸ“„ [source.py](#app-crud-source-py)
    - ğŸ“„ [summary.py](#app-crud-summary-py)
  - ğŸ“ langchain_agent/
    - ğŸ“„ [agent.py](#app-langchain_agent-agent-py)
    - ğŸ“„ [evaluation.py](#app-langchain_agent-evaluation-py)
    - ğŸ“„ [llm_config.py](#app-langchain_agent-llm_config-py)
    - ğŸ“„ [memory.py](#app-langchain_agent-memory-py)
    - ğŸ“„ [prompts.py](#app-langchain_agent-prompts-py)
    - ğŸ“„ [rag_agent.py](#app-langchain_agent-rag_agent-py)
    - ğŸ“„ [tools.py](#app-langchain_agent-tools-py)
  - ğŸ“„ [main.py](#app-main-py)
  - ğŸ“ models/
    - ğŸ“„ [history.py](#app-models-history-py)
    - ğŸ“„ [schemas.py](#app-models-schemas-py)
    - ğŸ“„ [source.py](#app-models-source-py)
    - ğŸ“„ [summary.py](#app-models-summary-py)
  - ğŸ“ services/
    - ğŸ“„ [file_storage.py](#app-services-file_storage-py)
    - ğŸ“„ [task_manager.py](#app-services-task_manager-py)
- ğŸ“„ [requirements.txt](#requirements-txt)

## Source Code

### <a id="app-api-history-py"></a>app/api/history.py

```python
# backend/app/api/history.py
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from app.models.schemas import HistoryCreate, HistoryResponse
from app.crud.history import create_history, list_histories, get_history
from app.core.database import get_db

router = APIRouter(prefix="/history", tags=["history"])

@router.post("", response_model=HistoryResponse)
def save_history(history: HistoryCreate, db: Session = Depends(get_db)):
    return create_history(db, history.conversation)

@router.get("", response_model=list[HistoryResponse])
def get_all_histories(db: Session = Depends(get_db)):
    return list_histories(db)

@router.get("/{history_id}", response_model=HistoryResponse)
def read_history(history_id: str, db: Session = Depends(get_db)):
    h = get_history(db, history_id)
    if not h:
       raise HTTPException(status_code=404, detail="History not found")
    return h

```

### <a id="app-api-process-py"></a>app/api/process.py

```python
# backend/app/api/process.py
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models.schemas import ProcessingRequest, TaskStatus
from app.services import task_manager, file_storage
from app.crud.source import get_source
import asyncio

router = APIRouter(prefix="/process", tags=["processing"])

@router.post("", status_code=202)
async def start_processing(
    request: ProcessingRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    # éªŒè¯æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for source_id in request.source_ids:
        if not get_source(db, source_id):
            raise HTTPException(404, detail=f"Source {source_id} not found")
    task_id = task_manager.create_task()
    # åå°ä»»åŠ¡ä¸­é‡æ–°åˆ›å»º Session é¿å…è¯·æ±‚ç»“æŸåå·²å…³é—­
    background_tasks.add_task(
        process_documents_background,
        task_id,
        request.source_ids,
        request.llm_model
    )
    return {"task_id": task_id}

@router.get("/results/{task_id}", response_model=TaskStatus)
def get_processing_result(task_id: str):
    status = task_manager.get_task(task_id)
    if not status:
        raise HTTPException(404, detail="Task not found")
    return {"task_id": task_id, **status}

def process_documents_background(task_id: str, source_ids: List[str], llm_model: str):
    try:
        from app.services.task_manager import update_task
        update_task(task_id, status="processing")
        
        file_paths = []
        for source_id in source_ids:
            file_path = file_storage.file_storage.get_file_path(source_id)
            if not file_path.exists():
                raise HTTPException(404, detail=f"File with ID {source_id} not found")
            file_paths.append(str(file_path))
        
        # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        print("File paths:", file_paths)
        # è°ƒç”¨å¼‚æ­¥å‡½æ•°ç”Ÿæˆ Markdown æ‘˜è¦ï¼ˆä¸å«å¼•ç”¨ï¼‰
        from app.langchain_agent.agent import process_documents
        markdown = asyncio.run(process_documents(file_paths, llm_model))
        print("Generated markdown:", markdown)
        
        # ä½¿ç”¨æ–°çš„æ•°æ®åº“ Session å†™å…¥æ‘˜è¦è®°å½•
        from app.core.database import SessionLocal
        new_db = SessionLocal()
        try:
            from app.crud.summary import create_summary
            summary_record = create_summary(new_db, source_ids, markdown, vector_index_path=None)
            print("Summary record created, ID:", summary_record.id)
        finally:
            new_db.close()
        
        update_task(task_id, status="completed", result={
            "markdown": markdown,
            "summary_id": summary_record.id,
            "created_at": summary_record.created_at.isoformat()
        })
    except Exception as e:
        update_task(task_id, status="failed", error=str(e))

```

### <a id="app-api-sources-py"></a>app/api/sources.py

```python
# backend/app/api/sources.py
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.crud.source import create_source, get_source
from app.models.schemas import SourceResponse
from app.services import file_storage

router = APIRouter(prefix="/sources", tags=["sources"])

@router.post("", response_model=SourceResponse)
async def upload_source(file: UploadFile = File(...), db: Session = Depends(get_db)):
    try:
        contents = await file.read()
        # æ­¤å¤„åº”è°ƒç”¨æ–‡ä»¶å­˜å‚¨æœåŠ¡å°†æ–‡ä»¶å†™å…¥ uploaded_sources æ–‡ä»¶å¤¹ï¼Œä»£ç ç•¥
        # æ­¤å¤„ä»…ç”Ÿæˆæ•°æ®åº“è®°å½•
        source_id = create_source(db, file.filename, file.content_type)
        return {"id": source_id, "filename": file.filename, "content_type": file.content_type}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

```

### <a id="app-core-config-py"></a>app/core/config.py

```python
# backend/app/core/config.py
from pydantic_settings import BaseSettings
from pathlib import Path

class Settings(BaseSettings):
    app_name: str = "Document Processor"
    database_url: str = "sqlite:///./documents.db"
    upload_dir: Path = Path("uploaded_sources")
    # é…ç½®ç›¸å…³ API Key
    openai_api_key: str
    openrouter_api_key: str
    gemini_api_key: str

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        # Pydantic V2 ä¸­å°† orm_mode æ”¹ä¸º from_attributes
        from_attributes = True

settings = Settings()

```

### <a id="app-core-cors-py"></a>app/core/cors.py

```python
# backend/app/core/cors.py
from fastapi.middleware.cors import CORSMiddleware

def add_cors(app):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # å¼€å‘é˜¶æ®µå…è®¸æ‰€æœ‰è·¨åŸŸè¯·æ±‚ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶å…·ä½“æ¥æº
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    return app

```

### <a id="app-core-database-py"></a>app/core/database.py

```python
# backend/app/core/database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from app.core.config import settings

engine = create_engine(settings.database_url, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    """
    FastAPI ä¾èµ–ï¼Œç”¨äºç”Ÿæˆæ•°æ®åº“ Sessionï¼Œå¹¶ç¡®ä¿è¯·æ±‚ç»“æŸåå…³é—­ã€‚
    ä½¿ç”¨æ–¹æ³•ï¼š
      db: Session = Depends(get_db)
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


```

### <a id="app-crud-history-py"></a>app/crud/history.py

```python
# backend/app/crud/history.py
import uuid
from sqlalchemy.orm import Session
from app.models.history import DBHistory

def create_history(db: Session, conversation: str) -> DBHistory:
    history = DBHistory(
        id=str(uuid.uuid4()),
        conversation=conversation
    )
    db.add(history)
    db.commit()
    db.refresh(history)
    return history

def get_history(db: Session, history_id: str) -> DBHistory:
    return db.query(DBHistory).filter(DBHistory.id == history_id).first()

def list_histories(db: Session):
    return db.query(DBHistory).all()

```

### <a id="app-crud-source-py"></a>app/crud/source.py

```python
# backend/app/crud/source.py
from sqlalchemy.orm import Session
from app.models.source import DBSource

def get_source(db: Session, source_id: str):
    return db.query(DBSource).filter(DBSource.id == source_id).first()

def create_source(db: Session, filename: str, content_type: str) -> str:
    import uuid
    source = DBSource(id=str(uuid.uuid4()), filename=filename, content_type=content_type)
    db.add(source)
    db.commit()
    db.refresh(source)
    return source.id

```

### <a id="app-crud-summary-py"></a>app/crud/summary.py

```python
# backend/app/crud/summary.py
import uuid
from sqlalchemy.orm import Session
from app.models.summary import DBSummary

def create_summary(db: Session, source_ids: list[str], markdown: str, vector_index_path: str = None) -> DBSummary:
    summary = DBSummary(
        id=str(uuid.uuid4()),
        source_ids=",".join(source_ids),
        markdown=markdown,
        vector_index_path=vector_index_path
    )
    db.add(summary)
    db.commit()
    db.refresh(summary)
    return summary

```

### <a id="app-langchain_agent-agent-py"></a>app/langchain_agent/agent.py

```python
# backend/app/langchain_agent/agent.py
import asyncio
from typing import List
from langchain.chains import LLMChain
from langchain.output_parsers import StrOutputParser
from langchain.schema import Document
from .llm_config import get_llm
from .prompts import SUMMARY_PROMPT, CONVERSATION_PROMPT
from .tools import load_documents

async def process_documents(file_paths: List[str], llm_model: str) -> str:
    """
    æ ¹æ®ç»™å®šçš„ PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒåŠ è½½æ–‡ä»¶å†…å®¹å¹¶æ‹†åˆ†ï¼Œ
    è°ƒç”¨ LLM ç”Ÿæˆç»“æ„åŒ–çš„ Markdown æ‘˜è¦ï¼ˆä¸å«å¼•ç”¨ï¼‰ã€‚
    """
    docs: List[Document] = load_documents(file_paths)
    content = "\n\n".join([doc.page_content for doc in docs])
    llm = get_llm(llm_model)
    chain = LLMChain(
        llm=llm,
        prompt=SUMMARY_PROMPT,
        output_parser=StrOutputParser()
    )
    output = await chain.ainvoke({"context": content})
    return output

def create_conversational_agent(memory_type: str = "buffer", llm_model: str = "gemini-flash"):
    """
    åˆ›å»ºä¸€ä¸ªå¸¦è®°å¿†çš„äº¤äº’å¼å¯¹è¯ä»£ç†ï¼Œ
    åœ¨ä¸ç”¨æˆ·å¤šè½®å¯¹è¯ä¸­ç”Ÿæˆå›ç­”æ—¶ä¼šåŒ…å«å¼•ç”¨ï¼Œ
    åŒæ—¶ä¿ç•™å¯¹è¯å†å²ã€‚
    """
    from langchain.agents import initialize_agent, AgentType
    from .memory import get_conversation_memory
    memory = get_conversation_memory(memory_type)
    llm = get_llm(llm_model)
    agent = initialize_agent(
        tools=[],  # å¦‚æœ‰éœ€è¦ï¼Œå¯å¢åŠ é¢å¤–å·¥å…·
        llm=llm,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
        memory=memory,
        verbose=True,
        prompt=CONVERSATION_PROMPT
    )
    return agent

```

### <a id="app-langchain_agent-evaluation-py"></a>app/langchain_agent/evaluation.py

```python
# backend/app/langchain_agent/evaluation.py
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StrOutputParser
from .llm_config import get_llm

EVALUATION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„è¯„ä¼°å‘˜ã€‚è¯·åŸºäºä¸‹æ–¹æ£€ç´¢ç»“æœï¼Œå¯¹ç”Ÿæˆçš„ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€é€»è¾‘æ€§åŠç›¸å…³æ€§åšå‡ºè¯¦ç»†è¯„ä»·ã€‚"),
    ("human", "æ£€ç´¢ç»“æœ:\n{search_results}\n\nç”Ÿæˆç­”æ¡ˆ:\n{answer}\n\nè¯·è¯¦ç»†è¯„ä»·å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚")
])

def evaluate_answer(answer: str, search_results: str, llm_model: str = "gemini-flash") -> str:
    """
    åˆ©ç”¨ LLM å¯¹ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œè¯„ä»·ï¼Œè¿”å›è¯¦ç»†çš„è¯„ä»·æ–‡æœ¬ã€‚
    """
    llm = get_llm(llm_model)
    chain = LLMChain(
        llm=llm,
        prompt=EVALUATION_PROMPT,
        output_parser=StrOutputParser()
    )
    evaluation = chain.run({"search_results": search_results, "answer": answer})
    return evaluation

```

### <a id="app-langchain_agent-llm_config-py"></a>app/langchain_agent/llm_config.py

```python
# backend/app/langchain_agent/llm_config.py
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models import BaseChatModel

def load_llm() -> BaseChatModel:
    """
    åˆå§‹åŒ–å¹¶è¿”å› Gemini 2.0 Flash æ¨¡å‹å®ä¾‹ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
      - ä½¿ç”¨ ChatGoogleGenerativeAI è°ƒç”¨ Gemini 2.0 Flash æ¨¡å‹ã€‚
      - convert_system_message_to_human è®¾ç½®ä¸º Trueï¼Œæœ‰åŠ©äºé€‚åº”å¯¹è¯åœºæ™¯ã€‚
      - safety_settings åŠ generation_config å¯è¿›ä¸€æ­¥æ§åˆ¶è¾“å‡ºï¼Œå¦‚å¯ç”¨å¼•ç”¨åŠŸèƒ½ï¼ˆ"citations": Trueï¼‰ã€‚
      - temperature å‚æ•°è®¾ä¸º 0.7ï¼Œç”¨äºæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚
    
    è¿”å›ï¼š
      - ä¸€ä¸ª BaseChatModel å®ä¾‹ã€‚
    """
    return ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-latest",
        temperature=0.7,
        convert_system_message_to_human=True,
        safety_settings={"HARASSMENT": "BLOCK_NONE"},
        generation_config={"citations": True}
    )


```

### <a id="app-langchain_agent-memory-py"></a>app/langchain_agent/memory.py

```python
# backend/app/langchain_agent/memory.py

from langchain.memory import ConversationBufferMemory
from langchain.chains.conversation.base import ConversationChain
from langchain_core.language_models import BaseChatModel

def get_conversation_memory() -> ConversationBufferMemory:
    """
    è¿”å›ä¸€ä¸ªåŸºäºç¼“å†²åŒºçš„å¯¹è¯è®°å¿†å®ä¾‹ï¼Œç”¨äºä¿å­˜æ‰€æœ‰å¯¹è¯å†å²ã€‚
    """
    return ConversationBufferMemory(return_messages=True)

def build_memory_chain(llm: BaseChatModel) -> ConversationChain:
    """
    æ„é€ ä¸€ä¸ªå¸¦æœ‰å†…å­˜è®°å½•çš„å¯¹è¯é“¾ã€‚
    
    å‚æ•°ï¼š
      - llm: å·²åˆå§‹åŒ–çš„è¯­è¨€æ¨¡å‹å®ä¾‹ã€‚
      
    è¿”å›:
      - ConversationChain å¯¹è±¡ï¼Œè¯¥å¯¹è±¡å¯ä»¥ç”¨äºå¤šè½®å¯¹è¯ï¼Œå¹¶ä¿ç•™å¯¹è¯å†å²ã€‚
    """
    memory = get_conversation_memory()
    return ConversationChain(
        llm=llm,
        memory=memory,
        verbose=True
    )

```

### <a id="app-langchain_agent-prompts-py"></a>app/langchain_agent/prompts.py

```python
# backend/app/langchain_agent/prompts.py
from langchain.prompts import ChatPromptTemplate

# é’ˆå¯¹ PDF æ–‡æ¡£ç”Ÿæˆ Markdown æ‘˜è¦ï¼ˆä¸å«å¼•ç”¨ï¼‰
SUMMARY_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½å­¦ä¹ åŠ©ç†ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆç»“æ„åŒ–çš„ Markdown ç¬”è®°ï¼Œè¦æ±‚å†…å®¹å‡†ç¡®ã€å±‚æ¬¡æ¸…æ™°ã€‚"),
    ("human", "{context}")
])

# é’ˆå¯¹å¯¹è¯äº¤äº’ï¼Œè¦æ±‚å›ç­”ä¸­åŒ…å«å¼•ç”¨ï¼ˆä¾‹å¦‚ Markdown é“¾æ¥ï¼Œå¯ç‚¹å‡»è·³è½¬ï¼‰
CONVERSATION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½æ™ºèƒ½å­¦ä¹ åŠ©ç†ã€‚è¯·æ ¹æ®ç”¨æˆ·æé—®ç”Ÿæˆå›ç­”ï¼Œå¹¶åœ¨å›ç­”ä¸­é€‚å½“åœ°æ·»åŠ å¼•ç”¨ï¼Œä¾‹å¦‚ï¼š[å¼•ç”¨åç§°](https://example.com)ã€‚"),
    ("human", "{input}")
])

```

### <a id="app-langchain_agent-rag_agent-py"></a>app/langchain_agent/rag_agent.py

```python
# backend/app/langchain_agent/rag_agent.py
from typing import List
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from .llm_config import get_llm
from .tools import load_documents

def load_documents_for_rag(paths: List[str]) -> List[str]:
    """
    ä»ç»™å®šçš„ PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸­åŠ è½½æ–‡æ¡£ï¼Œå¹¶æ‹†åˆ†ä¸ºå¤šä¸ªæ–‡æœ¬å—ï¼Œ
    è¿”å›æ‰€æœ‰æ–‡æœ¬å—ï¼ˆchunkï¼‰çš„åˆ—è¡¨ã€‚
    """
    docs = load_documents(paths)
    return [doc.page_content for doc in docs]

def create_vectorstore_from_texts(texts: List[str]) -> FAISS:
    """
    æ ¹æ®æ–‡æœ¬åˆ—è¡¨è®¡ç®—åµŒå…¥å‘é‡ï¼Œå¹¶åˆ©ç”¨ FAISS æ„å»ºå‘é‡å­˜å‚¨ã€‚
    """
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_texts(texts, embeddings)
    return vectorstore

def create_rag_chain(paths: List[str], llm_model: str, top_k: int = 3) -> RetrievalQA:
    """
    æ„å»º Retrieval-Augmented Generationï¼ˆRAGï¼‰é—®ç­”é“¾ï¼š
    1. åŠ è½½ PDF å¹¶æ‹†åˆ†ä¸ºæ–‡æœ¬å—ï¼›
    2. æ ¹æ®æ–‡æœ¬å—è®¡ç®—åµŒå…¥å¹¶æ„å»º FAISS å‘é‡å­˜å‚¨ï¼›
    3. é…ç½®æ£€ç´¢å™¨ï¼Œè¿”å›ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ top_k ä¸ªæ–‡æœ¬å—ï¼›
    4. åˆ©ç”¨ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆ"stuff" æ¨¡å¼ï¼‰ã€‚
    """
    texts = load_documents_for_rag(paths)
    vectorstore = create_vectorstore_from_texts(texts)
    llm = get_llm(llm_model)
    retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        verbose=True
    )
    return qa_chain

if __name__ == "__main__":
    file_paths = ["uploaded_sources/sample.pdf"]  # ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨
    rag_chain = create_rag_chain(file_paths, "gemini-flash")
    query = "è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹ã€‚"
    answer = rag_chain.run(query)
    print("RAG Chain Answer:\n", answer)

```

### <a id="app-langchain_agent-tools-py"></a>app/langchain_agent/tools.py

```python
# backend/app/langchain_agent/tools.py

from pathlib import Path
from typing import List
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.vectorstores import FAISS

# è®¾ç½®å‘é‡æ•°æ®åº“çš„æœ¬åœ°ä¿å­˜ç›®å½•
VECTORSTORE_DIR = Path("vectorstore")

def load_and_split_pdfs(pdf_paths: List[str]) -> List[Document]:
    """
    æ ¹æ®ç»™å®šçš„ PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒåŠ è½½æ–‡ä»¶å†…å®¹å¹¶æ‹†åˆ†æˆå¤šä¸ªæ–‡æœ¬å—ï¼ˆchunkï¼‰ã€‚
    """
    documents: List[Document] = []
    for path in pdf_paths:
        loader = PyPDFLoader(path)
        raw_docs = loader.load()
        documents.extend(raw_docs)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    return splitter.split_documents(documents)

def embed_documents(
    chunks: List[Document],
    store_name: str,
    embedding_model: str = "openai"
) -> FAISS:
    """
    å¯¹æ‹†åˆ†å¥½çš„æ–‡æœ¬å—è®¡ç®—åµŒå…¥å‘é‡ï¼Œå¹¶åˆ©ç”¨ FAISS æ„å»ºä¸€ä¸ªå‘é‡å­˜å‚¨ï¼ŒæŒä¹…åŒ–å­˜å‚¨åˆ°æœ¬åœ°ã€‚
    
    å‚æ•°ï¼š
      - chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª Document å¯¹è±¡ã€‚
      - store_name: æŒ‡å®šå­˜å‚¨çš„åç§°ï¼ˆæ–‡ä»¶åï¼‰ï¼Œç”¨äºåç»­åŠ è½½ã€‚
      - embedding_model: é€‰æ‹©ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ä¸º "openai"ï¼Œå¯é€‰ "google"ã€‚
      
    è¿”å›:
      - æ„å»ºå¥½çš„ FAISS å‘é‡å­˜å‚¨å¯¹è±¡ã€‚
    """
    if embedding_model == "google":
        embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    else:
        embedding = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(chunks, embedding)
    save_path = VECTORSTORE_DIR / store_name
    vectorstore.save_local(str(save_path))
    return vectorstore

def load_vectorstore(
    store_name: str,
    embedding_model: str = "openai"
) -> FAISS:
    """
    åŠ è½½æŒ‡å®šåç§°çš„æœ¬åœ° FAISS å‘é‡å­˜å‚¨ã€‚
    
    å‚æ•°ï¼š
      - store_name: å‘é‡å­˜å‚¨ä¿å­˜çš„æ–‡ä»¶åã€‚
      - embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ã€‚
      
    è¿”å›:
      - åŠ è½½åçš„ FAISS å‘é‡å­˜å‚¨å¯¹è±¡ã€‚
    """
    if embedding_model == "google":
        embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    else:
        embedding = OpenAIEmbeddings()
    return FAISS.load_local(str(VECTORSTORE_DIR / store_name), embeddings=embedding)

```

### <a id="app-main-py"></a>app/main.py

```python
# backend/app/main.py
from fastapi import FastAPI
from app.core.config import settings
from app.core.database import engine, Base
from app.core.cors import add_cors
from app.api import sources, process, history
from app.models import source, summary, history as history_model

# åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

app = FastAPI(title=settings.app_name)
app = add_cors(app)

app.include_router(sources.router)
app.include_router(process.router)
app.include_router(history.router)

@app.get("/health")
def health_check():
    return {"status": "healthy"}

```

### <a id="app-models-history-py"></a>app/models/history.py

```python
# backend/app/models/history.py
from sqlalchemy import Column, String, Text, DateTime, func
from app.core.database import Base

class DBHistory(Base):
    __tablename__ = "histories"
    id = Column(String, primary_key=True, index=True)
    conversation = Column(Text, nullable=False)  # å­˜å‚¨å¯¹è¯å†å²ï¼ˆç”¨æˆ·ä¸ LLM çš„äº¤äº’å†…å®¹ï¼‰
    created_at = Column(DateTime(timezone=True), server_default=func.now())

```

### <a id="app-models-schemas-py"></a>app/models/schemas.py

```python
# backend/app/models/schemas.py
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, List

class ProcessingRequest(BaseModel):
    source_ids: List[str]
    llm_model: str = "gemini-flash"

class TaskStatus(BaseModel):
    task_id: str
    status: str
    result: Optional[dict] = None
    error: Optional[str] = None

class SourceCreate(BaseModel):
    filename: str
    content_type: str

class SourceResponse(BaseModel):
    id: str
    filename: str
    content_type: str

class SummaryResponse(BaseModel):
    id: str
    source_ids: List[str]
    markdown: str
    vector_index_path: Optional[str] = None
    created_at: datetime

    class Config:
        from_attributes = True

class HistoryCreate(BaseModel):
    conversation: str

class HistoryResponse(BaseModel):
    id: str
    conversation: str
    created_at: datetime

    class Config:
        from_attributes = True

```

### <a id="app-models-source-py"></a>app/models/source.py

```python
# backend/app/models/source.py
from sqlalchemy import Column, String, DateTime, func
from app.core.database import Base

class DBSource(Base):
    __tablename__ = "sources"
    id = Column(String, primary_key=True, index=True)
    filename = Column(String, nullable=False)
    content_type = Column(String, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

```

### <a id="app-models-summary-py"></a>app/models/summary.py

```python
# backend/app/models/summary.py
from sqlalchemy import Column, String, Text, DateTime, func
from app.core.database import Base

class DBSummary(Base):
    __tablename__ = "summaries"
    id = Column(String, primary_key=True, index=True)
    source_ids = Column(String, nullable=False)  # å­˜å‚¨å…³è”çš„å¤šä¸ªæºæ–‡ä»¶ IDï¼ˆä»¥é€—å·åˆ†éš”ï¼‰
    markdown = Column(Text, nullable=False)      # LLM ç”Ÿæˆçš„ Markdown æ‘˜è¦
    vector_index_path = Column(String, nullable=True)  # å¯é€‰ï¼šæŒä¹…åŒ– FAISS ç´¢å¼•çš„æ–‡ä»¶è·¯å¾„
    created_at = Column(DateTime(timezone=True), server_default=func.now())

```

### <a id="app-services-file_storage-py"></a>app/services/file_storage.py

```python
# backend/app/services/file_storage.py
import os
from pathlib import Path
from app.core.config import settings

class FileStorageService:
    def __init__(self):
        self.upload_dir = settings.upload_dir
        if not self.upload_dir.exists():
            self.upload_dir.mkdir(parents=True, exist_ok=True)
    
    def get_file_path(self, source_id: str) -> Path:
        # å‡è®¾ä¸Šä¼ æ–‡ä»¶å‘½åè§„åˆ™ä¸º source_id + ".pdf"
        return self.upload_dir / f"{source_id}.pdf"

file_storage = FileStorageService()

```

### <a id="app-services-task_manager-py"></a>app/services/task_manager.py

```python
# backend/app/services/task_manager.py
import threading
from typing import Optional, Dict

_lock = threading.Lock()
_tasks: Dict[str, dict] = {}

def create_task() -> str:
    import uuid
    task_id = str(uuid.uuid4())
    with _lock:
        _tasks[task_id] = {"status": "pending", "result": None, "error": None}
    return task_id

def update_task(task_id: str, status: str, result: Optional[dict] = None, error: Optional[str] = None):
    with _lock:
        if task_id in _tasks:
            _tasks[task_id].update({"status": status, "result": result, "error": error})
        else:
            _tasks[task_id] = {"status": status, "result": result, "error": error}

def get_task(task_id: str) -> Optional[dict]:
    with _lock:
        return _tasks.get(task_id)

```

### <a id="requirements-txt"></a>requirements.txt

```plaintext
Name: fastapi
Version: 0.115.12

Name: uvicorn
Version: 0.34.0

Name: SQLAlchemy
Version: 2.0.34

Name: pydantic-settings
Version: 2.8.1

Name: python-dotenv
Version: 0.21.0

Name: langchain
Version: 0.1.4

Name: langchain-community
Version: 0.0.20

Name: langchain-core
Version: 0.1.23

Name: langsmith
Version: 0.0.87

Name: numpy
Version: 1.26.4
```

