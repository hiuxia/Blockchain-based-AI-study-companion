# RAG System Benchmark

## Purpose

This directory contains scripts and data for evaluating the performance and quality of the LangChain-based Retrieval-Augmented Generation (RAG) system developed for the CS6493 NLP project. The primary goals are to assess:

1.  **Response Quality:** Evaluate the relevance, faithfulness (factual consistency with source context), and completion rate (including appropriate refusal to answer) of the RAG system's answers using the default configuration.
2.  **Chunking Strategy Impact:** Analyze the memory-performance trade-offs and the effect on response quality when using different document chunking strategies (`chunk_size`, `chunk_overlap`).

This evaluation aligns with the requirements outlined in the course project description (`CS6493_projects_DG.pdf`).

## System Overview

The system under test is a FastAPI backend application that uses LangChain to implement RAG. Key components include:
* PDF document ingestion and storage.
* Document chunking using `RecursiveCharacterTextSplitter`.
* Vector embedding storage using FAISS and `HuggingFaceEmbeddings`.
* A QA endpoint (`/qa`) that retrieves relevant document chunks based on a query and generates an answer using an LLM (e.g., Gemma 3, Llama 4).

## Directory Structure

* `sources/`: Contains the PDF documents used as the knowledge base for evaluation. (Requires 10-15 diverse PDFs).
* `data/`: Contains the test questions and potentially ground truth information. See `data/README_DATA.md` for details.
* `results/`: Stores the output CSV files and plots generated by the evaluation scripts.
* `common/`: Contains shared Python utility modules for the test scripts (e.g., API client, helper functions).
* `modified_backend/`: (Optional) Holds copies of backend modules modified specifically for testing (e.g., making chunking configurable).
* `test_response_quality.py`: Script to evaluate response quality with default settings.
* `test_chunking_impact.py`: Script to evaluate different chunking strategies.
* `requirements_benchmark.txt`: Python dependencies needed for running the benchmark scripts.
* `README.md`: This file.
* `APPROACH.md`: Detailed explanation of the testing methodology and metrics.

## Setup

1.  **Place Source PDFs:** Add 10-15 relevant PDF documents into the `benchmark/sources/` directory.
2.  **Prepare Test Data:** Ensure `benchmark/data/test_questions.csv` is populated according to the format described in `data/README_DATA.md`.
4.  **Configure Environment:**
    * Create a `.env` file in the `backend/` directory (or ensure the existing one is configured).
    * Add your `DEEPSEEK_API_KEY="your_actual_api_key"` to the `.env` file. The testing scripts assume this key is available for the LLM-as-a-judge evaluations.
    * Ensure other necessary keys (like `GEMINI_API_KEY` if used by the main RAG LLM) are also present.
5.  **Backend Modifications:** Before running tests, ensure the necessary backend modifications are in place:
    * The `/qa` API endpoint must be modified to return the retrieved `contexts` (list of text chunks) along with the answer.
    * For `test_chunking_impact.py`, the backend's chunking logic (in `app/langchain_agent/tools.py` or a modified copy) must read `CHUNK_SIZE` and `CHUNK_OVERLAP` values from environment variables.

## Running the Tests

Tests are designed to be run using `pytest` from within the `backend/benchmark/` directory.

1.  **Ensure Backend is Running:** Start the FastAPI backend server. For chunking tests, the script `test_chunking_impact.py` might manage starting/stopping the backend if configured to do so.
2.  **Run Response Quality Test:**
    ```bash
    pytest test_response_quality.py
    ```
    Results will be saved in `results/`.
3.  **Run Chunking Impact Test:**
    ```bash
    pytest test_chunking_impact.py
    ```
    This script will likely take longer as it iterates through different configurations, potentially restarting the backend. Results and plots will be saved in `results/`.

## Evaluation Approach

* **LLM-as-a-Judge:** DeepSeek V3 is used to evaluate the Relevance and Faithfulness of generated answers.
* **Metrics:** Key metrics include Relevance Score, Faithfulness Score, Task Completion Rate, Refusal Accuracy, API Latency, and Process Memory Usage (RSS).
* **Chunking:** Tests systematically vary `chunk_size` and `chunk_overlap` to analyze trade-offs.
* **Visualization:** Plots are generated to visualize the impact of chunking on different metrics.

Refer to `APPROACH.md` for a detailed explanation of the methodology.
