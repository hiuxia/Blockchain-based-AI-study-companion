<?xml version="1.0" encoding="UTF-8"?>
<Phase1ImplementationPrompt>
    <Goal>
        Implement the remaining steps (6-9) outlined in Phase 1 (Setup and Preparation) of the Detailed Phased RAG Testing Plan.
        Steps 1-5 (Directory Structure, Source Docs, Test Questions, Dependencies, Environment Config) are complete.
        The focus is now on modifying necessary backend code and implementing common testing utilities and judge functions.
    </Goal>

    <ContextFiles>
        <File path="./backend/benchmark/context_files/README.md">
            <Content><![CDATA[

# RAG System Benchmark

## Purpose

This directory contains scripts and data for evaluating the performance and quality of the LangChain-based Retrieval-Augmented Generation (RAG) system developed for the CS6493 NLP project. The primary goals are to assess:

1. **Response Quality:** Evaluate the relevance, faithfulness (factual consistency with source context), and completion rate (including appropriate refusal to answer) of the RAG system's answers using the default configuration.
2. **Chunking Strategy Impact:** Analyze the memory-performance trade-offs and the effect on response quality when using different document chunking strategies (`chunk_size`, `chunk_overlap`).

This evaluation aligns with the requirements outlined in the course project description (`CS6493_projects_DG.pdf`).

## System Overview

The system under test is a FastAPI backend application that uses LangChain to implement RAG. Key components include:

* PDF document ingestion and storage.
* Document chunking using `RecursiveCharacterTextSplitter`.
* Vector embedding storage using FAISS and `HuggingFaceEmbeddings`.
* A QA endpoint (`/qa`) that retrieves relevant document chunks based on a query and generates an answer using an LLM (e.g., Gemma 3, Llama 4).

## Directory Structure

* `sources/`: Contains the PDF documents used as the knowledge base for evaluation. (Requires 10-15 diverse PDFs).
* `data/`: Contains the test questions and potentially ground truth information. See `data/README_DATA.md` for details.
* `results/`: Stores the output CSV files and plots generated by the evaluation scripts.
* `common/`: Contains shared Python utility modules for the test scripts (e.g., API client, helper functions).
* `modified_backend/`: (Optional) Holds copies of backend modules modified specifically for testing (e.g., making chunking configurable).
* `test_response_quality.py`: Script to evaluate response quality with default settings.
* `test_chunking_impact.py`: Script to evaluate different chunking strategies.
* `requirements_benchmark.txt`: Python dependencies needed for running the benchmark scripts.
* `README.md`: This file.
* `APPROACH.md`: Detailed explanation of the testing methodology and metrics.

## Setup

1. **Place Source PDFs:** Add 10-15 relevant PDF documents into the `benchmark/sources/` directory.
2. **Prepare Test Data:** Ensure `benchmark/data/test_questions.csv` is populated according to the format described in `data/README_DATA.md`.
3. **Install Dependencies:** Create a Python virtual environment and install requirements:

    ```bash
    python -m venv venv
    source venv/bin/activate # or venv\Scripts\activate on Windows
    pip install -r requirements_benchmark.txt
    # Also ensure backend dependencies are met if running locally
    pip install -r ../requirements.txt
    ```

4. **Configure Environment:**
    * Create a `.env` file in the `backend/` directory (or ensure the existing one is configured).
    * Add your `ARK_API_KEY="your_actual_api_key"` to the `.env` file. The testing scripts assume this key is available for the LLM-as-a-judge evaluations.
    * Ensure other necessary keys (like `GEMINI_API_KEY` if used by the main RAG LLM) are also present.
5. **Backend Modifications:** Before running tests, ensure the necessary backend modifications are in place:
    * The `/qa` API endpoint must be modified to return the retrieved `contexts` (list of text chunks) along with the answer.
    * For `test_chunking_impact.py`, the backend's chunking logic (in `app/langchain_agent/tools.py` or a modified copy) must read `CHUNK_SIZE` and `CHUNK_OVERLAP` values from environment variables.

## Running the Tests

Tests are designed to be run using `pytest` from within the `backend/benchmark/` directory.

1. **Ensure Backend is Running:** Start the FastAPI backend server. For chunking tests, the script `test_chunking_impact.py` might manage starting/stopping the backend if configured to do so.
2. **Run Response Quality Test:**

    ```bash
    pytest test_response_quality.py
    ```

    Results will be saved in `results/`.
3. **Run Chunking Impact Test:**

    ```bash
    pytest test_chunking_impact.py
    ```

    This script will likely take longer as it iterates through different configurations, potentially restarting the backend. Results and plots will be saved in `results/`.

## Evaluation Approach

* **LLM-as-a-Judge:** DeepSeek V3 is used to evaluate the Relevance and Faithfulness of generated answers.
* **Metrics:** Key metrics include Relevance Score, Faithfulness Score, Task Completion Rate, Refusal Accuracy, API Latency, and Process Memory Usage (RSS).
* **Chunking:** Tests systematically vary `chunk_size` and `chunk_overlap` to analyze trade-offs.
* **Visualization:** Plots are generated to visualize the impact of chunking on different metrics.

Refer to `APPROACH.md` for a detailed explanation of the methodology.
            ]]></Content>
        </File>
        <File path="./backend/benchmark/context_files/data/README_DATA.md">
            <Content><![CDATA[

# Benchmark Test Data Explanation

This directory contains the data used as input for the RAG system evaluation scripts.

## `test_questions.csv`

This is the primary file containing the questions used to test the RAG system. It should be formatted as a standard CSV file.

**Columns:**

1. **`question_id`**
    * **Type:** String
    * **Description:** A unique identifier for each question (e.g., `Q001`, `Q002`, ...). Used for tracking results.

2. **`question_text`**
    * **Type:** String
    * **Description:** The actual question text that will be sent to the RAG system's `/qa` API endpoint.

3. **`question_type`**
    * **Type:** String
    * **Description:** A category describing the nature of the question. This helps in analyzing results across different types of queries. Recommended values:
        * `fact`: Seeking specific, factual information explicitly stated or easily inferred from the text.
        * `summary`: Asking for a condensed overview of a topic, section, or document.
        * `comparison`: Asking to compare or contrast two or more concepts, methods, or entities discussed in the text.
        * `refusal`: A question specifically designed *not* to be answerable from the designated source document(s). These are crucial for testing the system's ability to recognize its limitations and avoid hallucination.

4. **`source_docs`**
    * **Type:** String
    * **Description:** Specifies the filename(s) of the PDF document(s) located in the `benchmark/sources/` directory that are relevant to this question.
        * The RAG system should ideally be able to answer the question based *only* on the content of these specified documents.
        * For `refusal` type questions, these are the documents where the answer *cannot* be found.
        * If multiple filenames are relevant (e.g., for comparison questions spanning documents), separate them with a pipe symbol (`|`). Example: `doc1.pdf|doc2.pdf`.

5. **`is_answerable`**
    * **Type:** Boolean (`True` or `False`)
    * **Description:** This flag serves as the ground truth for evaluating the Task Completion Rate and Refusal Accuracy.
        * Set to `True` if the question is of type `fact`, `summary`, or `comparison` and the answer is expected to be found within the specified `source_docs`.
        * Set to `False` if the question is of type `refusal`. The system is expected to state that it cannot answer this question based on the provided `source_docs`.

**Example Row:**

```csv
question_id,question_text,question_type,source_docs,is_answerable
Q040,"Does the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf specify which particular pre-trained LLMs (e.g., GPT-4, Llama 3) must be used for the experiments?",refusal,CS6493_projects_DG.pdf,False
```

**Generation and Curation:**

This file should be carefully generated (e.g., using an LLM like NotebookLM with specific instructions, as previously discussed) and then manually reviewed and curated to ensure:

* Questions are clear and relevant to the `source_docs`.
* `question_type` accurately reflects the question's nature.
* `is_answerable` flags are correctly set, especially for `refusal` questions.
* Filenames in `source_docs` match the files present in `benchmark/sources/`.
            ]]></Content>
        </File>
        <File path="./backend/benchmark/context_files/APPROACH.md">
            <Content><![CDATA[

# RAG System Evaluation Approach

This document details the methodology used in the benchmark scripts (`test_response_quality.py` and `test_chunking_impact.py`) to evaluate the RAG system. The approach is based on the findings of the "RAG Testing Script Development" research report and the CS6493 project requirements.

## Evaluation Goals

1. **Assess Response Quality (Baseline):** Measure the relevance, faithfulness, and completion/refusal accuracy of the RAG system using its default configuration.
2. **Analyze Chunking Impact:** Evaluate how different `chunk_size` and `chunk_overlap` settings affect response quality, API latency, and backend memory usage.

## Metrics

The following metrics are calculated:

1. **Relevance Score (LLM-as-Judge):**
    * **Definition:** How pertinent the generated answer is to the user's query intent.
    * **Method:** A separate LLM (DeepSeek V3, accessed via API key stored in `.env`) acts as a judge.
    * **Prompting:** The judge is prompted with the original `question_text` and the RAG system's generated `answer`. It's asked to rate relevance on a scale (e.g., 1-5) and provide reasoning, focusing only on pertinence, not factual accuracy.
    * **Output:** A numerical score (parsed from the judge's response).

2. **Faithfulness Score (LLM-as-Judge):**
    * **Definition:** The factual consistency of the generated answer with the retrieved context chunks. Measures hallucination.
    * **Method:** DeepSeek V3 acts as a judge.
    * **Prompting:** The judge is prompted with the `question_text`, the generated `answer`, and the list of `contexts` (text chunks retrieved by the RAG system from the source documents). It's asked to verify if each statement in the answer is supported *only* by the provided contexts and assign a score/verdict.
    * **Output:** A numerical score or binary verdict (parsed from the judge's response).
    * **Prerequisite:** Requires the backend `/qa` endpoint to return the `contexts`.

3. **Task Completion Rate (TCR):**
    * **Definition:** Percentage of test questions handled correctly (answered appropriately when answerable, refused correctly when unanswerable).
    * **Method:** Compares the generated `answer` against the `is_answerable` ground truth flag from `test_questions.csv`.
    * **Logic:** Uses predefined `refusal_phrases` (e.g., "I cannot answer", "unable to find information").
        * If `is_answerable` is `True`: Correct if no refusal phrase is present. Incorrect if refusal phrase is present.
        * If `is_answerable` is `False`: Correct if refusal phrase is present. Incorrect if no refusal phrase is present.
    * **Output:** Overall percentage of correct handling.

4. **Refusal Accuracy (RA):**
    * **Definition:** Percentage of *unanswerable* questions (where `is_answerable` is `False`) that were correctly refused by the system.
    * **Method:** Subset of TCR calculation, focusing only on questions marked as `refusal` type / `is_answerable=False`.
    * **Output:** Percentage of correct refusals for unanswerable questions.

5. **API Latency:**
    * **Definition:** End-to-end time taken for the `/qa` API endpoint to respond to a query.
    * **Method:** Measured for each API call using `requests.elapsed.total_seconds()` or `time.perf_counter()`.
    * **Output:** Latency in seconds per query. Aggregated statistics (average, median, p95, p99) are calculated per run/configuration.

6. **Memory Usage (RSS):**
    * **Definition:** Peak and average Resident Set Size (RSS) memory consumed by the backend FastAPI process during a test run for a specific configuration. RSS is a practical measure of physical RAM usage.
    * **Method:** Uses the `psutil` library to monitor the backend process's `memory_info().rss` periodically (e.g., every 0.5 seconds) during the execution of test queries for one chunking configuration.
    * **Output:** Peak and average RSS values (in MB) per configuration. Comparisons focus on relative differences between configurations.
    * **Prerequisite:** Requires identifying the Process ID (PID) of the backend server.

## Testing Workflow

1. **Baseline Quality (`test_response_quality.py`):** Runs all questions from `test_questions.csv` against the backend using its default chunking settings. Calculates Relevance, Faithfulness, TCR, RA, and Latency.
2. **Chunking Impact (`test_chunking_impact.py`):**
    * Iterates through a predefined list of `(chunk_size, chunk_overlap)` configurations.
    * For each configuration:
        * Sets backend chunking parameters (via environment variables, requiring backend restart).
        * Monitors backend memory usage using `psutil`.
        * Runs all test questions.
        * Calculates all metrics (Relevance, Faithfulness, TCR, RA, Latency, Memory Usage).
        * Logs results specific to that configuration.

## Chunking Strategy Configuration

* The `test_chunking_impact.py` script relies on the backend's chunking logic (in `app/langchain_agent/tools.py` or a modified version) being sensitive to environment variables `CHUNK_SIZE` and `CHUNK_OVERLAP`.
* The script manages setting these variables and potentially restarting the backend between testing different configurations.

## Analysis and Visualization

* Results from all runs are collected into `pandas` DataFrames.
* Aggregate statistics are computed for each metric per configuration.
* Plots are generated using `matplotlib`/`seaborn` to visualize:
  * Quality metrics vs. Chunk Size.
  * Completion/Refusal rates vs. Chunk Size.
  * Latency distribution vs. Chunk Size.
  * Memory usage vs. Chunk Size.
  * Trade-offs (e.g., Relevance vs. Latency).

This systematic approach allows for empirical analysis of how chunking choices impact the different facets of the RAG system's performance and quality.

*(Refer to the "RAG Testing Script Development" research report for more in-depth discussion of evaluation frameworks like RAGAS and LLM-as-a-judge prompting techniques.)*
            ]]></Content>
        </File>
    </ContextFiles>

    <BackendFilesToModify>
        <File path="./backend/app/api/qa.py">
            <Content><![CDATA[

# backend/app/api/qa.py

from typing import List

from app.core.database import get_db
from app.core.logger import logger
from app.langchain_agent.rag_agent import create_rag_chain
from app.services.file_storage import FileStorageService
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from sqlalchemy.orm import Session

# Initialize the router with a prefix

router = APIRouter(prefix="/qa", tags=["qa"])

# Initialize the file storage service

file_storage = FileStorageService()

class QARequest(BaseModel):
    question: str
    source_ids: List[str]
    llm_model: str

class QAResponse(BaseModel):
    answer: str
    references: List[str]
    # --- MODIFICATION NEEDED ---
    # Add contexts field to return retrieved chunks
    # contexts: List[str]

@router.post("", response_model=QAResponse) # Update response_model if adding contexts
async def ask_question(request: QARequest, db: Session = Depends(get_db)):
    """
    Process a question using the RAG model with the specified sources.
    """
    logger.info(
        f"Received QA request with {len(request.source_ids)} sources and model {request.llm_model}"
    )

    try:
        # Validate that source_ids are provided
        if not request.source_ids:
            raise HTTPException(status_code=400, detail="No source documents selected")

        # Resolve file paths from source IDs
        paths = []
        for source_id in request.source_ids:
            try:
                file_path = file_storage.get_file_path(source_id)
                paths.append(str(file_path))
            except Exception as e:
                logger.error(
                    f"Error retrieving file path for source ID {source_id}: {str(e)}"
                )
                raise HTTPException(
                    status_code=404,
                    detail=f"File with ID {source_id} not found. Error: {str(e)}",
                )

        # Validate the LLM model selection
        valid_models = ["gemma3", "llama4"] # Note: Original plan used gemini/llama, backend uses gemma/llama
        if request.llm_model not in valid_models:
            request.llm_model = "gemma3"  # Default to gemma3 if not valid

        # Create RAG chain and run question
        logger.info(
            f"Creating RAG chain with model {request.llm_model} and paths: {paths}"
        )
        chain = create_rag_chain(paths, request.llm_model)
        logger.info(f"Invoking RAG chain with question: {request.question}")
        result = chain.invoke({"input": request.question})
        logger.info(f"RAG chain result keys: {result.keys()}") # Should contain 'answer' and 'context'

        # Extract answer and source information
        answer = result.get("answer", "No answer generated")
        logger.info(f"Generated answer: {answer[:100]}...")  # Log first 100 chars

        # --- MODIFICATION NEEDED ---
        # Extract context chunks for response
        retrieved_contexts = []
        if "context" in result and isinstance(result["context"], list):
             retrieved_contexts = [doc.page_content for doc in result["context"]]
             logger.info(f"Retrieved {len(retrieved_contexts)} context chunks.")
        else:
             logger.warning("Could not find or parse 'context' in RAG chain result.")


        # Extract source references (existing logic)
        references = []
        if "context" in result and isinstance(result["context"], list):
            logger.info(f"Context has {len(result['context'])} documents")
            seen_sources = set()
            for i, doc in enumerate(result["context"]):
                # Extract metadata or create a default reference
                if hasattr(doc, "metadata") and doc.metadata:
                    source_name = doc.metadata.get("source", f"Source Document {i + 1}")
                    # Attempt to get just the filename
                    source_name = source_name.split('/')[-1].split('\\')[-1]
                    if source_name not in seen_sources:
                        references.append(source_name)
                        seen_sources.add(source_name)
                else:
                    ref_name = f"Source Document {i + 1}"
                    if ref_name not in seen_sources:
                         references.append(ref_name)
                         seen_sources.add(ref_name)


        logger.info(f"Generated answer with {len(references)} unique source references")

        # --- MODIFICATION NEEDED ---
        # Return the extracted contexts in the response
        # Update the return statement and the QAResponse model
        return QAResponse(answer=answer, references=references) # Modify to include contexts=retrieved_contexts

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Error in QA processing: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Error processing QA request: {str(e)}"
        )

            ]]></Content>
        </File>
        <File path="./backend/app/langchain_agent/tools.py">
            <Content><![CDATA[

# backend/app/langchain_agent/tools.py

import os
from pathlib import Path
from typing import List

from app.core.config import settings
from app.core.logger import logger
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OpenAIEmbeddings # Note: RAG agent uses HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pydantic import SecretStr

# 设置向量数据库的本地保存目录

VECTORSTORE_DIR = Path("vectorstore")

def load_documents(pdf_paths: List[str]) -> List[Document]:
    """
    加载 PDF 文档并拆分为文本块。

    参数:
        - pdf_paths: PDF 文件路径列表

    返回:
        - 拆分后的文档块列表
    """
    return load_and_split_pdfs(pdf_paths)

def load_and_split_pdfs(pdf_paths: List[str]) -> List[Document]:
    """
    根据给定的 PDF 文件路径列表，加载文件内容并拆分成多个文本块（chunk）。
    """
    documents: List[Document] = []
    for path in pdf_paths:
        try:
            loader = PyPDFLoader(path)
            raw_docs = loader.load()
            # --- Add metadata enrichment ---
            for doc in raw_docs:
                if not hasattr(doc, 'metadata') or not doc.metadata:
                    doc.metadata = {}
                doc.metadata['source'] = path # Add source path to metadata
            # --- End metadata enrichment ---
            documents.extend(raw_docs)
            logger.debug(f"Loaded {len(raw_docs)} pages from {path}")
        except Exception as e:
            logger.error(f"Failed to load PDF {path}: {e}")
            # Decide whether to skip or raise
            # continue # Option: skip problematic file

    if not documents:
         logger.warning("No documents were successfully loaded.")
         return []

    # --- MODIFICATION NEEDED for Chunking Evaluation ---
    # Read chunk size and overlap from environment variables
    default_chunk_size = 500
    default_chunk_overlap = 50
    try:
        chunk_size = int(os.getenv('CHUNK_SIZE', default_chunk_size))
        chunk_overlap = int(os.getenv('CHUNK_OVERLAP', default_chunk_overlap))
        if chunk_size <= 0:
            logger.warning(f"Invalid CHUNK_SIZE '{os.getenv('CHUNK_SIZE')}', using default {default_chunk_size}.")
            chunk_size = default_chunk_size
        if chunk_overlap < 0 or chunk_overlap >= chunk_size:
             logger.warning(f"Invalid CHUNK_OVERLAP '{os.getenv('CHUNK_OVERLAP')}' for chunk size {chunk_size}, using default {default_chunk_overlap}.")
             chunk_overlap = default_chunk_overlap
    except ValueError:
        logger.warning(f"Non-integer value for CHUNK_SIZE or CHUNK_OVERLAP in environment variables. Using defaults.")
        chunk_size = default_chunk_size
        chunk_overlap = default_chunk_overlap

    logger.info(f"Using chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        # Consider adding separators relevant to your documents if needed
        # separators=["\n\n", "\n", ". ", " ", ""]
        length_function=len # Default, measures in characters
    )
    # --- End MODIFICATION ---

    split_docs = splitter.split_documents(documents)
    logger.info(f"Split {len(documents)} pages into {len(split_docs)} chunks.")
    return split_docs

def embed_documents(
    chunks: List[Document], store_name: str, embedding_model: str = "google"
) -> FAISS:
    """
    对拆分好的文本块计算嵌入向量，并利用 FAISS 构建一个向量存储，持久化存储到本地。
    Note: This function seems unused by the core RAG agent logic which uses HuggingFaceEmbeddings directly.
          Keeping it here as it was in the original file structure.

    参数：
      - chunks: 文本块列表，每个元素为一个 Document 对象。
      - store_name: 指定存储的名称（文件名），用于后续加载。
      - embedding_model: 选择使用的嵌入模型，默认为 "google"，可选 "openai"。

    返回:
      - 构建好的 FAISS 向量存储对象。
    """
    if embedding_model == "google":
        gemini_api_key = settings.gemini_api_key
        if not gemini_api_key:
            logger.warning("GEMINI_API_KEY not found in settings")
        embedding = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=SecretStr(gemini_api_key) if gemini_api_key else None,
        )
    else: # Defaulting to OpenAI if not 'google'
        openai_api_key = settings.openai_api_key # Assuming this exists in settings
        if not openai_api_key:
             logger.warning("OPENAI_API_KEY not found in settings for embedding.")
             # Handle error or use a default public embedding?
             # For now, let it potentially fail if key is needed and missing.
        embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)

    vectorstore = FAISS.from_documents(chunks, embedding)
    save_path = VECTORSTORE_DIR / store_name
    # Ensure directory exists
    save_path.parent.mkdir(parents=True, exist_ok=True)
    vectorstore.save_local(str(save_path))
    logger.info(f"Saved vector store '{store_name}' to {save_path}")
    return vectorstore

def load_vectorstore(store_name: str, embedding_model: str = "openai") -> FAISS:
    """
    加载指定名称的本地 FAISS 向量存储。
    Note: This function also seems unused by the core RAG agent logic.

    参数：
      - store_name: 向量存储保存的文件名。
      - embedding_model: 使用的嵌入模型。

    返回:
      - 加载后的 FAISS 向量存储对象。
    """
    load_path = VECTORSTORE_DIR / store_name
    if not load_path.exists():
        logger.error(f"Vector store not found at {load_path}")
        raise FileNotFoundError(f"Vector store '{store_name}' not found.")

    if embedding_model == "google":
        gemini_api_key = settings.gemini_api_key
        if not gemini_api_key:
            logger.warning("GEMINI_API_KEY not found in settings")
        embedding = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=SecretStr(gemini_api_key) if gemini_api_key else None,
        )
    else: # Defaulting to OpenAI
        openai_api_key = settings.openai_api_key
        if not openai_api_key:
             logger.warning("OPENAI_API_KEY not found in settings for embedding.")
        embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)

    logger.info(f"Loading vector store '{store_name}' from {load_path} using {embedding_model} embeddings.")
    # Allow dangerous deserialization if needed, though generally discouraged
    # return FAISS.load_local(str(load_path), embeddings=embedding, allow_dangerous_deserialization=True)
    return FAISS.load_local(str(load_path), embeddings=embedding)

            ]]></Content>
        </File>
    </BackendFilesToModify>

    <Phase1Steps>
        <Step number="1" status="Completed">
            <Action>Create Directory Structure</Action>
            <Details>Establish the `backend/benchmark/` directory and its subdirectories (`sources`, `data`, `results`, `common`, `modified_backend`).</Details>
        </Step>
        <Step number="2" status="Completed">
            <Action>Populate Source Documents</Action>
            <Details>Copy 10-15 diverse PDF documents relevant to the course/project into `benchmark/sources/`.</Details>
        </Step>
        <Step number="3" status="Completed">
            <Action>Finalize Test Questions</Action>
            <Details>Generate, review, and save the final `test_questions.csv` file in `benchmark/data/` according to the format specified in `context_files/data/README_DATA.md`.</Details>
        </Step>
        <Step number="4" status="Completed">
            <Action>Install Dependencies</Action>
            <Details>Create a Python virtual environment. Install packages from `backend/benchmark/requirements_benchmark.txt` (including `pytest`, `pandas`, `requests`, `psutil`, `python-dotenv`, `matplotlib`, `seaborn`, `openai`). Ensure backend dependencies from `backend/requirements.txt` are also installed.</Details>
        </Step>
        <Step number="5" status="Completed">
            <Action>Configure Environment</Action>
            <Details>Create or update the `.env` file in the `backend/` root directory. Add/verify `ARK_API_KEY="your_actual_deepseek_api_key"` and other necessary API keys (e.g., `GEMINI_API_KEY`).</Details>
        </Step>
        <Step number="6" status="Pending">
            <Action>Implement Backend Modifications</Action>
            <SubStep number="6.1">
                <TargetFile>./backend/app/api/qa.py</TargetFile>
                <Modification>Modify the `ask_question` function and the `QAResponse` Pydantic model.</Modification>
                <Guidance>
                    - Add a `contexts: List[str]` field to the `QAResponse` model.
                    - In the `ask_question` function, after `result = chain.invoke(...)`, extract the list of page content strings from `result['context']`. Store this list in a variable (e.g., `retrieved_contexts`).
                    - Update the `return` statement to include `contexts=retrieved_contexts` when creating the `QAResponse` object.
                    - Ensure the `response_model` in the `@router.post` decorator is updated if you rename or redefine `QAResponse`.
                    - Consider enhancing the reference extraction logic to provide more robust source filenames if needed.
                </Guidance>
            </SubStep>
            <SubStep number="6.2">
                <TargetFile>./backend/app/langchain_agent/tools.py</TargetFile>
                <Modification>Modify the `load_and_split_pdfs` function to make chunking configurable via environment variables.</Modification>
                <Guidance>
                    - Import the `os` module.
                    - Before initializing `RecursiveCharacterTextSplitter`, add logic to read `CHUNK_SIZE` and `CHUNK_OVERLAP` from environment variables using `os.getenv('VAR_NAME', default_value)`.
                    - Convert the retrieved values to integers using `int()`. Include error handling (e.g., `try-except ValueError`) to fall back to default values (e.g., 500 for size, 50 for overlap) if the environment variables are missing, not set, or not valid integers. Add validation to ensure chunk_size > 0 and 0 &lt;= chunk_overlap &lt; chunk_size.
                    - Use these retrieved/validated `chunk_size` and `chunk_overlap` variables when creating the `RecursiveCharacterTextSplitter` instance.
                    - Add logging (`logger.info`) to indicate which chunk size and overlap are being used for a given run.
                    - *Optional Enhancement:* Add source path metadata to each loaded document page (`doc.metadata['source'] = path`) within the loading loop to ensure context sources can be tracked.
                </Guidance>
            </SubStep>
        </Step>
        <Step number="7" status="Pending">
            <Action>Implement Common Utilities</Action>
            <Details>Create the following files and functions within `benchmark/common/`:</Details>
            <SubStep number="7.1">
                <TargetFile>./backend/benchmark/common/api_client.py</TargetFile>
                <Guidance>
                    - Implement `query_qa(question_text, source_ids, llm_model)`:
                        - Takes question, source IDs, and target LLM model name.
                        - Constructs the JSON payload for the `POST /qa` request.
                        - Uses the `requests` library to make the POST call.
                        - Includes timeout and robust error handling (`try...except requests.exceptions.RequestException`, check `response.status_code`).
                        - Parses the JSON response, extracting `answer`, `contexts` (the list of strings added in Step 6.1), and `references`.
                        - Measures and returns latency (`response.elapsed.total_seconds()` or `time.perf_counter`).
                        - Returns a dictionary containing `answer`, `contexts`, `references`, `latency`, `status_code`, `error`.
                    - *Optional:* Implement functions for `POST /sources` if needed for setup automation.
                </Guidance>
            </SubStep>
            <SubStep number="7.2">
                <TargetFile>./backend/benchmark/common/utils.py</TargetFile>
                <Guidance>
                    - Implement `load_test_data(csv_path)`: Uses `pandas.read_csv` to load `test_questions.csv`.
                    - Implement `initialize_deepseek_client()`:
                        - Imports `os` and `openai`.
                        - Reads `ARK_API_KEY` from `os.environ.get()`. Handles missing key error.
                        - Initializes and returns `openai.OpenAI(base_url="https://ark.cn-beijing.volces.com/api/v3", api_key=...)`.
                    - Implement `find_backend_pid(process_name='uvicorn')` (or similar): Uses `psutil.process_iter` to find the PID of the running backend server. Needs careful implementation to target the correct process.
                    - Implement `manage_backend_process(action='start'|'stop'|'restart', script_path='../start.py')`: (Advanced) Functions to control the backend server using `subprocess.Popen` or system commands, needed for Phase 3 environment variable changes. This requires careful handling of process management.
                </Guidance>
            </SubStep>
        </Step>
        <Step number="8" status="Pending">
            <Action>Implement Judge Functions</Action>
            <Details>In `benchmark/common/utils.py` or directly in the test scripts:</Details>
            <SubStep number="8.1">
                <Function>judge_relevance_deepseek(client, question, answer, model_name="deepseek-v3-250324")</Function>
                <Guidance>
                    - Takes the initialized DeepSeek `client`, question text, and generated answer.
                    - Constructs a prompt message list: `[{"role": "system", "content": "Evaluate relevance... Scale 1-5... Format: Score: <score>"}, {"role": "user", "content": f"Question: {question}\nAnswer: {answer}"}]`.
                    - Calls `client.chat.completions.create(model=model_name, messages=..., temperature=0.0)`.
                    - Parses `completion.choices[0].message.content` to extract the numerical score (e.g., using regex or string splitting).
                    - Includes error handling for API calls and parsing failures. Returns the score or None/error indicator.
                </Guidance>
            </SubStep>
            <SubStep number="8.2">
                <Function>judge_faithfulness_deepseek(client, question, answer, contexts, model_name="deepseek-v3-250324")</Function>
                <Guidance>
                    - Takes the client, question, answer, and the list of retrieved context strings.
                    - Constructs a prompt message list: `[{"role": "system", "content": "Evaluate faithfulness... Is the answer supported ONLY by the context?... Format: Verdict: Faithful/Unfaithful"}, {"role": "user", "content": f"Context: {'\n---\n'.join(contexts)}\nQuestion: {question}\nAnswer: {answer}"}]`.
                    - Calls `client.chat.completions.create(...)`.
                    - Parses the response to extract the verdict ('Faithful' or 'Unfaithful'). Consider asking for a score (e.g., 1-5) for more granularity.
                    - Includes error handling. Returns score/verdict or None/error indicator.
                </Guidance>
            </SubStep>
        </Step>
        <Step number="9" status="Pending">
            <Action>Implement Completion Check</Action>
            <Details>Implement the `check_completion(generated_answer, is_answerable_ground_truth, refusal_phrases)` function in `common/utils.py` or test scripts.</Details>
            <Guidance>Follow the logic described in the research report and `APPROACH.md` to compare the answer against ground truth and refusal phrases, returning 'Correct Answer', 'Correct Refusal', 'Incorrect Refusal', or 'Incorrect Answer (Missing Refusal)'.</Guidance>
        </Step>
    </Phase1Steps>

    <FinalChecklist>
        <Item status="Completed">Directory structure created?</Item>
        <Item status="Completed">Source PDFs placed in `benchmark/sources/`?</Item>
        <Item status="Completed">`test_questions.csv` finalized in `benchmark/data/`?</Item>
        <Item status="Completed">Dependencies installed from `requirements_benchmark.txt`?</Item>
        <Item status="Completed">`.env` file configured with `ARK_API_KEY`?</Item>
        <Item status="Pending">Backend `qa.py` modified to return contexts?</Item>
        <Item status="Pending">Backend `tools.py` modified for configurable chunking?</Item>
        <Item status="Pending">`api_client.py` implemented?</Item>
        <Item status="Pending">`utils.py` implemented (data loading, client init, PID finder)?</Item>
        <Item status="Pending">DeepSeek judge functions implemented?</Item>
        <Item status="Pending">Completion check function implemented?</Item>
    </FinalChecklist>

</Phase1ImplementationPrompt>
