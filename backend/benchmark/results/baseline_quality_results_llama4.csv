question_id,question_text,question_type,source_docs,is_answerable,model,generated_answer,retrieved_contexts,references,latency,api_status_code,api_error,relevance_score,faithfulness_score,completion_status,completion_is_correct,judge_error
Q001,What are the main categories of NLG tasks mentioned in Lecture 6?,fact,L6_NLG tasks.pdf,True,llama4,"根据提供的文档“Lecture 6: NLP tasks (2)”，Lecture 6 中提到的主要 NLG 任务类别包括：

*   Machine translation
*   Paraphrasing (rewriting)
*   Report generation & long text generation
*   Summarization
*   Dialog generation [文档 Lecture 6]","['Lecture 6: NLP tasks (2)\nGeneration tasks\nCS6493 Natural Language Processing\nInstructor: Linqi Song', 'Natural language generation tasks\n● NLG focuses on generating human-like text that conveys information or \ncommunicates effectively. It involves tasks such as machine translation and dialogue \ngeneration.\n● Generation tasks\n○ Machine translation\n○ Paraphrasing (rewriting)\n○ Report generation & long text generation\n○ Summarization\n○ Dialog generation\n○ …\n3', 'Outline\n● Natural language generation tasks\n● Machine translation\n● What is machine translation?\n● Neural machine translation\n● Evaluation\n● Dialogue system\n● What is a dialogue system?\n● Task-oriented dialogue systems\n● Chitchat dialogue systems\n● Performance evaluation \n2']",['L6_NLG tasks.pdf'],6.272854333015857,200,,5.0,1.0,Correct Answer,True,
Q002,"According to the slides for Lecture 6, what were the three main eras of machine translation development discussed (rule-based, example-based, statistical)?",fact,L6_NLG tasks.pdf,True,llama4,"According to the slides, the three main eras of machine translation development discussed were: rule-based MT (1950s), example-based MT (1980s-1990s), and statistical machine translation (1990s-2010s) [文档6].","['Timeline of machine translation\n● 1950s: rule-based MT\n● 1980s-1990s: example-based MT\n● 1990s - 2010s: statistical machine translation (SMT)\n● Current trend: neural machine translation (NMT)\n6', 'Machine Translation\n4', 'What is machine translation (MT)?\n● Translate a sentence x from one language (the source language) to a sentence y in \nanother language (the target language).\n5']",['L6_NLG tasks.pdf'],5.61751287500374,200,,5.0,1.0,Correct Answer,True,
Q003,Summarize the key components or steps involved in a task-oriented dialogue system as presented in Lecture 6.,summary,L6_NLG tasks.pdf,True,llama4,"According to the document, a task-oriented dialogue system involves the following key components or steps:

1. **ASR (optional)**: Automatic Speech Recognition (optional)
2. **SLU (NLU)**: Semantic Language Understanding (also known as Natural Language Understanding)
3. **DST**: Dialogue State Tracking
4. **DM**: Dialogue Management (which interacts with a knowledge base)
5. **NLG**: Natural Language Generation
6. **TTS (optional)**: Text-to-Speech (optional)

[Document 40] provides this sequence.","['Summary\n● Task-oriented dialogue systems\n● Chitchat dialogue systems\n● Performance evaluation \n● Challenges (not covered in the lecture)\n● Knowledge accuracy (Factoid)\n● Ethics\n● Bias\n● Explainability\n● Privacy and data security\n64', 'What is a dialogue system?\n● A dialogue system is a computer system intended to converse with a human. \n○ It employed one or more of text, speech, graphics, haptics, gestures, and other modes for \ncommunication on both the input and output channel.\n○ We focus on natural language-based dialogue systems for different purposes (e.g., obtaining \nknowledge, booking tickets, casual chat, etc.)\n38\nSpoken dialogue systems are being incorporated into various devices (smart-', 'Task-oriented dialogue systems\nASR (optional) -> SLU (NLU) -> DST -> DM (<-> knowledge base) -> NLG -> TTS (optional)\n40']",['L6_NLG tasks.pdf'],7.095064833993092,200,,5.0,1.0,Correct Answer,True,
Q004,Compare ranking models and generative models for dialogue systems based on the evaluation discussion in Lecture 6.,comparison,L6_NLG tasks.pdf,True,llama4,"根据提供的文档，Lecture 6 讨论了 ranking 模型和 generative 模型，但没有提供关于它们之间比较的详细信息。

文档提到：

*   **Generative 模型** (例如 seq2seq, Serban et al., 2016) 用于构建端到端的 chitchat 对话系统，从离线对话语料库中学习生成对话。[文档 54]
*   **Ranking 模型** (例如 Distributed utterance order ranking) 是 DialogBERT (Gu et. al., 2020) 的两个任务之一。[文档 6]

然而，文档**没有**提供关于这两种模型在对话系统中的性能评估或比较的讨论。

因此，我无法从提供的文档中找到这个问题的答案。","['Implementation examples (6)\n● Pretrained models\n○ BERT, GPT, DialoGPT pretrained models can be used for various tasks in dialogue systems\n○ DialogBERT, Gu et. al., https://arxiv.org/pdf/2012.01775.pdf\nTwo tasks: \n1. Masked context regression\n2. Distributed utterance order ranking\n53', 'Chitchat dialogue systems\n● Learns to generate dialogues from offline dialogue corpora. No state, action, intent, slot, \netc.\n○ External information/characteristics: sentiment, topic, knowledge ground, etc. \n○ E.g., seq2seq, Serban et al., 2016, Building end-to-end dialogue systems using generative hierarchical neural \nnetwork models\n54', 'Summary\n● Task-oriented dialogue systems\n● Chitchat dialogue systems\n● Performance evaluation \n● Challenges (not covered in the lecture)\n● Knowledge accuracy (Factoid)\n● Ethics\n● Bias\n● Explainability\n● Privacy and data security\n64']",['L6_NLG tasks.pdf'],7.2454300000099465,200,,3.0,1.0,Correct Answer,True,
Q005,Does Lecture 6 provide detailed Python code examples for implementing the PERSONACHAT model?,refusal,L6_NLG tasks.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了可以使用预训练模型（如BERT、GPT、DialoGPT、DialogBERT）来实现对话系统，但没有说明Lecture 6是否提供了PERSONACHAT模型的详细Python代码示例。根据[文档X]，只提到了实现示例（Implementation examples）以及可以使用的预训练模型。,"['Personalizing dialogue agents: PERSONACHAT\n● Zhang et al., 2018. Personalizing Dialogue Agents: I have a dog, do you have pets too?\n● Dataset with consistent personalities & evaluate different models\n● Endow each agent with explicit persona\n● 1155 personas\n● 10,981 dialogues (~19 dialogs per persona)\n● 164,356 utterances (sentences)\n● 3–5 persona sentences per dialog\n● 6–8 chat turns per dialog\n59', 'Summary\n● Task-oriented dialogue systems\n● Chitchat dialogue systems\n● Performance evaluation \n● Challenges (not covered in the lecture)\n● Knowledge accuracy (Factoid)\n● Ethics\n● Bias\n● Explainability\n● Privacy and data security\n64', 'Implementation examples (6)\n● Pretrained models\n○ BERT, GPT, DialoGPT pretrained models can be used for various tasks in dialogue systems\n○ DialogBERT, Gu et. al., https://arxiv.org/pdf/2012.01775.pdf\nTwo tasks: \n1. Masked context regression\n2. Distributed utterance order ranking\n53']",['L6_NLG tasks.pdf'],5.823839832999511,200,,3.0,1.0,Incorrect Answer (Missing Refusal),False,
Q006,"What does RLHF stand for, as explained in Lecture 7 on Large Language Models?",fact,L7_LLM.pdf,True,llama4,"According to the provided document, RLHF is listed as a topic covered in Lecture 7 on Large Language Models, alongside large language model definition, scaling law, and instruction tuning [文档]. However, the document does *not* explain what RLHF stands for. 

Therefore, I cannot answer your question based on the provided text.","['Summary\n● Large language model definition\n● Scaling law and Emergent ability\n● Instruction tuning\n● RLHF\n● Challenges (not covered in the lecture)\n● Hallucination\n● Bias and fairness\n● Explainability\n● Privacy and data security\n31', 'Outline\n● Large language model definition\n● Scaling law and emergent abilities\n● Alignment\n2', 'Lecture 7: \nLarge Language Models\nCS6493 Natural Language Processing\nInstructor: Linqi Song']",['L7_LLM.pdf'],5.206916959024966,200,,3.0,1.0,Incorrect Refusal,False,
Q007,What are the two main components of LLM alignment discussed in Lecture 7 (Instruction Tuning and RLHF)?,fact,L7_LLM.pdf,True,llama4,"According to the provided document, the two main components of LLM alignment discussed in Lecture 7 are **Instruction tuning** and **RLHF**. [Summary]

The document states these were topics covered in the lecture alongside a definition of large language models, scaling laws and emergent ability, and challenges like hallucination, bias, explainability, and privacy. [Summary] It also provides a brief description of instruction tuning: collecting (instruction, output) pairs and finetuning a language model. [Instruction finetuning]","['Summary\n● Large language model definition\n● Scaling law and Emergent ability\n● Instruction tuning\n● RLHF\n● Challenges (not covered in the lecture)\n● Hallucination\n● Bias and fairness\n● Explainability\n● Privacy and data security\n31', 'Instruction finetuning\n• Collect examples of (instruction, output) pairs across many tasks and finetune an LM\n[FLAN-T5; Chung et al., 2022]\n• Evaluate on unseen tasks\n26', 'Scaling law of LLMs (1)\n14\n● Scaling laws which are simple, predictive rules for model performance\n● Empirical observation: scaling up models leads to reliable gains in perplexity\n● Performance measures: cross entropy loss\n● Resource measures: Model size, dataset size, computing power\n[Kaplan et al., 2020]\nScaling Laws for Neural Language Models']",['L7_LLM.pdf'],6.483604000008199,200,,5.0,1.0,Correct Answer,True,
Q008,Briefly explain the concept of 'emergent abilities' in Large Language Models according to Lecture 7.,summary,L7_LLM.pdf,True,llama4,"According to Lecture 7, emergent abilities relate to the scaling law in large language models. Specifically, the lecture highlights **chain-of-thought prompting** as an example of an emergent ability [Wei et al., 2022; also see Nye et al., 2021]. This is illustrated with the example of solving middle school math word problems [Wei et al., 2022; also see Nye et al., 2021].","['Outline\n● Large language model definition\n● Scaling law and emergent abilities\n● Alignment\n2', 'Emergent abilities: Chain-of-thought prompting (1)\n[Wei et al., 2022; also see Nye et al., 2021]\n22', 'Emergent abilities: Chain-of-thought prompting (2)\n[Wei et al., 2022; also see Nye et al., 2021]\nMiddle school \nmath word \nproblems\n23']",['L7_LLM.pdf'],5.698949917015852,200,,5.0,1.0,Correct Answer,True,
Q009,How does instruction finetuning differ from Reinforcement Learning from Human Feedback (RLHF) based on the descriptions in Lecture 7?,comparison,L7_LLM.pdf,True,llama4,"根据提供的文档，instruction finetuning 涉及收集（指令，输出）对的例子，并在多个任务上微调一个语言模型 [FLAN-T5; Chung et al., 2022]。然后，在未见过的任务上进行评估。

而 InstructGPT 使用来自人类反馈的强化学习 (RLHF) [Ouyang et al., 2022]。

文档中没有提供关于两者差异的更详细描述。","['Summary\n● Large language model definition\n● Scaling law and Emergent ability\n● Instruction tuning\n● RLHF\n● Challenges (not covered in the lecture)\n● Hallucination\n● Bias and fairness\n● Explainability\n● Privacy and data security\n31', 'Instruction finetuning\n• Collect examples of (instruction, output) pairs across many tasks and finetune an LM\n[FLAN-T5; Chung et al., 2022]\n• Evaluate on unseen tasks\n26', 'InstructGPT: Reinforcement Learning from Human \nFeedback\n[Ouyang et al., 2022]\n30k \ntasks\n29']",['L7_LLM.pdf'],5.817048833006993,200,,5.0,1.0,Correct Answer,True,
Q010,Does Lecture 7 specify the exact size of the dataset used for training the initial ChatGPT model released in November 2022?,refusal,L7_LLM.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。虽然文档提到了ChatGPT于2022年11月发布，并提到了其他大型语言模型的参数大小和训练数据量，但它没有明确说明用于训练初始ChatGPT模型的具体数据集大小。,"['Training data reaches Trillions\n● To verify their findings, they trained an LLM (i.e. Chinchilla) using the same compute budget as \nGopher, with much fewer model parameters (70B vs 280B) but much more training tokens \n(1.4T vs 300B).\n# tokens seen during training\nhttps://babylm.github.io/\n[Hoffmann et al., 2022]13', 'How large are large language models\n• Large model parameter size, large training data, large computing cost\nMegatron-Turing NLG \n530B\nPaLM, Google, 540B\n12', ""ChatGPT becomes the the fastest-growing 'app' of all time\n• ChatGPT, Chat Generative Pre-\ntrained Transformer, is an artificial \nintelligence (AI) chatbot developed \nby OpenAI and released in \nNovember 2022. \n• ChatGPT can communicate with \nhuman beings in a more natural \nway.\n3""]",['L7_LLM.pdf'],5.543906832986977,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q011,What is the main motivation cited in Lecture 8 for using prompt learning instead of full fine-tuning for large Pre-trained Language Models (PLMs)?,fact,L8_prompts_alignment.pdf,True,llama4,"According to the document, the main motivation for using prompt learning instead of full fine-tuning for large Pre-trained Language Models (PLMs) is that **fine-tuning hundreds of billions of parameters (even a few iterations) is costly** [文档: Motivation]. The document highlights the ""language model scaling wars"" and the desire to reduce the ""usage cost"" of these PLMs.","['Why prompt learning?\n• Prompting makes it possible for downstream tasks to take the same format as the pre-training \nobjectives and requires no new parameters. \n• Just need to design a template (""It was"") and the expected text responses (we call these label \nwords, e.g., ""great"" for the positive label and ""terrible"" for the negative label.\n• Sampling efficiency for few-shot case--a dozen of training examples for a new task, it is hard to', 'Computational Linguistics. \n• [10] Taylor Shin, Yasaman Razeghi, Robert L. Logan, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from \nlanguage models with automatically generated prompts, 2020. \n• [11] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be \ncomparable to fine-tuning universally across scales and tasks, 2021.', 'Motivation\n• The language model scaling wars!\n• Pretraining + finetuning becomes a \nparadigm for downstream tasks\n• For large models, fine-tuning\nhundreds of billions of parameters \n(even a few iterations) is also costly!\n• How to reduce the “usage cost” of \nthese PLMs?\n4\n• LLaMa (Meta): 65B params, 80 layers, 8192 d_model,1.4 trillion tokens of training data\n• PaLM (Google): 540B params, 118 layers, 18432 d_model, 780 billion training tokens']",['L8_prompts_alignment.pdf'],5.916327916987939,200,,5.0,1.0,Correct Answer,True,
Q012,What does 'Chain of Thought Prompting' aim to achieve according to Lecture 8?,fact,L8_prompts_alignment.pdf,True,llama4,"According to Lecture 8, 'Chain of Thought Prompting' aims to elicit reasoning in large language models through a series of intermediate natural language reasoning steps that lead to the final output [文档 20]. It is described as a ""chain of thought"" being these intermediate steps [文档 20].","['Chain of thought prompting (9)\nTree of Thoughts, Graph of Thoughts……', 'Chain of Thought Prompting\n20', 'Chain of thought prompting (1)\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models (Jason Wei)\nA chain of thought is a series of intermediate natural language reasoning steps that \nlead to the final output, and we refer to this approach as chain-of-thought prompting.']",['L8_prompts_alignment.pdf'],5.787260749988491,200,,5.0,1.0,Correct Answer,True,
Q013,Summarize the concept of P-tuning v2 as mentioned in Lecture 8.,summary,L8_prompts_alignment.pdf,True,llama4,"According to the provided documents, P-tuning v2 is a prompt tuning method that ""can be comparable to fine-tuning universally across scales and tasks"" [11]. The document cites Xiao Liu et al. (2021) as the source for this information.

It's important to note that the document does *not* elaborate on *how* P-tuning v2 achieves this comparability, only stating the claim itself.","['Computational Linguistics. \n• [10] Taylor Shin, Yasaman Razeghi, Robert L. Logan, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from \nlanguage models with automatically generated prompts, 2020. \n• [11] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be \ncomparable to fine-tuning universally across scales and tasks, 2021.', 'PPO in LLM Alignment\n• Supervised Fine-Tuning (SFT): Train the model on high-\nquality data.\n• Reward Modeling: Collect human feedback to create a \nreward function\n• PPO Optimization: Use PPO to fine-tune the model to \nmaximize the reward.\n• Evaluation: Ensure alignment with human values.\n47', 'PPO in LLM Alignment\n• Strength\nProven effectiveness in RLHF (Step 3)\n• Weakness\nComputationally expensive and complex to implement.\n48']",['L8_prompts_alignment.pdf'],6.041436917003011,200,,4.0,1.0,Correct Answer,True,
Q014,"Based on the context provided in Lecture 8, what is the main difference between zero-shot and few-shot prompting?",comparison,L8_prompts_alignment.pdf,True,llama4,我无法从提供的文档中找到这个问题的答案。提供的文档提到了“few-shot”学习，但没有解释“zero-shot”学习，也没有比较这两种 prompting 方法的主要区别。,"['Reference (2)\n• [8] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the \n59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural \nLanguage Processing (Volume 1: Long Papers), pages 3816–3830, Online, August 2021. Association for Computational Linguistics.', 'Why prompt learning?\n• Prompting makes it possible for downstream tasks to take the same format as the pre-training \nobjectives and requires no new parameters. \n• Just need to design a template (""It was"") and the expected text responses (we call these label \nwords, e.g., ""great"" for the positive label and ""terrible"" for the negative label.\n• Sampling efficiency for few-shot case--a dozen of training examples for a new task, it is hard to', 'Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec \nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, \neditors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.']",['L8_prompts_alignment.pdf'],5.302013874985278,200,,3.0,1.0,Correct Answer,True,
Q015,"Does Lecture 8 provide a specific cost comparison, for instance in US dollars, between fine-tuning LLaMa and using prompt tuning techniques?",refusal,L8_prompts_alignment.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了微调大型语言模型（如LLaMa）的成本，以及提示调整作为一种降低成本的方法，但没有提供具体的成本比较，例如美元金额。,"['Motivation\n• The language model scaling wars!\n• Pretraining + finetuning becomes a \nparadigm for downstream tasks\n• For large models, fine-tuning\nhundreds of billions of parameters \n(even a few iterations) is also costly!\n• How to reduce the “usage cost” of \nthese PLMs?\n4\n• LLaMa (Meta): 65B params, 80 layers, 8192 d_model,1.4 trillion tokens of training data\n• PaLM (Google): 540B params, 118 layers, 18432 d_model, 780 billion training tokens', 'fine-tune the pre-trained models and the new task-specific parameters effectively, but the \nprocess is much smoother with prompting. \n9', 'Continuous prompts (3)\n• Hard-soft prompt hybrid tuning: continuous prompts are learned by inserting \ntrainable variables into the embedded input.\n• P-Tuning [11]: Instead of using a purely learnable prompt template, these methods insert \nsome tunable embeddings into a hard prompt template.\n17']",['L8_prompts_alignment.pdf'],5.871727416000795,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q016,"According to Lecture 9, what are some limitations of standard Large Language Models that LLM agents aim to address?",fact,L9_LLMAgents.pdf,True,llama4,"According to Lecture 9, standard Large Language Models (LLMs) have the following limitations:

*   Hallucination
*   Inaccuracy of knowledge
*   Lack of interpretability
*   Inconsistency
*   Limited scalability
*   Restricted capabilities

(Based on the provided document)","['Language models are powerful, but they still \nsuffer from\n• Hallucination\n• Inaccuracy of knowledge\n• Lack of interpretability \n• Inconsistency \n• Limited scalability \n• Restricted capabilities \n• …\nCan LLMs utilize internal reasoning/planning and external tools/knowledge\nto not only expand their capacities but also to make our NLP systems more \nrobust, scalable, and interpretable?\n3', '[54] LLM+P: Empowering Large Language Models with Optimal Planning Proficiency \n[55] Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive \nTasks \n[56] OpenAGI: When LLM Meets Domain Experts \n[57] Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs \n[58] Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task \nPlanning', '[37] PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents \n[38] Large Language Models as Tool Makers \n[39] Gorilla: Large Language Model Connected with Massive APIs \n[40] On the Tool Manipulation Capability of Open-source Large Language Models \n[41] Making Language Models Better Tool Learners with Execution Feedback \n[42] Small models are valuable plug-ins for large language models \n[43] Hierarchical Prompting Assists Large Language Model on Web Navigation']",['L9_LLMAgents.pdf'],6.8996168749872595,200,,5.0,1.0,Correct Answer,True,
Q017,"What are the three core capabilities or components of an LLM agent discussed in Lecture 9 (Reasoning, Tool Learning, Knowledge Incorporation/RAG)?",fact,L9_LLMAgents.pdf,True,llama4,"根据提供的文档，Lecture 9 讨论的 LLM agent 的三个核心能力或组件是：

1. **Reasoning**
2. **Tool learning**
3. **Knowledge incorporation-RAG** (Retrieval-Augmented Generation)

这些能力在文档的“Outline”部分被列出。[文档2]","['Outline\n• LLM agents introduction\n• Reasoning\n• Tool learning\n• Knowledge incorporation-RAG\n2', 'What are LLM agents?\n6', 'LLMs + tool use in perspective of executable language \ngrounding (1)\n• Ground language models into executable actions\n• Mapping natural language instructions into code or actions \nexecutable within various environments such as databases, web \napplications, and robotic physical world. \n• LM (planning and reasoning) + actions\nData analysis                                                     Web/Apps                                            Robotic physical world\n12']",['L9_LLMAgents.pdf'],5.347277708002366,200,,5.0,1.0,Correct Answer,True,
Q018,"Provide a high-level summary of how LLM agents utilize external tools, based on the concepts presented in Lecture 9.",summary,L9_LLMAgents.pdf,True,llama4,"Based on Lecture 9, LLM agents utilize external tools by **grounding language models into executable actions**. This involves **mapping natural language instructions into code or actions** that can be performed in environments like databases, web applications, and the physical world [文档 12]. Essentially, LLMs handle the **planning and reasoning** while the tools execute the **actions** [文档 12]. ","['What are LLM agents?\n6', 'Outline\n• LLM agents introduction\n• Reasoning\n• Tool learning\n• Knowledge incorporation-RAG\n2', 'LLMs + tool use in perspective of executable language \ngrounding (1)\n• Ground language models into executable actions\n• Mapping natural language instructions into code or actions \nexecutable within various environments such as databases, web \napplications, and robotic physical world. \n• LM (planning and reasoning) + actions\nData analysis                                                     Web/Apps                                            Robotic physical world\n12']",['L9_LLMAgents.pdf'],6.440534624998691,200,,5.0,1.0,Correct Answer,True,
Q019,Contrast the role of internal reasoning/planning and external tool use within LLM agents as presented in Lecture 9.,comparison,L9_LLMAgents.pdf,True,llama4,"According to the provided document, Lecture 9 presents LLM agents as utilizing both internal reasoning/planning and external tool use in an interleaved manner through the ReAct framework. 

ReAct prompts LLMs to:

*   **Reason to act:** Generate verbal reasoning traces and create/maintain/adjust plans.
*   **Act to reason:** Interact with external environments (like Wikipedia) to incorporate additional information into the reasoning process. [Document 29]

Essentially, the LLM both *thinks* through the problem (internal reasoning/planning) and *acts* by using external tools to gather information and inform its thought process (external tool use).","['Outline\n• LLM agents introduction\n• Reasoning\n• Tool learning\n• Knowledge incorporation-RAG\n2', 'LLM + tool use for QA and decision making: ReAct\nReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved \nmanner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans \nfor acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate \nadditional information into reasoning (act to reason).\n29', 'What are LLM agents?\n6']",['L9_LLMAgents.pdf'],6.761431375023676,200,,5.0,1.0,Correct Answer,True,
Q020,Does Lecture 9 provide specific details about the algorithms or neural network architecture used within the AlphaGo agent?,refusal,L9_LLMAgents.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。提供的文档只列出了关于“agents”、“agent examples”和“LLM agents”的主题，并没有关于AlphaGo的具体细节，例如其算法或神经网络架构。,"['What are agents?\n5', 'Agent examples\n4', 'What are LLM agents?\n6']",['L9_LLMAgents.pdf'],4.815795916016214,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q021,What are the three main parallelization strategies discussed in Lecture 10 for efficiently training Large Language Models?,fact,L10_Efficient Training of LLM.pdf,True,llama4,"According to the provided outline, the three main parallelization strategies discussed in Lecture 10 for efficiently training Large Language Models are:

*   Model Parallelism
*   Data Parallelism
*   Parameter-Efficient Fine-Tuning

(Based on the ""Outline"" section of the document).","['References\n• Rasley, Jeff, et al. ""Deepspeed: System optimizations enable training deep learning models with over 100 billion \nparameters."" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. \n2020.\n• Shoeybi, Mohammad, et al. ""Megatron-lm: Training multi-billion parameter language models using model \nparallelism."" arXiv preprint arXiv:1909.08053 (2019).', 'Outline\n● Background\n● Model Parallelism\n● Data Parallelism\n● Parameter-Efficient Fine-Tuning\n2', '• Huang, Yanping, et al. ""Gpipe: Efficient training of giant neural networks using pipeline parallelism."" Advances in neural \ninformation processing systems 32 (2019).\n• Hillis, W. Daniel, and Guy L. Steele Jr. ""Data parallel algorithms."" Communications of the ACM 29.12 (1986): 1170-1183.\n• Narayanan, Deepak, et al. ""PipeDream: generalized pipeline parallelism for DNN training."" Proceedings of the 27th \nACM symposium on operating systems principles. 2019.']",['L10_Efficient Training of LLM.pdf'],7.545648917002836,200,,5.0,1.0,Correct Answer,True,
Q022,What does LoRA stand for in the context of Parameter-Efficient Fine-Tuning (PEFT) as mentioned in Lecture 10?,fact,L10_Efficient Training of LLM.pdf,True,llama4,"I cannot find the answer to what LoRA stands for in the provided documents. The documents describe *what* LoRA is and *how* it functions within Parameter-Efficient Fine-Tuning (PEFT), but do not state its full name.","['LoRA\n• Different from the adapter, instead of sequentially added to the transformer layers, LoRA is \nadded parallel to the pretrained weights.\n• After training, LoRA weights can be merged into the model weights, so there is no extra \ninference latency.', 'LoRA\n• Consider the weight matrix, W0, which measures d by d in size and \nis kept unchanged during the training procedure. The updated \nweights, ∆W, also measured by d. \n• Directly update weight matrix by training the d by d weights \nconsumes memory and time.\n• Training with a sequential adapter add latency during inference \ntime.\n• Similar to the adapter, in LoRA, a parameter r is introduced which reduces \nthe size of the matrix. The smaller matrices, A and B, are defined with a', 'LoRA\n• Consider the weight matrix, W0, which \nmeasures d by d in size and is kept \nunchanged during the training \nprocedure. The updated weights, ∆W , \nalso measured by d. \n• Directly update weight matrix by \ntraining the d by d weights consumes \nmemory and time.\n• Training with a sequential adapter add \nlatency during inference time.']",['L10_Efficient Training of LLM.pdf'],5.2156361249799374,200,,3.0,1.0,Correct Answer,True,
Q023,Briefly explain the concept of Data Parallelism for training neural networks as described in Lecture 10.,summary,L10_Efficient Training of LLM.pdf,True,llama4,"根据提供的文档，数据并行（DP）是指将数据集分割成多个分片，每个分片分配给一个设备。每个设备都持有模型的一个完整副本，并在分配给它的数据集分片上进行训练。 [文档10]

参考文献：
* Huang, Yanping, et al. ""Gpipe: Efficient training of giant neural networks using pipeline parallelism."" Advances in neural information processing systems 32 (2019).
* Hillis, W. Daniel, and Guy L. Steele Jr. ""Data parallel algorithms."" Communications of the ACM 29.12 (1986): 1170-1183.
* Narayanan, Deepak, et al. ""PipeDream: generalized pipeline parallelism for DNN training."" Proceedings of the 27th ACM symposium on operating systems principles. 2019.","['Data parallelism (DP) – Naïve DP\n• In data parallel training, the \ndataset is split into several \nshards, each shard is \nallocated to a device.\n• Each device will hold a full \ncopy of the model replica \nand trains on the dataset \nshard allocated.', '• Huang, Yanping, et al. ""Gpipe: Efficient training of giant neural networks using pipeline parallelism."" Advances in neural \ninformation processing systems 32 (2019).\n• Hillis, W. Daniel, and Guy L. Steele Jr. ""Data parallel algorithms."" Communications of the ACM 29.12 (1986): 1170-1183.\n• Narayanan, Deepak, et al. ""PipeDream: generalized pipeline parallelism for DNN training."" Proceedings of the 27th \nACM symposium on operating systems principles. 2019.', 'Outline\n● Background\n● Model Parallelism\n● Data Parallelism\n● Parameter-Efficient Fine-Tuning\n2']",['L10_Efficient Training of LLM.pdf'],7.548969833005685,200,,4.0,1.0,Correct Answer,True,
Q024,"Based on the descriptions in Lecture 10, what is the fundamental difference between Model Parallelism (or Tensor Parallelism) and Pipeline Parallelism?",comparison,L10_Efficient Training of LLM.pdf,True,llama4,根据提供的文档，Model Parallelism (MP) 通过垂直划分的方式，将模型层组分布在多个GPU上，而Tensor Parallelism (TP) 则将矩阵乘法分解到多个GPU上。此外，Tensor Parallelism中每个GPU只处理张量的一部分，并在需要整个张量时才进行聚合。[文档：Modern MP – MegatronLM (Tensor Parallelism) & Model parallelism (MP)]。,"['Modern MP – MegatronLM (Tensor Parallelism)\n• I n T ensor Parallelism each G PU  \nprocesses only a slice of a tensor and \nonly aggregates the full tensor for \noperations that require the whole \nthing.\n• It  parallize  the tensor in column - and \nrow-wise methods.', 'Tensor parallelism (TP)\n• C ore idea: matrix \nmultiplication can be \nsplit between multiple \nGPUs', 'Model parallelism (MP)\n• Naïve Model Parallelism (MP) \n– Vertical partition, spreads \ngroups of model layers \nacross multiple GPUs.\nGPU 0 GPU 1']",['L10_Efficient Training of LLM.pdf'],7.489471499982756,200,,4.0,1.0,Correct Answer,True,
Q025,Does Lecture 10 provide specific benchmark results comparing the training speed improvement of ZeRO versus Megatron-LM on a defined hardware configuration like A100 GPUs?,refusal,L10_Efficient Training of LLM.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了 Megatron-LM 的性能（scaling efficiency 和在测试集上的结果），以及 ZeRO 和 Megatron-LM 的一些优缺点，但没有提供关于在特定硬件配置（如 A100 GPU）上 ZeRO 和 Megatron-LM 训练速度的具体基准测试结果比较。,"['• Rajbhandari, Samyam, et al. ""Zero: Memory optimizations toward training trillion parameter models."" SC20: \nInternational Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020.\n• Rajbhandari, Samyam, et al. ""Zero-infinity: Breaking the gpu memory wall for extreme scale deep \nlearning."" Proceedings of the international conference for high performance computing, networking, storage and \nanalysis. 2021.', 'Modern MP – MegatronLM (TP)\n• Advantage:\n• a simple and efficient model parallel approach by making \nonly a few targeted modifications to an existing PyTorch \ntransformer implementation.\n• demonstrate up to 76% scaling efficiency using 512 GPUs.\n• Can scaling up the model size with limited hardware \nresources.\n• The scaled MegatronLM achieve state of the art results on \ntest sets: perplexity on WikiText103 (10.8 ppl), accuracy \non LAMBADA (66.5%), and accuracy on RACE(90.9%)\n• Disadvantage:', 'Naïve DP vs distributed data parallel (DDP)\n• DataParallel is single-process, multi-thread, and only works on a single machine. (Threads-based)\n• In DP , GPU0 needs to \n• reads the batch of data and then split them into mini-batch, and then send them to each GPU.\n• Receive output from each GPU, computes loss\n• scatters loss to each GPU, run backward\n• Receive gradients from each GPU, average those.\n• GPU 0 in DP mode performs a lot more work than the rest of the GPUs, thus resulting in under-']",['L10_Efficient Training of LLM.pdf'],5.716151916974923,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q026,"What does RAG stand for in the context of Large Language Models, according to Lecture 11?",fact,L11_RAG.pdf,True,llama4,"According to Lecture 11, RAG stands for Retrieval-Augmented Generation. However, the provided document snippet does *not* explicitly state what the acronym RAG stands for, only mentioning its use and related issues. Therefore, **I cannot fully answer your question from the provided document.**","['Where is RAG being used?', 'Main issues in RAG – what/when/how', 'Naïve RAG']",['L11_RAG.pdf'],5.490246291999938,200,,4.0,0.0,Correct Answer,True,
Q027,"List three distinct reasons why Retrieval-Augmented Generation (RAG) is considered beneficial for LLMs, as mentioned in the Lecture 11 slides.",fact,L11_RAG.pdf,True,llama4,"According to the provided Lecture 11 slides, RAG is beneficial for LLMs because it:

1.  Enhances LLMs by integrating external knowledge bases.
2.  Employs a synergistic approach, combining information retrieval mechanisms and generation techniques.
3.  Bolsters the NLP performance.

(Source: Lecture 11 slides - ""What’s RAG?"")","['What’s RAG?\n• RAG is a paradigm that enhances LLMs by integrating external knowledge bases.\n• It employs a synergistic approach, combining information retrieval mechanisms and generation \ntechniques to bolster the NLP performance.', 'Main issues in RAG – what/when/how', 'arXiv:2110.06176 (2021).\n• [10] Gao, Yunfan, et al. ""Retrieval-augmented generation for large language models: A survey."" arXiv preprint arXiv:2312.10997 (2023).\n• https://qdrant.tech/articles/what-is-rag-in-ai/#\n• https://www.llamaindex.ai/blog\n• https://qdrant.tech/articles/what-are-embeddings/\n• https://www.databricks.com/glossary/retrieval-augmented-generation-rag\n• https://www.elastic.co/what-is/vector-database']",['L11_RAG.pdf'],5.359805999993114,200,,5.0,1.0,Correct Answer,True,
Q028,Describe the basic workflow of a Naive RAG system (Query -> Retrieve -> Augment -> Generate) as depicted in Lecture 11.,summary,L11_RAG.pdf,True,llama4,"根据提供的文档，一个 Naive RAG 系统（Query -> Retrieve -> Augment -> Generate）的基本流程可以描述如下：

1. **Querying (Retrieve):** 系统接收查询，并从向量数据库中检索相关信息。[文档]
2. **Augment:** (文档中未明确提及，但可以推断为检索到的信息被用于增强 LLM 的输入)
3. **Generate:** 使用 LLM（如 GPT、BART、T5）生成回复。[文档]

文档中提到了 RAG 通过结合信息检索和生成技术来增强 LLM 的性能。[文档] 并且提到了向量数据库、嵌入、索引等是 RAG 架构的关键组成部分。[文档]","['RAG architecture\n• Vector Database\n• Embedding\n• Indexing\n• Querying (Retrieve)\n• Post-process\n• Generator\n• LLM (like GPT, BART, T5)', 'What’s RAG?\n• RAG is a paradigm that enhances LLMs by integrating external knowledge bases.\n• It employs a synergistic approach, combining information retrieval mechanisms and generation \ntechniques to bolster the NLP performance.', 'Content\n• What’s RAG?\n• What / When / How to retrieve?']",['L11_RAG.pdf'],7.572385458013741,200,,4.0,1.0,Correct Answer,True,
Q029,"Based on the overview provided in Lecture 11, how does Advanced RAG generally differ from Naive RAG (e.g., involving optimization in indexing, pre/post-retrieval)?",comparison,L11_RAG.pdf,True,llama4,我无法从提供的文档中找到这个问题的答案。提供的文档仅包含“Advanced RAG”、“Main issues in RAG – what/when/how”和“Naïve RAG”这些标题，没有关于Advanced RAG和Naive RAG之间差异的详细信息，例如索引优化或预/后检索优化。,"['Advanced RAG', 'Main issues in RAG – what/when/how', 'Naïve RAG']",['L11_RAG.pdf'],5.961381250002887,200,,1.0,1.0,Correct Answer,True,
Q030,Does Lecture 11 provide the specific mathematical formula or algorithm used by the RePLUG model for its retrieval mechanism?,refusal,L11_RAG.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档仅列出了与检索增强语言模型相关的参考文献，并没有提供 RePLUG 模型检索机制的具体数学公式或算法。,"['Iterative retrieval and adaptive retrieval', 'References\n• [1] Guu, Kelvin, et al. ""Retrieval augmented language model pre-training."" International conference on machine learning. PMLR, 2020.\n• [2] Ram, Ori, et al. ""In-context retrieval-augmented language models."" Transactions of the Association for Computational Linguistics 11 (2023): 1316-1331.\n• [3] Shi, Weijia, et al. ""Replug: Retrieval-augmented black-box language models."" arXiv preprint arXiv:2301.12652 (2023).', '• [4] Borgeaud, Sebastian, et al. ""Improving language models by retrieving from trillions of tokens."" International conference on machine learning. PMLR, 2022.\n• [5] Khandelwal, Urvashi, et al. ""Generalization through memorization: Nearest neighbor language models."" arXiv preprint arXiv:1911.00172 (2019).\n• [6] Jiang, Zhengbao, et al. ""Active retrieval augmented generation."" arXiv preprint arXiv:2305.06983 (2023).']",['L11_RAG.pdf'],5.155846875015413,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q031,"What are the main assessment components and their percentage weightings for the CS6493 course, as stated in the Lecture 12 review slides?",fact,L12_CourseReview.pdf,True,llama4,"According to the provided document, the main assessment components and their percentage weightings for the CS6493 course are:

*   **Continuous assessment:** 60%
*   **2 individual homework assignments:** each 15% (total 30%)
*   **1 group project with presentations:** 30%
*   **Final exam:** 40%

[Document slides, page 119]","['Assessment\n• Continuous assessment (60%)\n• 2 individual homework assignments (each 15%)\n• 1 group project with presentations (30%)\n• Final exam (40%)\n119', 'Outline\n• Course review\n• LOQ\n• Final exam arrangements\n• Q&A \n2', 'Final exam (1)\n• It will be a closed-book exam. No materials and electronics are \nallowed; no calculators.\n• The scope of the exam includes all lectures, tutorials, and homework \nassignments. \n• Less focus will be put on memorization. The key focus will be put on \nunderstanding basic concepts and principles, understanding \nfundamentals techniques, analyzing advantages and disadvantages of \ndifferent techniques, and proposing solutions for specific problems.\n120']",['L12_CourseReview.pdf'],6.258012417005375,200,,5.0,1.0,Correct Answer,True,
Q032,"According to the Lecture 12 course review slides, what are the key NLP preprocessing steps mentioned (e.g., tokenization, normalization)?",fact,L12_CourseReview.pdf,True,llama4,"According to the Lecture 12 course review slides, the key NLP preprocessing steps mentioned are:

*   Tokenization [文档7]
*   Stop words removal [文档7]
*   Stemming [文档7]
*   Lemmatization [文档7]
*   Vectorization (N-gram, BOW, TF-IDF) [文档7]","['Preprocessing of data\n• Tokenization\n• Tokenization is the process of breaking up text document into individual \nwords called tokens.\n• Tokens can be either words, characters, or sub-words (oov, BPE).\n• Stop words removal\n• Stemming\n• Lemmatization\n• Vectorization (N-gram, BOW , TF-IDF)\n7', 'NLU vs. NLG\n● NLU focuses on comprehending and extracting meaning from natural \nlanguage input. It involves tasks such as text classification and question \nanswering (reading comprehension).\n● NLG focuses on generating human-like text that conveys information or \ncommunicates effectively. It involves tasks such as machine translation \nand dialogue generation.\n● For today’s lecture, we will focus on two classic NLU tasks, i.e., text \nclassification and question answering.\n39', 'Bird’s-eye view of this course\n• Basics: Linguistics, language models, word embeddings\n• Tasks: NLU, NLG, machine translation, question answering, dialogue, text \nclassification\n• Large language models: Transformers, pretraining (e.g., BERT, GPT), \nprompting and alignment, LLM agent, efficient finetuning, RAG\n6']",['L12_CourseReview.pdf'],5.852557500009425,200,,5.0,1.0,Correct Answer,True,
Q033,"Summarize the main high-level topics covered in the CS6493 course, as outlined in the 'Bird's-eye view of this course' slide in Lecture 12.",summary,L12_CourseReview.pdf,True,llama4,"根据Lecture 12的“Bird’s-eye view of this course”幻灯片，CS6493课程涵盖的主要主题包括：

*   **基础知识 (Basics):** 语言学、语言模型、词嵌入
*   **任务 (Tasks):** 自然语言理解 (NLU)、自然语言生成 (NLG)、机器翻译、问答、对话、文本分类
*   **大型语言模型 (Large language models):** Transformers、预训练 (例如 BERT, GPT)、提示和对齐、LLM agent、高效微调、RAG。","['Lecture 12: Course Review\nCS6493 Natural Language Processing\nInstructor: Linqi Song\n1', 'Final exam (1)\n• It will be a closed-book exam. No materials and electronics are \nallowed; no calculators.\n• The scope of the exam includes all lectures, tutorials, and homework \nassignments. \n• Less focus will be put on memorization. The key focus will be put on \nunderstanding basic concepts and principles, understanding \nfundamentals techniques, analyzing advantages and disadvantages of \ndifferent techniques, and proposing solutions for specific problems.\n120', 'Bird’s-eye view of this course\n• Basics: Linguistics, language models, word embeddings\n• Tasks: NLU, NLG, machine translation, question answering, dialogue, text \nclassification\n• Large language models: Transformers, pretraining (e.g., BERT, GPT), \nprompting and alignment, LLM agent, efficient finetuning, RAG\n6']",['L12_CourseReview.pdf'],6.507831708993763,200,,5.0,1.0,Correct Answer,True,
Q034,"What is the format of the final exam for CS6493 described in Lecture 12 (e.g., closed-book, question types)?",fact,L12_CourseReview.pdf,True,llama4,"根据提供的文档，CS6493的期末考试将是闭卷考试，不允许携带任何材料或电子设备，包括计算器[文档1]。考试范围涵盖所有讲座、辅导课和作业[文档1]。

考试将包含总共5道题[文档2]。其中约50%为简答题，考察对概念、原理、原因和技术关键特征的理解[文档2]。另外约50%会涉及设计、计算和程序代码分析等[文档2]。","['Final exam (1)\n• It will be a closed-book exam. No materials and electronics are \nallowed; no calculators.\n• The scope of the exam includes all lectures, tutorials, and homework \nassignments. \n• Less focus will be put on memorization. The key focus will be put on \nunderstanding basic concepts and principles, understanding \nfundamentals techniques, analyzing advantages and disadvantages of \ndifferent techniques, and proposing solutions for specific problems.\n120', 'Final exam\n118', 'Final exam (2)\n• There will be in total 5 questions. \n• Short questions to ask your understanding of some concepts, principles, \nreasons, key features of some techniques, etc. (around 50%)\n• A little bit design, calculation, program codes analysis, etc. (around 50%)\n• We will hold an office hour via Zoom (to be announced later) to \nanswer questions approximately one week before the final exam. \nWelcome to join if you have questions.\n121']",['L12_CourseReview.pdf'],6.221988208009861,200,,5.0,1.0,Correct Answer,True,
Q035,Does Lecture 12 specify the exact date and time for the final exam review session mentioned?,refusal,L12_CourseReview.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。虽然Lecture 12提到了将通过Zoom举行一次答疑时间（“We will hold an office hour via Zoom (to be announced later) to answer questions approximately one week before the final exam.”），但具体日期和时间尚未公布 (“to be announced later”)。,"['Course review\n3', 'Final exam (2)\n• There will be in total 5 questions. \n• Short questions to ask your understanding of some concepts, principles, \nreasons, key features of some techniques, etc. (around 50%)\n• A little bit design, calculation, program codes analysis, etc. (around 50%)\n• We will hold an office hour via Zoom (to be announced later) to \nanswer questions approximately one week before the final exam. \nWelcome to join if you have questions.\n121', 'Outline\n• Course review\n• LOQ\n• Final exam arrangements\n• Q&A \n2']",['L12_CourseReview.pdf'],6.244551249983488,200,,5.0,1.0,Incorrect Answer (Missing Refusal),False,
Q036,"What is the deadline for submitting the project report and source code for the CS6493 course, according to the instructions document?",fact,CS6493_projects_DG.pdf,True,llama4,"According to the instructions document, the deadline for submitting the project report and source code for the CS6493 course is **6pm, Apr. 22, Monday, 2025** [文档X, Instruction 1].","['project in class on April 22. The project report should at least consist of following parts, including introduction,\nrelated work, methodology, experiments and discussions. The report may contain up to 6 pages of main content,\nplus unlimited pages of references and appendix. The source code can be submitted by Jupyter Notebook or\nPython files.\n5. Please attach your presentation slides at the end of the report.', 'CS6493: Natural Language Processing - Projects\nInstructions\n1. Due at 6pm, Apr. 22, Monday, 2025.\n2. This is the group project. Each group has 1-6 members. Please register your group before 6pm, March 9, 2025\non Canvas-People-Groups.\n3. Select one topic among the following 6 topics for your group.\n4. You are required to submit the project report and source code via Canvas and give a 15-min presentation for your', 'discussion. The assistant should recognize commitments like “I will send the report by Friday” and automatically\nassign tasks to relevant participants.\nThis project challenges you to apply NLP, speech processing, and context awareness to create a useful AI tool\nfor workplace productivity. You are encouraged to design a dataset and explore innovative features to enhance the\nsystem’s effectiveness.\nReference']",['CS6493_projects_DG.pdf'],4.953026541013969,200,,5.0,1.0,Correct Answer,True,
Q037,What are the mandatory sections that the CS6493 project report must consist of?,fact,CS6493_projects_DG.pdf,True,llama4,"The CS6493 project report must consist of the following sections: introduction, related work, methodology, experiments, and discussions. [文档X]","['project in class on April 22. The project report should at least consist of following parts, including introduction,\nrelated work, methodology, experiments and discussions. The report may contain up to 6 pages of main content,\nplus unlimited pages of references and appendix. The source code can be submitted by Jupyter Notebook or\nPython files.\n5. Please attach your presentation slides at the end of the report.', 'CS6493: Natural Language Processing - Projects\nInstructions\n1. Due at 6pm, Apr. 22, Monday, 2025.\n2. This is the group project. Each group has 1-6 members. Please register your group before 6pm, March 9, 2025\non Canvas-People-Groups.\n3. Select one topic among the following 6 topics for your group.\n4. You are required to submit the project report and source code via Canvas and give a 15-min presentation for your', 'discussion. The assistant should recognize commitments like “I will send the report by Friday” and automatically\nassign tasks to relevant participants.\nThis project challenges you to apply NLP, speech processing, and context awareness to create a useful AI tool\nfor workplace productivity. You are encouraged to design a dataset and explore innovative features to enhance the\nsystem’s effectiveness.\nReference']",['CS6493_projects_DG.pdf'],4.512541167001473,200,,5.0,1.0,Correct Answer,True,
Q038,Summarize the core idea and requirements for project 'Topic 6: Open Your Mind' in the CS6493 project instructions.,summary,CS6493_projects_DG.pdf,True,llama4,"The core idea of project 'Topic 6: Open Your Mind' (and all projects in CS6493) is to select a topic, complete a project, and submit a report and source code, followed by a 15-minute presentation. [文档X]

The requirements are:

*   **Group Work:** The project is completed in groups of 1-6 members. [文档X]
*   **Registration:** Groups must be registered on Canvas by March 9, 2025. [文档X]
*   **Submission:** A project report (at least introduction, related work, methodology, experiments, and discussions – up to 6 pages of main content plus unlimited references/appendix) and source code (Jupyter Notebook or Python files) must be submitted via Canvas. [文档X]
*   **Presentation:** A 15-minute presentation is required in class on April 22. Presentation slides should be attached to the report. [文档X]
*   **Due Date:** All submissions are due by 6pm on April 22, 2025. [文档X]

**Note:** The provided document does *not* contain details about ""Topic 6: Open Your Mind"" itself, only ""Topic 5 - Smart Meeting Assistant"". Therefore, I cannot summarize the core idea and requirements *specifically* for Topic 6.","['CS6493: Natural Language Processing - Projects\nInstructions\n1. Due at 6pm, Apr. 22, Monday, 2025.\n2. This is the group project. Each group has 1-6 members. Please register your group before 6pm, March 9, 2025\non Canvas-People-Groups.\n3. Select one topic among the following 6 topics for your group.\n4. You are required to submit the project report and source code via Canvas and give a 15-min presentation for your', 'project in class on April 22. The project report should at least consist of following parts, including introduction,\nrelated work, methodology, experiments and discussions. The report may contain up to 6 pages of main content,\nplus unlimited pages of references and appendix. The source code can be submitted by Jupyter Notebook or\nPython files.\n5. Please attach your presentation slides at the end of the report.', 'Topic 5 - Smart Meeting Assistant\nIn this project, you are expected to design a smart meeting assistant that enhances virtual and in-person meetings by\nproviding real-time support for participants. The system should assist users by transcribing conversations, summarizing\ndiscussions, extracting key action items, and offering relevant insights.\nYour smart meeting assistant should incorporate the following key features:']",['CS6493_projects_DG.pdf'],9.82512312498875,200,,3.0,0.0,Correct Answer,True,
Q039,What is the maximum number of members allowed in a project group for CS6493?,fact,CS6493_projects_DG.pdf,True,llama4,"根据提供的文档，CS6493项目的团队成员数量为1-6人。[文档：CS6493: Natural Language Processing - Projects, Instruction 2]","['CS6493: Natural Language Processing - Projects\nInstructions\n1. Due at 6pm, Apr. 22, Monday, 2025.\n2. This is the group project. Each group has 1-6 members. Please register your group before 6pm, March 9, 2025\non Canvas-People-Groups.\n3. Select one topic among the following 6 topics for your group.\n4. You are required to submit the project report and source code via Canvas and give a 15-min presentation for your', 'recommend doing some fun things. Just open your mind and take a try. Please contact TAs to verify your own\nselected topic before starting the group work.\n7', 'project in class on April 22. The project report should at least consist of following parts, including introduction,\nrelated work, methodology, experiments and discussions. The report may contain up to 6 pages of main content,\nplus unlimited pages of references and appendix. The source code can be submitted by Jupyter Notebook or\nPython files.\n5. Please attach your presentation slides at the end of the report.']",['CS6493_projects_DG.pdf'],5.103799624979729,200,,5.0,1.0,Correct Answer,True,
Q040,"Does the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf specify which particular pre-trained LLMs (e.g., GPT-4, Llama 3) must be used for the experiments?",refusal,CS6493_projects_DG.pdf,False,llama4,"No, the project description for 'Topic 1: Mathematical Reasoning Ability' in CS6493_projects_DG.pdf does **not** specify which particular pre-trained LLMs must be used for the experiments. 

However, Topic 6 recommends experimenting with cutting-edge LLMs such as GPT, DeepSeek, and Llama [文档: CS6493_projects_DG.pdf, Topic 6]. It also mentions pairing with open-source models like Llama or closed-source models like GPT [文档: CS6493_projects_DG.pdf, Topic 6].","['Topic 1 - Mathematical Reasoning Ability of Large Language Models\nLarge language models (LLMs) have demonstrated remarkable capabilities in natural language processing. How-\never, their mathematical reasoning ability remains a critical area of research, especially when dealing with complex\nproblems found in challenging datasets such as MATH-500, GSM8K and AIME 2024. These datasets require models\nto interpret mathematical expressions, understand abstract concepts, and generate accurate solutions.', 'Topic 6 - Open Your Mind\nThis is a highly flexible project where you can explore any idea involving language models that interest you,\nprovided it connects to course concepts. We strongly recommend experimenting with cutting-edge LLMs (GPT,\nDeepSeek, Llama, etc.) through API integrations or local deployment. For example, you could consider combining\nmultiple AI approaches (e.g., RAG + fine-tuning) or addressing ethical dimensions (bias detection, fact-checking). We', 'paired with various LLMs (e.g., open-source models like Llama or closed-source models like GPT). You\nshould:\n• Implement at least 2 retrieval methods (e.g., BM25 vs. Contriever) and 2 generation models\n• Analyze how retrieval precision affects final answer quality using the HotpotQA dev set\n• Compare zero-shot vs. instruction-tuned models (e.g., https://huggingface.co/Intel/neural-chat-7b-v3-3 )\n2. Hallucination Mitigation: Design experiments to measure how RAG reduces model fabrication:']",['CS6493_projects_DG.pdf'],6.978999165992718,200,,5.0,1.0,Incorrect Answer (Missing Refusal),False,
Q041,What are the two novel model architectures proposed in the word2vec paper (word2vec.pdf) for computing vector representations?,fact,word2vec.pdf,True,llama4,"According to the document, many different types of models were proposed for estimating continuous representations of words [word2vec.pdf]. The document states that it extends a simple architecture and focuses on the first step where word vectors are learned [word2vec.pdf]. However, the document **does not explicitly name the two novel model architectures** proposed in the word2vec paper.

Therefore, I cannot answer your question based on the provided document.","['vectors were made available for future research and comparison2. However, as far as we know, these\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\nare used [23].\n2 Model Architectures\nMany different types of models were proposed for estimating continuous representations of words,', 'work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word', '6 Conclusion\nIn this paper we studied the quality of vector representations of words derived by various models on\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\nquality word vectors using very simple model architectures, compared to the popular neural network\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it']",['word2vec.pdf'],4.890172625018749,200,,3.0,1.0,Incorrect Refusal,False,
Q042,What task is primarily used in the word2vec paper (word2vec.pdf) to measure the quality of the learned word vector representations?,fact,word2vec.pdf,True,llama4,"According to the provided document, the quality of the learned word vector representations was measured by studying them on a collection of **syntactic and semantic language tasks** [文档6].","['estimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10', '6 Conclusion\nIn this paper we studied the quality of vector representations of words derived by various models on\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\nquality word vectors using very simple model architectures, compared to the popular neural network\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it', 'work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word']",['word2vec.pdf'],4.795779000007315,200,,4.0,1.0,Correct Answer,True,
Q043,Summarize the main contribution claimed by the authors in the abstract of the word2vec paper (word2vec.pdf).,summary,word2vec.pdf,True,llama4,"According to the abstract of the word2vec paper, the main contribution is focusing on learning word vectors using a simple model, directly extending existing architectures [word2vec.pdf]. The authors also anticipate that high-quality word vectors will be a valuable component for future NLP applications [word2vec.pdf]. Furthermore, they note that word vectors have already been shown to improve and simplify many NLP applications [word2vec.pdf].","['estimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10', 'work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word', 'R words from the future of the current word as correct labels. This will require us to do R × 2\nword classiﬁcations, with the current word as input, and each of the R + R words as output. In the\nfollowing experiments, we use C = 10.\n4 Results\nTo compare the quality of different versions of word vectors, previous papers typically use a table\nshowing example words and their most similar words, and understand them intuitively. Although']",['word2vec.pdf'],5.755631667008856,200,,5.0,1.0,Correct Answer,True,
Q044,"According to the introduction of the word2vec paper (word2vec.pdf), what is the N-gram model commonly used for?",fact,word2vec.pdf,True,llama4,"According to the provided document, the weight matrix between the input and the projection layer is shared ""in the same way as in the NNLM"" (word2vec.pdf). However, the document does *not* state what the N-gram model (or NNLM) is commonly used for. 

Therefore, I cannot answer your question based on the provided text.","['way as in the NNLM.\n3.2 Continuous Skip-gram Model\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\nprojection layer, and predict words within a certain range before and after the current word. We', 'work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word', 'words at the input, where the training criterion is to correctly classify the current (middle) word.\nTraining complexity is then\nQ = N × D + D × log2(V ). (4)\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\nweight matrix between the input and the projection layer is shared for all word positions in the same\nway as in the NNLM.']",['word2vec.pdf'],5.652030416007619,200,,1.0,1.0,Incorrect Refusal,False,
Q045,What is the key difference highlighted in the abstract of word2vec.pdf between the proposed models and previous neural network approaches regarding computational cost and accuracy?,comparison,word2vec.pdf,True,llama4,"According to the document, the key difference highlighted is that the proposed models (CBOW and Skip-gram) aim to learn word vectors **more data efficiently** compared to previous neural network approaches like NNLM. The document states that neural network language models can be successfully trained in two steps: learning continuous word vectors using a simple model, and then training an N-gram NNLM on top of these representations [13, 14]. It also mentions that the proposed models can be learned with **high accuracy** [section 1.2].","['words at the input, where the training criterion is to correctly classify the current (middle) word.\nTraining complexity is then\nQ = N × D + D × log2(V ). (4)\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\nweight matrix between the input and the projection layer is shared for all word positions in the same\nway as in the NNLM.', 'more data efﬁciently.\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\ndistributed representations of words. While there has been later substantial amount of work that', 'can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\non the dimensionality of the word vectors and on the amount of the training data.\n1.2 Previous Work\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to']",['word2vec.pdf'],6.540855166997062,200,,5.0,1.0,Correct Answer,True,
Q046,What size of dataset is mentioned in the abstract of word2vec.pdf from which high-quality word vectors could be learned in less than a day?,fact,word2vec.pdf,True,llama4,"According to the document, high-quality word vectors were learned from a Google News corpus containing about **6B tokens** [文档X].","['diminishing improvements. So, we have to increase both vector dimensionality and the amount\nof the training data together. While this observation might seem trivial, it must be noted that it is\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\n6', '4.2 Maximization of Accuracy\nWe have used a Google News corpus for training the word vectors. This corpus contains about\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\nare facing time constrained optimization problem, as it can be expected that both using more data\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of', 'estimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10']",['word2vec.pdf'],5.445521249988815,200,,4.0,1.0,Correct Answer,True,
Q047,"Explain the primary motivation for developing continuous vector representations of words, as outlined in the introduction of word2vec.pdf.",summary,word2vec.pdf,True,llama4,"According to the provided document, the primary motivation for developing continuous vector representations of words is to compute these representations from “very large data sets” (Abstract). The document also suggests that previous architectures were computationally expensive, motivating the need for a more efficient approach (page 1).","['vectors were made available for future research and comparison2. However, as far as we know, these\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\nare used [23].\n2 Model Architectures\nMany different types of models were proposed for estimating continuous representations of words,', 'diminishing improvements. So, we have to increase both vector dimensionality and the amount\nof the training data together. While this observation might seem trivial, it must be noted that it is\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\n6', 'Efﬁcient Estimation of Word Representations in\nVector Space\nTomas Mikolov\nGoogle Inc., Mountain View, CA\ntmikolov@google.com\nKai Chen\nGoogle Inc., Mountain View, CA\nkaichen@google.com\nGreg Corrado\nGoogle Inc., Mountain View, CA\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc., Mountain View, CA\njeff@google.com\nAbstract\nWe propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations']",['word2vec.pdf'],5.174182083981577,200,,4.0,1.0,Correct Answer,True,
Q048,How does the word2vec paper (word2vec.pdf) state that many traditional NLP systems often treat words?,fact,word2vec.pdf,True,llama4,"According to the word2vec paper (word2vec.pdf), many current NLP systems and techniques treat words as atomic units – meaning there is no notion of similarity between words, and they are represented as indices in a vocabulary [文档: word2vec.pdf, Introduction].","['estimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10', 'work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word', '1 Introduction\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\nity between words, as these are represented as indices in a vocabulary. This choice has several good\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\ndata outperform complex systems trained on less data. An example is the popular N-gram model']",['word2vec.pdf'],5.054428417002782,200,,5.0,1.0,Correct Answer,True,
Q049,Compare the conceptual difference between the Skip-gram and CBOW architectures as described in the word2vec paper (word2vec.pdf).,comparison,word2vec.pdf,True,llama4,"According to the provided document, the key conceptual difference between the CBOW and Skip-gram architectures is:

*   **CBOW** predicts the *current word* based on the *context* words.
*   **Skip-gram** predicts *surrounding words* given the *current word*.

This is visually represented in Figure 1 and explained in the text describing each model [word2vec.pdf].","['way as in the NNLM.\n3.2 Continuous Skip-gram Model\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\nprojection layer, and predict words within a certain range before and after the current word. We', 'w(t-2)\nw(t+1)\nw(t-1)\nw(t+2)\nw(t)\nSUM\n       INPUT         PROJECTION         OUTPUT\nw(t)\n          INPUT         PROJECTION      OUTPUT\nw(t-2)\nw(t-1)\nw(t+1)\nw(t+2)\n                   CBOW                                                   Skip-gram\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\ncontext, and the Skip-gram predicts surrounding words given the current word.', 'available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\n3We thank Geoff Zweig for providing us the test set.\n7']",['word2vec.pdf'],6.296936333994381,200,,5.0,1.0,Correct Answer,True,
Q050,"Does the word2vec paper (word2vec.pdf) provide the specific hyperparameter values (e.g., learning rate, window size) used for the hierarchical softmax optimization experiments?",refusal,word2vec.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了 hierarchical softmax 的使用以及它如何减少计算复杂度，但没有提供 hierarchical softmax 优化实验中使用的具体超参数值（例如，学习率、窗口大小）。,"['this further reduces the number of output units that need to be evaluated: while balanced binary tree\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\nneural network LMs as the computational bottleneck is in theN ×D×H term, we will later propose', 'representations of the vocabulary, the number of output units that need to be evaluated can go down\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\ntree. This follows previous observations that the frequency of words works well for obtaining classes\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and', 'The complexity per training example of the RNN model is\nQ = H × H + H × V, (3)\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\ncomplexity then comes from H × H.\n3']",['word2vec.pdf'],5.0180118329881225,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q051,"What are the full names of the two authors of the 'Speech and Language Processing' book (3rd Edition Draft, ed3book.pdf)?",fact,ed3book.pdf,True,llama4,我无法从提供的文档中找到这个问题的答案。文档中提到了 James Baker 和 Frederick Jelinek，但没有提供他们的全名，也没有提及他们是“Speech and Language Processing”一书的作者。,"['15.5 Dialogue System Design . . . . . . . . . . . . . . . . . . . . . . . 325\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 328\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n16 Automatic Speech Recognition and Text-to-Speech 331\n16.1 The Automatic Speech Recognition Task . . . . . . . . . . . . . . 332', 'James Baker learned of this work and applied the algorithm to speech processing\n(Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek\nand collaborators (drawing from their research in information-theoretical models\ninﬂuenced by the work of Shannon (1948)) applied HMMs to speech at the IBM\nThomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\nthe decoding algorithm; Baker’s DRAGON system used Viterbi (dynamic program-', 'beginning with 1000-word speaker-independent read-speech tasks like “Resource\nManagement” (Price et al., 1988), recognition of sentences read from theWall Street\nJournal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of\nactual news broadcasts, including quite difﬁcult passages such as on-the-street inter-\nviews) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey\net al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or']",['ed3book.pdf'],18.83324912498938,200,,2.0,1.0,Correct Answer,True,
Q052,"According to the table of contents in ed3book.pdf, which chapter number covers N-gram Language Models?",fact,ed3book.pdf,True,llama4,"According to the table of contents in ed3book.pdf, Chapter 3 covers N-gram Language Models [文档: 48 CHAPTER 3 • N- GRAM LANGUAGE MODELS].","['century, especially because they showed that caches and class-based models pro-\nvided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Heaﬁeld 2011, Heaﬁeld et al. 2013) are publicly available toolkits for building n-\ngram language models.\nLarge language models are based on neural networks rather than n-grams, en-\nabling them to solve the two major problems with n-grams: (1) the number of param-', '48 CHAPTER 3 • N- GRAM LANGUAGE MODELS\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n3.6.3 Language Model Interpolation\nThere is an alternative source of knowledge we can draw on to solve the problem\nof zero frequency n-grams. If we are trying to compute P(wn|wn−2wn−1) but we\nhave no examples of a particular trigram wn−2wn−1wn, we can instead estimate its', '3.1 • N-G RAMS 33\nlanguage model. An n-gram is a sequence of n words: a 2-gram (which we’ll call\nbigram) is a two-word sequence of words like The water, or water of, and a 3-\ngram (a trigram) is a three-word sequence of words like The water of, or water\nof Walden. But we also (in a bit of terminological ambiguity) use the word ‘n-\ngram’ to mean a probabilistic model that can estimate the probability of a word given']",['ed3book.pdf'],17.96504420900601,200,,5.0,1.0,Correct Answer,True,
Q053,"Summarize the main topics covered in Part I, 'Fundamental Algorithms for NLP', based on the chapter titles listed in the table of contents of ed3book.pdf.",summary,ed3book.pdf,True,llama4,"Based on the table of contents from ed3book.pdf, Part I, 'Fundamental Algorithms for NLP', covers the following main topics:

*   **Introduction** to the field (Chapter 1)
*   **Regular Expressions, Tokenization, and Edit Distance** (Chapter 2)
*   **N-gram Language Models** (Chapter 3)
*   **Naive Bayes, Text Classification, and Sentiment Analysis** (Chapter 4)

According to the text, this part of the book introduces the ""fundamental suite of algorithmic tools"" used in modern neural language models. It starts with preprocessing techniques like tokenization and edit distance, then moves on to classification tasks and models like logistic regression and neural networks.","['NLP research (Bengio et al., 2013).', 'Summary of Contents\nI Fundamental Algorithms for NLP 1\n1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Regular Expressions, Tokenization, Edit Distance . . . . . . . . . . . . . . . 4\n3 N-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4 Naive Bayes, Text Classiﬁcation, and Sentiment . . . . . . . . . . . . . . . . . 56', 'Part I\nFUNDAMENTAL ALGORITHMS FOR\nNLP\nIn the ﬁrst part of the book we introduce the fundamental suite of algorithmic\ntools that make up the modern neural language model that is the heart of end-to-end\nNLP systems. We begin with tokenization and preprocessing, as well as useful algo-\nrithms like computing edit distance, and then proceed to the tasks of classiﬁcation,\nlogistic regression, neural networks, proceeding through feedforward networks, re-']",['ed3book.pdf'],19.443245917005697,200,,5.0,1.0,Correct Answer,True,
Q054,Which chapter number in ed3book.pdf discusses The Transformer architecture?,fact,ed3book.pdf,True,llama4,"According to the provided document, Chapter 9 discusses The Transformer architecture [ed3book.pdf]. Specifically, section 9.1 covers Attention within the Transformer architecture [ed3book.pdf].","['will deﬁne soon). 3\n3 Note that we are using the most common current transformer architecture, which is called theprenorm', '8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 182\n9 The Transformer 184\n9.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185', 'architecture).\n9.6 Summary\nThis chapter has introduced the transformer and its components for the task of lan-\nguage modeling. We’ll continue the task of language modeling including issues like\ntraining and sampling in the next chapter.']",['ed3book.pdf'],18.38393037498463,200,,5.0,1.0,Correct Answer,True,
Q055,"Based on the chapter titles in the table of contents of ed3book.pdf, what distinguishes Chapter 10 (Large Language Models) from Chapter 11 (Masked Language Models)?",comparison,ed3book.pdf,True,llama4,"Based on the provided document, Chapter 10 (Large Language Models) is not explicitly described, but Chapter 11 (Masked Language Models) is distinguished as being ""encoder-only"" models, producing an encoding for each input token and generally *not* used for generating text [p9]. They are instead used for interpretative tasks [p9]. Furthermore, masked language models differ from the causal transformers in previous chapters in that their attention function isn’t causal [p9].","['p9\n[SEP] \n++\ns1 s1 s1 s1 s2 s2 s2 s2\n+ + + + + + + + + + + + + +\nhCLS\nFigure 11.4 An example of the NSP loss calculation.\n11.2.3 Training Regimes\nBERT and other early transformer-based language models were trained on about\n3.3 billion words (a combination of English Wikipedia and a corpus of book texts\ncalled BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property\nreasons). Modern masked language models are now trained on much larger datasets', 'a classiﬁcation or a decision based on the token in context.\nRemember that we said the models of Chapter 9 are sometimes called decoder-\nonly, because they correspond to the decoder part of the encoder-decoder model we\nwill introduce in Chapter 13. By contrast, the masked language models of this chap-\nter are sometimes called encoder-only, because they produce an encoding for each\ninput token but generally aren’t used to produce running text by decoding/sampling.', 'That’s an important point: masked language models are not used for generation.\nThey are generally instead used for interpretative tasks.\n11.1.1 The architecture for bidirectional masked models\nLet’s ﬁrst discuss the overall architecture. Bidirectional transformer-based language\nmodels differ in two ways from the causal transformers in the previous chapters. The\nﬁrst is that the attention function isn’t causal; the attention for a token i can look at']",['ed3book.pdf'],19.217726583010517,200,,4.0,1.0,Correct Answer,True,
Q056,"What NLP application is the main focus of Chapter 14 in ed3book.pdf, according to the table of contents?",fact,ed3book.pdf,True,llama4,"According to the provided table of contents, the main focus of Part II, which includes Chapter 14, is on fundamental NLP applications: **machine translation, information retrieval, question answering, dialogue systems, and speech recognition.** [Document: NLP research (Bengio et al., 2013)].","['NLP research (Bengio et al., 2013).', 'Part II\nNLP APPLICATIONS\nIn this second part of the book we introduce fundamental NLP applications:\nmachine translation, information retrieval, question answering, dialogue systems,\nand speech recognition.', 'Subject Index 591\nTextTiling, 544\ntf-idf, 113\nThe Pile, 211\nthematic grid, 463\nthematic role, 462\nand diathesis alternation,\n463\nexamples of, 462\nproblems, 464\ntheme, 462\ntheme, as thematic role, 462\nTimeBank, 451\ntokenization, 4\nsentence, 24\nword, 18\nTop-k sampling, 208\ntop-p sampling, 209\ntopic models, 104\ntoxicity detection, 74\ntraining oracle, 419\ntraining set, 38\ncross-validation, 69\nhow to choose, 39\ntranscription\nof speech, 331\nreference, 346\ntransduction grammars, 287']",['ed3book.pdf'],18.01571458298713,200,,3.0,1.0,Correct Answer,True,
Q057,"Provide a brief overview of the topics likely covered in Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings'.",summary,ed3book.pdf,True,llama4,"Chapter 6 of ed3book.pdf, titled 'Vector Semantics and Embeddings', likely covers the following topics:

*   **Vector Semantics:** The chapter introduces vector semantics, which learns representations of word meaning (embeddings) from text distributions.
*   **Embeddings:** It focuses on *static embeddings*, where each word is mapped to a fixed embedding—a vector in high-dimensional space.
*   **Visualization of Embeddings:** Methods for visualizing these high-dimensional embeddings, such as projecting them into 2 dimensions using t-SNE, are discussed (referencing van der Maaten and Hinton, 2008).
*   **Semantic Properties of Embeddings:** The chapter briefly summarizes studied semantic properties of embeddings.

According to the document, these representations are used in all natural language processing applications that utilize meaning. [Document: ed3book.pdf, page 124]","['to the amount of difference in their environments” (Harris, 1954, p. 157).\nIn this chapter we introduce vector semantics, which instantiates this linguisticvector\nsemantics\nhypothesis by learning representations of the meaning of words, calledembeddings,embeddings\ndirectly from their distributions in texts. These representations are used in every nat-\nural language processing application that makes use of meaning, and the static em-', '124 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS\nmethod (Rohde et al., 2006).\nProbably the most common visualization method, how-\never, is to project the 100 dimensions of a word down into 2\ndimensions. Fig. 6.1 showed one such visualization, as does\nFig. 6.16, using a projection method called t-SNE (van der\nMaaten and Hinton, 2008).\n6.10 Semantic properties of embeddings\nIn this section we brieﬂy summarize some of the semantic properties of embeddings\nthat have been studied.', 'beddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n6.13 Summary\n• In vector semantics, a word is modeled as a vector—a point in high-dimensional\nspace, also called an embedding. In this chapter we focus on static embed-\ndings, where each word is mapped to a ﬁxed embedding.']",['ed3book.pdf'],20.98054516702541,200,,5.0,1.0,Correct Answer,True,
Q058,What is the specific draft date mentioned on the title page of ed3book.pdf?,fact,ed3book.pdf,True,llama4,"我无法从提供的文档中找到这个问题的答案。文档中提到了文档日期为 July 2, 2007 (根据文档)，但没有提及 ed3book.pdf 标题页上的特定草稿日期。","['2004). Fig. 20.17 reproduces our earlier example with these value attributes.\nThe dateline, or document date, for this text was July 2, 2007. The ISO repre-\nsentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.\nThe encodings for the temporal expressions in our sample text all follow from this\ndate, and are shown here as values for the VALUE attribute.\nThe ﬁrst temporal expression in the text proper refers to a particular week of the', 'year. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week\nof the year being the one that has the ﬁrst Thursday of the year. These weeks are\nrepresented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as “2007-W26”.\nThe next temporal expression is the weekend. ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single', 'that can be assigned to a particular point, or interval, in time. The following markup\nof the sample text on page 435 shows all the events in this text.']",['ed3book.pdf'],18.018228333006846,200,,3.0,1.0,Correct Answer,True,
Q059,"According to the index snippets provided for ed3book.pdf, compare the primary page number listed for the 'Viterbi algorithm' versus the start of the 'Transformer' chapter.",comparison,ed3book.pdf,True,llama4,"According to the provided index snippets from ed3book.pdf, the 'Viterbi algorithm' is discussed starting on page **381** [Document 1]. 

However, the start of the 'Transformer' chapter is not mentioned in the provided document snippets. Therefore, I cannot compare the page numbers.","['17.6 • E VALUATION OF NAMED ENTITY RECOGNITION 381\nThe requisite changes from HMM Viterbi have to do only with how we ﬁll each\ncell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes\nthe Viterbi value of time t for state j as\nvt ( j) =\nN\nmax\ni=1\nvt−1(i)ai j bj(ot ); 1 ≤j ≤N,1 <t ≤T (17.31)\nwhich is the HMM implementation of\nvt ( j) =\nN\nmax\ni=1\nvt−1(i) P(sj|si) P(ot |sj) 1 ≤j ≤N,1 <t ≤T (17.32)', 'max is taken. In this case the ﬁnal value, 2.772e-8, comes from the NNP state at the\nprevious column. The reader should ﬁll in the rest of the lattice in Fig. 17.14 and\nbacktrace to see whether or not the Viterbi algorithm returns the gold state sequence\nNNP MD VB DT NN.\n17.5 Conditional Random Fields (CRFs)\nWhile the HMM is a useful and powerful model, it turns out that HMMs need a\nnumber of augmentations to achieve high accuracy. For example, in POS tagging', 'v3(5)=\nmax * .\n000223\nv3(4)=\nmax * .00034\nv3(3)=\nmax * .00067\nv1(5)\nv1(6)\nv1(7)\nv2(1)\nv2(4)\nv2(6)\nv2(7)\nbacktrace\n* P(RB|NN)\n* P(NN|NN)\nstart start start start start\no5\nNNP\nP(MD|start) \n= .0006\nFigure 17.14 The ﬁrst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps\nthe probability of the best path so far and a pointer to the previous cell along that path. We have only ﬁlled out']",['ed3book.pdf'],18.533934041974135,200,,4.0,1.0,Incorrect Refusal,False,
Q060,"Does the table of contents in ed3book.pdf list a specific, distinct chapter dedicated solely to the topic of 'Ethical Considerations in NLP'?",refusal,ed3book.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。提供的文档内容是关于自然语言处理（NLP）的章节列表和索引，但没有明确提到一个专门的章节是关于“NLP中的伦理考量”。,"['16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 354\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\nIII Annotating Linguistic Structure 359\n17 Sequence Labeling for Parts of Speech and Named Entities 362\n17.1 (Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 363', 'Subject Index 591\nTextTiling, 544\ntf-idf, 113\nThe Pile, 211\nthematic grid, 463\nthematic role, 462\nand diathesis alternation,\n463\nexamples of, 462\nproblems, 464\ntheme, 462\ntheme, as thematic role, 462\nTimeBank, 451\ntokenization, 4\nsentence, 24\nword, 18\nTop-k sampling, 208\ntop-p sampling, 209\ntopic models, 104\ntoxicity detection, 74\ntraining oracle, 419\ntraining set, 38\ncross-validation, 69\nhow to choose, 39\ntranscription\nof speech, 331\nreference, 346\ntransduction grammars, 287', 'UMLS, the Uniﬁed Medical Language System from the US National Library of\nMedicine has a network that deﬁnes 134 broad subject categories, entity types, and\n54 relations between the entities, such as the following:\nEntity Relation Entity\nInjury disrupts Physiological Function\nBodily Location location-of Biologic Function\nAnatomical Structure part-of Organism\nPharmacologic Substance causes Pathological Function\nPharmacologic Substance treats Pathologic Function']",['ed3book.pdf'],18.594810374983354,200,,3.0,1.0,Incorrect Answer (Missing Refusal),False,
Q061,What are the main steps involved in text preprocessing according to L1_Introduction.pdf?,fact,L1_Introduction.pdf,True,llama4,"According to L1_Introduction.pdf, the main steps involved in text preprocessing are:

1. **Transforming raw data into an understandable format.** Text preprocessing is a data mining technique used to achieve this [文档 L1_Introduction.pdf, 页码 32].
2. **Cleaning the text data.** This is done before encoding text into numeric vectors [文档 L1_Introduction.pdf, 页码 32].
3. **Tokenization.** This involves breaking up the text into individual words (tokens) which can also be characters or sub-words [文档 L1_Introduction.pdf, 页码 33].
4. **Stop words removal.** This involves removing common words that don’t contribute much information, like “the”, “is”, and “a” [文档 L1_Introduction.pdf, 页码 34].","['How NLP works? \n• First step of NLP is text preprocessing, a data mining technique that \ninvolves transforming the raw data into an understandable format.\n• Before encoding text into numeric vector, we first need to clean the \ntext data .\n• Process to prepare(or clean) text data before encoding is called text \npreprocessing.\n32', 'Preprocessing of data - Tokenization\n• Tokenization is the process of breaking up text document into \nindividual words called tokens.\n• Tokens can be either words, characters, or sub-words.\n33', 'Preprocessing of data - Stop words removal\n• Stop words are common words that do not contribute much of the \ninformation in a text document. Words like ‘the’, ‘is’, ‘a’ have less \nvalue and add noise to the text data.  “NLTK”\n34']",['L1_Introduction.pdf'],7.577920584008098,200,,5.0,1.0,Correct Answer,True,
Q062,Summarize the key challenges in NLP mentioned in L1_Introduction.pdf.,summary,L1_Introduction.pdf,True,llama4,"According to L1_Introduction.pdf, the key challenges in NLP are:

*   **Ambiguity:** Similar strings can have different meanings, and different strings can have the same meaning.
*   **Context:** Understanding the context is crucial.
*   **Commonsense knowledge:** NLP systems require commonsense knowledge to properly interpret language.

(Source: L1_Introduction.pdf, page 27)","['Why is NLP challenging?\n• Ambiguity:\n• Similar strings mean different things, different strings \nmean the same thing.\n• Context\n• Commonsense knowledge\n27', 'NLP history (1)\n19', 'Outline\n• 0. Course logistics\n• 1. Recent applications\n• 2. History of NLP\n• 3. Challenges\n• 4. How NLP works?\n2']",['L1_Introduction.pdf'],5.59940516599454,200,,5.0,1.0,Correct Answer,True,
Q063,Compare Stemming and Lemmatization based on the descriptions in L1_Introduction.pdf.,comparison,L1_Introduction.pdf,True,llama4,"Based on the provided documents, here's a comparison of stemming and lemmatization:

*   **Stemming:** Reduces words to their root form by chopping off the ends of words. The resulting stem may *not* be a valid word in the language (e.g., 'movi' from 'movie'). [Document: L1\_Introduction.pdf, page 36 & 37]
*   **Lemmatization:** Also reduces words to their root form, but ensures the root word *is* a valid word in the language (e.g., 'care' from 'caring'). [Document: L1\_Introduction.pdf, page 37]

In essence, lemmatization is a more sophisticated version of stemming, aiming for linguistically correct root words. Both aim to reduce inflection, but stemming is described as a ""crude heuristic process."" [Document: L1\_Introduction.pdf, page 36 & 37]","['Preprocessing of data - Lemmatization\n• Lemmatization does the same thing as stemming, converting a word \nto its root form but with one difference, i.e., the root word in this \ncase belongs to a valid word in the language. For example the word \ncaring would map to ‘care’ and not ‘car’ as the in case of stemming.\na) Stemming b) Lemmatization\n37', 'Preprocessing of data - Stemming\n• Stemming is the process of reducing a word to its stem/root word. It \nreduces inflection in words (e.g. ‘help’, ’helping’, ’helped’, ’helpful’) to \ntheir root form (e.g. ‘help’). It removes the morphological affixes from \nwords, leaving only the word stem.\n• A crude heuristic process that chops off the ends of words in the \nhope of correctly transforming words into its root form. So the words \n“trouble”, “troubled” and “troubles” might actually be converted', 'Preprocessing of data - Stemming\n• The stem word may or may not be a valid word in the language. For \nexample ‘movi’ is the root word for ‘movie’, ‘emot’ is the root word for \n‘emotion’.\n36']",['L1_Introduction.pdf'],11.117875875002937,200,,5.0,1.0,Correct Answer,True,
Q064,What is Byte Pair Encoding (BPE) as explained in the appendix of L1_Introduction.pdf?,fact,L1_Introduction.pdf,True,llama4,"According to the provided document, Byte Pair Encoding (BPE) is a word segmentation algorithm that iteratively merges the most frequently occurring character or character sequences [L1_Introduction.pdf]. It is also a widely used tokenization method among transformer-based models and effectively tackles Out-Of-Vocabulary (OOV) issues by segmenting them into subwords [L1_Introduction.pdf]. The document also states that BPE results in shorter input and output sentences compared to character tokenization [L1_Introduction.pdf].

The steps to learn BPE are:
*   Split the words in the corpus into characters after appending </w>
*   Initialize the vocabulary with unique characters in the corpus
*   Compute the frequency of a pair of characters or character sequences in the corpus
*   Merge the most frequent pair in the corpus [L1_Introduction.pdf].","['Welcome to Byte Pair Encoding (BPE).\n• BPE is a word segmentation algorithm that merges the most \nfrequently occurring character or character sequences iteratively. \nHere is a step by step guide to learn BPE.\n• Steps to learn BPE\n• Split the words in the corpus into characters after appending </w>\n• Initialize the vocabulary with unique characters in the corpus\n• Compute the frequency of a pair of characters or character sequences in \ncorpus\n• Merge the most frequent pair in corpus', 'Welcome to Byte Pair Encoding (BPE).\n• Byte Pair Encoding (BPE) is a widely used tokenization method among \ntransformer-based models. BPE addresses the issues of Word and \nCharacter Tokenizers:\n• BPE tackles OOV effectively. It segments OOV as subwordsand represents the \nword in terms of these subwords\n• The length of input and output sentences after BPE are shorter compared to \ncharacter tokenization\n54', 'Tokenization  - Subword Tokenization\n• Subword Tokenization splits the piece of text into subwords (or n-\ngram characters). For example, words like lower can be segmented as \nlow-er, smartest as smart-est, and so on.\n• Transformer-based models– the SOTA in NLP – rely on Subword\nTokenization algorithms for preparing vocabulary. \n• Welcome to Byte Pair Encoding (BPE).\n53']",['L1_Introduction.pdf'],7.608172792010009,200,,5.0,1.0,Correct Answer,True,
Q065,Does L1_Introduction.pdf provide specific Python code examples for implementing TF-IDF?,refusal,L1_Introduction.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。该文档解释了 TF-IDF 的概念和计算方法，并提供了一些示例，但没有提供具体的 Python 代码示例。,"['Term Frequency-Inverse Document \nFrequency (TF-IDF)\n• In NLP tasks, each text sentence is called a document and collection of \nsuch documents is referred to as text corpus.\n• Wikipedia: “Term frequency–inverse document frequency, is a \nnumerical statistic that is intended to reflect how important a word is \nto a document in a collection or corpus.”\n45', 'Term Frequency-Inverse Document \nFrequency (TF-IDF)\n• Inverse Document Frequency (IDF): a measure of how important a \nterm is. \n𝑖𝑖𝑑𝑑𝑓𝑓𝑡𝑡 = log 𝑛𝑛𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁 𝑜𝑜𝑜𝑜 𝑑𝑑𝑜𝑜𝑑𝑑𝑁𝑁𝑁𝑁𝑁𝑁𝑛𝑛𝑡𝑡𝑡𝑡\n𝑛𝑛𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁 𝑜𝑜𝑜𝑜 𝑑𝑑𝑜𝑜𝑑𝑑𝑁𝑁𝑁𝑁𝑁𝑁𝑛𝑛𝑡𝑡𝑡𝑡𝑤𝑤𝑖𝑖𝑡𝑡𝑡𝑡𝑡𝑁𝑁𝑁𝑁𝑁𝑁""𝑡𝑡""\nReview 1: This movie is very scary and long\nReview 2: This movie is not scary and is slow\nReview 3: This movie is spooky and good\n• IDF(‘this’) = log(number of documents / \nnumber of documents containing the word \n‘this’) = log(3/3) = log(1) = 0', 'Term Frequency-Inverse Document \nFrequency (TF-IDF)\n• Term Frequency, a measure of how frequently a term 𝑡𝑡appears in a \ndocument 𝑑𝑑:\n• 𝑛𝑛𝑡𝑡,𝑑𝑑 is the number of times the term 𝑡𝑡appears in the document 𝑑𝑑. \n𝑡𝑡𝑓𝑓𝑡𝑡,𝑑𝑑 = 𝑛𝑛𝑡𝑡,𝑑𝑑\n𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑜𝑜𝑜𝑜 𝑡𝑡𝑁𝑁𝑁𝑁𝑁𝑁𝑡𝑡𝑖𝑖𝑛𝑛𝑡𝑡𝑡𝑁𝑁𝑑𝑑𝑜𝑜𝑑𝑑𝑁𝑁𝑁𝑁𝑁𝑁𝑛𝑛𝑡𝑡\nReview 1: This movie is very scary and long\nReview 2: This movie is not scary and is slow\nReview 3: This movie is spooky and good\n46']",['L1_Introduction.pdf'],6.113975999993272,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q066,What is the definition of a language model according to L2_LanguageModel.pdf?,fact,L2_LanguageModel.pdf,True,llama4,"According to the provided document, a language model is defined in two ways:

1.  As assigning a probability to a piece of text, where for a sequence of words (𝑥𝑥(1), 𝑥𝑥(2), … 𝑥𝑥(𝑇𝑇)), the probability of the text is 𝑃𝑃(𝑥𝑥(1), 𝑥𝑥(2), … 𝑥𝑥(𝑇𝑇)) [L2_LanguageModel.pdf, page 4 & 5].
2.  As predicting what word comes next [L2_LanguageModel.pdf, page 5].

Additionally, a language model is a probability distribution over sequences of words [L2_LanguageModel.pdf, page 4]. It can also be used to determine if a sequence of words sounds like natural language [L2_LanguageModel.pdf, page 4].","['Another interpretation of a language model\n● The task of language models could be regarded as assigning probability to a piece of \ntext. \n● For example, consider a piece of text (𝑥𝑥(1), 𝑥𝑥(2), … 𝑥𝑥(𝑇𝑇)), then according to the \nLangugage Model, the probability of the text could be written as: \nA language model can also be interpreted as predicting what word comes next\n5', 'What is a language model?\n● A language model is a probability distribution over sequences of words. Given such a \nsequence, say of length T, it assigns a probability 𝑃𝑃(𝑥𝑥(1), 𝑥𝑥(2), … 𝑥𝑥(𝑇𝑇)) to the whole \nsequence.\n● To determine whether a given ordering of words sounds like natural language.\nE.g., two orderings \n‘NLP is about how machines understand and process natural languages.’\n‘machines NLP about languages understand how and is natural process.’\n4', 'Outline\n● 1. Language model definition & applications\n● 2. Model construction\n○ Statistical language models\n○ Neural language models\n● 3. Language model evaluation\n2']",['L2_LanguageModel.pdf'],7.595685458014486,200,,5.0,1.0,Correct Answer,True,
Q067,Summarize the sparsity and storage problems associated with n-gram models as discussed in L2_LanguageModel.pdf.,summary,L2_LanguageModel.pdf,True,llama4,"According to the provided document, n-gram models face two main problems:

*   **Storage problems:** Increasing the size of the corpus leads to an increase in model size [文档17].
*   **Sparsity problems:** Increasing 'n' (the length of the n-gram) exacerbates sparsity issues, typically limiting 'n' to a maximum of 5 [文档16].","['Storge problems with n-gram\nNote:\nIncreasing corpus increases the model size. \n17', 'n-gram model\n● Every word in Σ is assigned some probability, conditoned on a fixed-length history (n \n– 1).\n● An n-gram is a chunk of n consecutive words.\n11', 'Sparsity problems with n-gram\nNote:\nIncreasing n makes \nsparsity problems \nworse.\nTypically we can’t have \nn bigger than 5.\n16']",['L2_LanguageModel.pdf'],5.935300000011921,200,,5.0,1.0,Correct Answer,True,
Q068,Compare the fixed-window neural language model with the RNN-based language model based on L2_LanguageModel.pdf.,comparison,L2_LanguageModel.pdf,True,llama4,"Based on the provided document, here's a comparison between the fixed-window neural language model and the RNN-based language model:

**Fixed-Window Neural Language Model:**

*   **Improvements over n-gram:** Doesn't have a sparsity problem and doesn't require storing all observed n-grams. [Document, page 19]
*   **Remaining Problems:** The fixed window size is limited, and increasing it increases the weight matrix (𝑾𝑾). It also lacks symmetry in input processing as different weights are used for different inputs. [Document, page 19]

**RNN-based Language Model:**

*   **Advantages:** Can process inputs of any length, potentially utilize information from many previous steps, maintains a consistent model size regardless of input length, and exhibits symmetry in input processing due to the application of the same weights across all timesteps. [Document, page 25]

**Model Performance:**

*   LSTM performs better than RNN, which in turn performs better than n-gram models (lower perplexity is better). [Document, page 25]","['A fixed-window neural language model\nY. Bengio, et al. (2003): A Neural Probabilistic Language Model\nImprovements over n-gram:\n● No sparsity problem\n● Don’t need to store all \nobserved n-grams\nRemaining problems:\n● Fixed window is too small.\n● Enlarging window enlarges \n𝑾𝑾; Window can never be \nlarge enough.\n● and       are multiplied \nby completely different \nweights in 𝑾𝑾. No \nsymmetry in how the \ninputs are processed.\n19', 'RNN-based language model\nRNN Advantages:\n● Can process any length input\n● Computation for step t can \n(in theory) use information \nfrom many steps back\n● Model size doesn’t increase \nfor longer input\n● Same weights applied on \nevery timestep, so there is \nsymmetry in how inputs are \nprocessed.\n25', 'Model performance comparison (2)\nPerplexity improves \n(lower is better)\n 39https://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/\nLSTM < RNN < n-gram']",['L2_LanguageModel.pdf'],9.14550287500606,200,,5.0,1.0,Correct Answer,True,
Q069,How is perplexity defined and used to evaluate language models in L2_LanguageModel.pdf?,fact,L2_LanguageModel.pdf,True,llama4,"根据提供的文档，perplexity 是语言模型（LM）的标准评估指标[文档2]。perplexity 可以从交叉熵损失 𝐽𝐽(𝜃𝜃) 直接推导出来[文档2]。较低的 perplexity 更好[文档2, 文档39]。","['Evaluating language models\n● The standard evaluation metrics for LM is perplexity.\n● Perplexity can be derived directly from the cross-entropy loss 𝐽𝐽(𝜃𝜃):\n● Lower perplexity is better.\n 37', 'Model performance comparison (2)\nPerplexity improves \n(lower is better)\n 39https://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/\nLSTM < RNN < n-gram', 'Outline\n● 1. Language model definition & applications\n● 2. Model construction\n○ Statistical language models\n○ Neural language models\n● 3. Language model evaluation\n2']",['L2_LanguageModel.pdf'],5.628764292021515,200,,4.0,1.0,Correct Answer,True,
Q070,Does L2_LanguageModel.pdf provide a detailed mathematical derivation for the GRU update gates?,refusal,L2_LanguageModel.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档中提到了 GRU，但没有提供 GRU 更新门的详细数学推导。,"['GRU\n● A simpler version of LSTM introduced by Cho et al. in 2014\n● Fewer gates, less computation\n● Replace the forget gate (   ) with\n● On each timestep t we have input and hidden state (no cell state).\n: input gate\n: reset gate\n: new hidden state content\n: hidden state\n33', 'Evaluating language models\n● The standard evaluation metrics for LM is perplexity.\n● Perplexity can be derived directly from the cross-entropy loss 𝐽𝐽(𝜃𝜃):\n● Lower perplexity is better.\n 37', 'Outline\n● 1. Language model definition & applications\n● 2. Model construction\n○ Statistical language models\n○ Neural language models\n● 3. Language model evaluation\n2']",['L2_LanguageModel.pdf'],4.778102458018111,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q071,What is the Distributional Hypothesis as explained in L3_WordEmbedding.pdf?,fact,L3_WordEmbedding.pdf,True,llama4,"根据提供的文档，Distributional Hypothesis 指的是：在相似语境中出现的词语往往具有相似的含义。这个假设由 J. R. Firth 在 1957 年提出，可以用“通过它所处的环境来认识一个词”来概括。它也是现代统计自然语言处理中最成功的思想之一。[文档L3_WordEmbedding.pdf]

此外，文档还解释说，一个词 w 的语境是指在文本中附近出现的词语集合（具有固定大小的窗口）。通过使用 w 的许多语境来构建 w 的表示。[文档L3_WordEmbedding.pdf]","['Distributional hypothesis (1)\n● Words that occur in similar contexts tend to \nhave similar meanings\n● Proposed by J. R. Firth in 1957\n● “You shall know a word by the company it \nkeeps.”\n● One of the most successful ideas of modern \nstatistical NLP\n8\nJohn Rupert Firth', 'Distributional hypothesis (2)\n● When a word w appears in a text, its context is the set of words that appear nearby \n(with a fixed-size window)\n● Use many contexts of w to build up a representation of w\n● In the example, the context words will represent banking\nExample: context words represent banking\n... government debt problems turning into banking crises as happened in 2009…\n...saying that Europe needs unified banking regulation to replace the hodgepodge…', 'Inspired by distributional hypothesis\n● Latent Semantic Analysis – representation\no (Deerwester et al, 1990): co-occurrence counting + SVD\n● Collobert & Weston vectors - first neural pretrained word embedding\no (Collobert et al, 2008: A unified architecture for natural language processing) based on joint \nprobability of target and context words, word embedding to support multiple downstream tasks\n● Word2vec - learn good vector presentations for words/phrases']",['L3_WordEmbedding.pdf'],6.460777875006897,200,,5.0,1.0,Correct Answer,True,
Q072,Compare the Skip-gram and CBOW models within the Word2vec framework based on L3_WordEmbedding.pdf.,comparison,L3_WordEmbedding.pdf,True,llama4,我无法从提供的文档中找到这个问题的答案。文档仅提到了 Word2vec 包含 CBOW 和 skip-gram 模型，但没有对这两种模型进行比较。,"['Word2vec examples', 'What we have learned so far (2)?\n● Word embeddings\n○ Represent each word with a vector\n○ Word2vec: CBOW or skip-gram\n○ GloVe: global statistics (co-occurrence) + local context window\n49', 'References\nWord2vec Skip gram Explained: https://thinkinfi.com/word2vec-skip-gram-explained/\nword2vec Parameter Learning Explained: https://arxiv.org/pdf/1411.2738.pdf\nCS224n: Natural Language Processing with Deep Learning Lecture Notes: Part I: \nhttp://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf\nNatural Language Processing (Almost) from Scratch: \nhttps://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf?source=post_pag\ne---------------------------']",['L3_WordEmbedding.pdf'],5.391060833004303,200,,1.0,1.0,Correct Answer,True,
Q073,"Summarize the Negative Sampling technique used to improve Word2vec training efficiency, as described in L3_WordEmbedding.pdf.",summary,L3_WordEmbedding.pdf,True,llama4,根据提供的文档“L3_WordEmbedding.pdf”，Negative sampling 是一种用于提高 Word2vec 训练效率的技术。该文档将 Negative sampling 和 Hierarchical softmax 列为改进训练效率的方法 [文档L3_WordEmbedding.pdf]。文档中没有提供关于 Negative sampling 具体如何运作的细节。,"['Word2vec performance – different dimensions', 'Word2vec examples', 'Outline\n● 1. Word embedding definition and principles\n● 2. Embedding methods – word2vec\n○ Continuous bag-of-words\n○ Skip-gram\n● 3. Improve training efficiency\n○ Negative sampling\n○ Hierarchical softmax\n● 4. Other word embedding methods\n○ GloVe\n● 5. Contextualized word embeddings (ELMo)\n2']",['L3_WordEmbedding.pdf'],5.7964570420153905,200,,3.0,1.0,Correct Answer,True,
Q074,What is the core idea behind GloVe embeddings according to L3_WordEmbedding.pdf?,fact,L3_WordEmbedding.pdf,True,llama4,"According to L3_WordEmbedding.pdf, the core idea behind GloVe embeddings is to combine **global statistics (LSA)** with a **local context window (word2vec)**. It utilizes a **co-occurrence matrix** with **decreasing weighting** where the weight decays as the distance between word pairs increases (𝑋𝑋𝑖𝑖𝑗𝑗=1/d) [L3_WordEmbedding.pdf].","['Other word embedding models – GloVe (2)\n● Loss function', 'Visualizing word embeddings - GloVe', 'Other word embedding models – GloVe (1)\n● GloVe: Global Vectors for Word Representation\n○ Global statistics (LSA) + local context window (word2vec)\n○ Co-occurrence matrix, decreasing weighting: decay 𝑋𝑋𝑖𝑖𝑗𝑗=1/d (distance of word pairs)\n•I like deep learning.\n•I like NLP.\n•I enjoy flying.']",['L3_WordEmbedding.pdf'],7.399878541007638,200,,5.0,1.0,Correct Answer,True,
Q075,"Does L3_WordEmbedding.pdf provide performance comparisons (e.g., accuracy scores on specific tasks) between Word2vec and GloVe?",refusal,L3_WordEmbedding.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了Word2vec和GloVe，但没有提供它们在特定任务上的性能比较（例如，准确率）。,"['Word2vec performance – different dimensions', 'Other word embedding models – GloVe (2)\n● Loss function', 'Other word embedding models – GloVe (1)\n● GloVe: Global Vectors for Word Representation\n○ Global statistics (LSA) + local context window (word2vec)\n○ Co-occurrence matrix, decreasing weighting: decay 𝑋𝑋𝑖𝑖𝑗𝑗=1/d (distance of word pairs)\n•I like deep learning.\n•I like NLP.\n•I enjoy flying.']",['L3_WordEmbedding.pdf'],4.715627958008554,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q076,"What are the Query, Key, and Value vectors used for in the attention mechanism described in L4_Transformer and pretraining-finetuning.pdf?",fact,L4_Transformer and pretraining-finetuning.pdf,True,llama4,"According to the provided documents, the Query, Key, and Value vectors are used as follows in the attention mechanism:

*   **Encoder:** All keys, values, and queries come from the output of the previous layer or the raw input embedding (with hidden dimension 𝑑𝑑model) in the encoder. ([Transformer: attention in encoder])
*   **Decoder:** Queries come from the output of the masked multi-head attention, while keys and values come from the top encoder output. ([Transformer: attention in decoder])
*   In both encoder and decoder, keys, values and queries can also come from the output of the previous layer. ([Transformer: attention in encoder], [Transformer: attention in decoder])

The documents also show a diagram illustrating the relationship between query (𝒒𝒒), keys and values (𝒌𝒌i, 𝒗𝒗i), attention distribution (𝑎𝑎i), and attention vector (𝒛𝒛). ([Attention: example])","['Transformer: attention in encoder\nEncoder\n● In a self-attention layer all of the keys, values and queries \ncome from the output of the previous layer or the raw \ninput embedding (with hidden dimension 𝑑𝑑model ) in the \nencoder.\n● Each position in the encoder can attend to all positions in \nthe previous layer of the encoder.\n27', 'Attention: example\nquery 𝒒𝒒\nkeys and values 𝒌𝒌i, 𝒗𝒗i\nL\ni=1\nattention distribution 𝑎𝑎i\n12\nattention vector 𝒛𝒛', 'Transformer: attention in decoder\nDecoder\n● Similarly, each position attends to all positions in the decoder \nup to and including that position.\n● Encoder-decoder attention: keys and values come from the top \nencoder output; queries come from the output of the masked\nmulti-head attention (with hidden dimension 𝑑𝑑model ) in the \ndecoder.\n● Masked self-attention\n○ Keys, values and queries come from the output of the previous layer or the']",['L4_Transformer and pretraining-finetuning.pdf'],7.766386625007726,200,,5.0,1.0,Correct Answer,True,
Q077,Explain the concept of Multi-Head Self-Attention as presented in L4_Transformer and pretraining-finetuning.pdf.,summary,L4_Transformer and pretraining-finetuning.pdf,True,llama4,"According to the provided documents, Multi-Head Self-Attention is the main technique used in the Transformer architecture, proposed by Google in ""Attention is All You Need"" (Vaswani et al., 2017) [25]. 

The concept involves using different forms of weight matrices –  𝑊𝑊Q, 𝑊𝑊K, and 𝑊𝑊V – with each group being called a ""head"" [20]. Using multiple heads increases representation performance by allowing the model to focus on different positions and utilize different representation subspaces [20]. For example, 8-head self-attention results in 8 groups of attention vectors [20].","['Attention is all you need: multi-head self-attention (3)\n● In a nutshell\n22', 'Transformer\n• Proposed by Google: Attention is All You Need (Vaswani et al., 2017)\n• Main technique: multi-head self attention mechanism.\n• The transformer is a novel architecture that aims to solve sequence-to-sequence tasks while handling long- \nrange dependencies with ease.\n25', 'Attention is all you need: multi-head self-attention (1)\n● What if we choose different forms of 𝑊𝑊Q , 𝑊𝑊K ,  and 𝑊𝑊V?\n○ Each group of (𝑊𝑊Q, 𝑊𝑊K , 𝑊𝑊V) is called a head.\n○ Increase the representation performance: different representation subspaces, model’s ability to focus \non different positions.\n8 head self-attention gives 8 groups of \nattention vectors\n20']",['L4_Transformer and pretraining-finetuning.pdf'],6.8180269169970416,200,,5.0,1.0,Correct Answer,True,
Q078,Compare the pre-training objectives (MLM and NSP) used in BERT according to L4_Transformer and pretraining-finetuning.pdf.,comparison,L4_Transformer and pretraining-finetuning.pdf,True,llama4,"According to the provided documents, BERT is pre-trained using two self-supervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) [L4_Transformer]. The documents state that these two tasks are proposed to enable bidirectional transformer pre-training [pretraining-finetuning.pdf]. 

The documents do not provide a detailed *comparison* of MLM and NSP, only that they are both used in BERT's pre-training process.","['BERT: pretraining + finetuning\n● Apart from output layers, the same architectures are used in both pre-training and fine-tuning.\n● The same pre-trained model parameters are used to initialize models for different down- \nstream tasks.\n● During fine-tuning, all parameters are fine-tuned.\n36', '39\nPre-training BERT\n● In order to pre-train BERT transformers bidirectionally, two self-\nsupervised tasks are proposed, namely Masked LM and Next \nSentence Prediction (NSP).\n● One of the reasons of BERT’s success is the amount of data that it \ngot trained on.\n● The two corpuses that were used to pretain the language models \nare:\n○ The BooksCorpus (800M words)\n○ English Wikipedia (2,500M words)', 'Comparison of pre-trained models:\n• https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda']",['L4_Transformer and pretraining-finetuning.pdf'],7.3387900829839054,200,,3.0,1.0,Correct Answer,True,
Q079,How does the Transformer architecture incorporate positional information according to L4_Transformer and pretraining-finetuning.pdf?,fact,L4_Transformer and pretraining-finetuning.pdf,True,llama4,"According to the provided documents, the Transformer architecture incorporates positional information because it ""does not know the order of input tokens"" and therefore ""we have to let the model know the positions of the tokens explicitly"" [文档X]. The input representation of a token is the sum of two embeddings: the token embedding and the positional embedding [文档X]. Positional encoding uses sinusoids, where each dimension corresponds to a sinusoid, and allows the model to learn to attend by relative positions [文档X]. Specifically, for any fixed offset *k*, *PE<sub>pos+k</sub>* can be represented as a linear transformation of *PE<sub>pos</sub>* [文档X]. ","['Positional encoding in transformer (2)\n● In order to add position information (order of the sequence)\n● 𝑝𝑝𝑜𝑜𝑠𝑠 is position (e.g., the 𝑝𝑝𝑜𝑜𝑠𝑠-th token in a sentence) and 𝑖𝑖 is the 𝑖𝑖-th element in a \nvector to represent each position.\n● Each dimension of the positional encoding corresponds to a sinusoid.\n● For any fixed offset 𝑘𝑘, 𝑃𝑃𝐸𝐸𝑝𝑝𝑜𝑜𝑠𝑠+𝑘𝑘 can be represented as a linear transformation of\n𝑃𝑃𝐸𝐸𝑝𝑝𝑜𝑜𝑠𝑠. This would allow the model to easily learn to attend by relative positions.\n31', 'Positional encoding in transformer (3)\n𝑝𝑝𝑜𝑜𝑠𝑠\n𝑖𝑖\n41', '30\nPositional encoding in transformer (1)\n● Transformer does not contain recurrence or convolution, it does not know the order\nof input tokens.\n● We have to let the model know the positions of the tokens explicitly\n● Idea: input representation of a token is the sum of two embeddings: token and \npositional']",['L4_Transformer and pretraining-finetuning.pdf'],6.7857939999958035,200,,5.0,1.0,Correct Answer,True,
Q080,"Does L4_Transformer and pretraining-finetuning.pdf provide the exact number of parameters for the different GPT versions (GPT-1, GPT-2, GPT-3)?",refusal,L4_Transformer and pretraining-finetuning.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档中提到了GPT框架和预训练模型，但没有提供GPT-1、GPT-2和GPT-3的具体参数数量。,"['GPT framework (2)\n- Supervised fine-tuning\n- keep the pre-trained transformers\n- replace the final linear layer 𝑊𝑊e with 𝑊𝑊y,\n- Given data inputs X and labels y, maximize\nauxiliary training objective\n52', 'GPT framework (1)\n51\n- Multi-layer transformer decoder\n- first layer: ℎ0 = 𝑈𝑈𝑊𝑊𝑒𝑒 + 𝑊𝑊𝑝𝑝\n- the l-th layer: ℎ𝑙𝑙 = 𝑡𝑡𝑟𝑟𝑎𝑎𝑡𝑡𝑠𝑠𝑓𝑓𝑜𝑜𝑟𝑟𝑚𝑚𝑡𝑡𝑟𝑟_𝑏𝑏𝑙𝑙𝑜𝑜𝑏𝑏𝑘𝑘 ℎ𝑙𝑙−1 , ∀𝑙𝑙 ∈ 1, 𝑡𝑡\n- Self-supervised pre-training, similar to embdeddings such as \nword2Vec\n- Given tokens U, maximize the contextual conditional probability\nwhere k is the window size; ℎ𝑛𝑛𝑊𝑊𝑒𝑒𝑇𝑇 is the score for each word.\n- Unidirectional!', 'Comparison of pre-trained models:\n• https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda']",['L4_Transformer and pretraining-finetuning.pdf'],5.322587500006193,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
Q081,What is the difference between NLU and NLG as defined in L5_NLU tasks.pdf?,fact,L5_NLU tasks.pdf,True,llama4,"According to the document, the difference between NLU and NLG is as follows:

* **NLU (Natural Language Understanding)** focuses on comprehending and extracting meaning from natural language input, and includes tasks like text classification and question answering [文档: NLP tasks – NLU vs. NLG (2), page 5].
* **NLG (Natural Language Generation)** focuses on generating human-like text to convey information or communicate effectively, and includes tasks like machine translation and dialogue generation [文档: NLP tasks – NLU vs. NLG (2), page 5].","['NLP tasks – NLU vs. NLG (2)\n5', 'NLP tasks – NLU vs. NLG (1)\n3', 'NLU vs. NLG\n● NLU focuses on comprehending and extracting meaning from natural language \ninput. It involves tasks such as text classification and question answering (reading \ncomprehension).\n● NLG focuses on generating human-like text that conveys information or \ncommunicates effectively. It involves tasks such as machine translation and dialogue \ngeneration.\n● For today’s lecture, we will focus on two classic NLU tasks, i.e., text classification and \nquestion answering.\n4']",['L5_NLU tasks.pdf'],6.164322874974459,200,,5.0,1.0,Correct Answer,True,
Q082,"Summarize the different types of text classification mentioned in L5_NLU tasks.pdf (e.g., binary, multi-class, multi-label, ordinal).",summary,L5_NLU tasks.pdf,True,llama4,"根据提供的文档，L5_NLU tasks.pdf 提到了文本分类是一个将标签或标记分配给文本单元（如句子、查询、段落和文档）的经典自然语言处理 (NLP) 问题。文档中列举了以下文本分类任务示例：垃圾邮件检测、情感分析、新闻分类、用户意图分类和内容审核。

然而，文档**没有**具体说明不同类型的文本分类（例如，二元、多类、多标签、序数）。因此，我无法从提供的文档中找到这个问题的答案。","['Text classification - What is text classification?\n● Classification (a.k.a. “categorization”): a ubiquitous enabling technology in data \nscience; studied within pattern recognition, statistics, and machine learning.\n● Text classification, also known as text categorization, is a classical problem in natural \nlanguage processing (NLP), which aims to assign labels or tags to textual units such \nas sentences, queries, paragraphs, and documents.', 'Outline\n● NLU vs. NLG\n● NLU task 1: text classification\n● NLU task 2: question answering\n2', 'Text classification tasks\n● Textual data sources\n○ Textual data can come from different sources, including web data, emails, chats, \nsocial media, tickets, insurance claims, user reviews, and questions and answers from \ncustomer services\n● Task examples\n○ spam detection, sentiment analysis, news categorization, user intent classification, \ncontent moderation\n9']",['L5_NLU tasks.pdf'],7.034182999981567,200,,1.0,1.0,Correct Answer,True,
Q083,Compare IR-based QA and Knowledge-based QA approaches as described in L5_NLU tasks.pdf.,comparison,L5_NLU tasks.pdf,True,llama4,"According to the document, here's a comparison of IR-based QA and Knowledge-based QA:

**IR-based QA:** This approach involves two steps: Information Retrieval (IR) followed by Reading Comprehension (RC). It's used for open-domain question answering, where IR finds relevant passages from a large text source (like the web), and then neural reading comprehension is applied [文档: L5_NLU tasks.pdf]. It is particularly suited for *factoid questions* – those answerable with simple facts in short texts [文档: L5_NLU tasks.pdf].

**Knowledge-based QA:** This approach focuses on generating a logical form expression to query a knowledge base [文档: L5_NLU tasks.pdf].","['IR-based QA\n● Two steps: IR + RC\nIR\n RC 36', 'Factoid QA\n● Factoid questions: questions that can be answered with simple facts expressed in \nshort texts, like the following:\n○ Where is the Louvre Museum located?\n○ What is the average age of the onset of autism?\n● Information-retrieval (IR) based QA (open domain question QA)\n○ Given a user question, information retrieval is used to find relevant passages (vast amount of text on \nthe web or in collections of scientific papers like PubMed). Then neural reading comprehension', 'Knowledge-based QA\n● Generate logical form expression to query the knowledge base\n37']",['L5_NLU tasks.pdf'],7.1985621670028195,200,,5.0,1.0,Correct Answer,True,
Q084,What are the key characteristics of the SQuAD 1.1 dataset according to L5_NLU tasks.pdf?,fact,L5_NLU tasks.pdf,True,llama4,"According to the provided document, SQuAD 1.1 consists of 100,000 questions that were combined with over 50,000 unanswerable questions to create SQuAD 2.0 [文档44]. The unanswerable questions were specifically designed to be similar to answerable ones [文档44]. 

The document does *not* detail the key characteristics of the SQuAD 1.1 dataset *according to L5_NLU tasks.pdf* beyond this information.","['SQuAD 2.0\n44\n● SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 \nunanswerable questions written adversarially by crowdworkers to look similar to \nanswerable ones. \n● To do well on SQuAD2.0, systems must not only answer questions when possible, \nbut also determine when no answer is supported by the paragraph and abstain \nfrom answering.', 'SQuAD 2.0 no answer example\n45', 'Results on SQuAD\nEnsemble: train 12 models, choose start and end indices with the highest sum of \nconfidence scores.\n52']",['L5_NLU tasks.pdf'],6.4948726250149775,200,,2.0,1.0,Correct Answer,True,
Q085,Does L5_NLU tasks.pdf provide specific accuracy results for the BiDAF model on the CoLA dataset from GLUE?,refusal,L5_NLU tasks.pdf,False,llama4,我无法从提供的文档中找到这个问题的答案。文档提到了GLUE基准测试和BiDAF模型，但没有提供BiDAF模型在CoLA数据集上的具体准确性结果。,"['NLU tasks – GLUE\nGeneral Language Understanding Evaluation (GLUE) benchmark is a collection of nine NLU tasks by scholars from \nNYU, U. Washington, etc.\n6', 'NLP tasks – NLU vs. NLG (2)\n5', 'BiDAF result on CNN/Daily Mail\n● Only predict start index\n● Mask out non-entity words \nin classification layer\n● For loss function: sum \nprobability over all \ninstances of the correct \nentity\n56']",['L5_NLU tasks.pdf'],4.922377291019075,200,,4.0,1.0,Incorrect Answer (Missing Refusal),False,
